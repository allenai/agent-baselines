[
  {
    "question": "In robotics, what are the leading methods for learning terrain traversibility costs automatically from robot experience?",
    "response": {
      "sections": [
        {
          "title": "Introduction: Terrain Traversibility in Robotics",
          "text": "\nTerrain traversibility in robotics refers to the ability of a robot to safely and efficiently navigate across different types of terrain. Traditionally, traversibility cost functions were hand-engineered based on human expertise, which limited robot adaptation to new environments. The field has now shifted toward learning these costs automatically from robot experience, allowing robots to develop their own understanding of which terrains are navigable and at what cost. This approach enables robots to adapt to new environments, account for their specific physical capabilities, and learn from their mistakes when encountering challenging surfaces.\n\nThe core challenge in learning traversibility costs is transforming raw sensor data (visual, proprioceptive, or otherwise) into meaningful cost metrics that represent the difficulty or risk of traversing specific terrain. Modern approaches leverage the robot's own experiences - successful and unsuccessful traversal attempts - to build models that can predict traversibility costs for unseen terrain. These learned models can then be integrated into planning and navigation systems to enable safer and more efficient robot movement across diverse environments. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
          "citations": []
        },
        {
          "title": "Self-Supervised Learning Approaches",
          "text": "\nSelf-supervised learning has emerged as a dominant paradigm for acquiring terrain traversability costs automatically, eliminating the need for extensive human labeling. This approach enables robots to generate their own training data by associating sensor observations with experienced traversability costs during navigation <Paper corpusId=\"208857799\" paperTitle=\"(Zurn et al., 2019)\" isShortName></Paper>. The key insight behind these methods is using proprioceptive feedback from robot-terrain interactions as supervisory signals to train models that predict traversability from exteroceptive data like images or point clouds.\n\nOne common approach uses inertial measurement unit (IMU) data to quantify terrain difficulty. Castro et al. developed a method that learns to predict traversability costmaps by combining visual environmental information with proprioceptive feedback in a self-supervised manner, using z-axis IMU measurements as traversability scores <Paper corpusId=\"252439156\" paperTitle=\"(Castro et al., 2022)\" isShortName></Paper> <Paper corpusId=\"277824204\" paperTitle=\"(Bu et al., 2025)\" isShortName></Paper>. Similarly, TerraPN uses RGB images of terrain surfaces and robot velocities as inputs, while using IMU vibrations and odometry errors as self-supervision labels to compute surface cost maps that differentiate between smooth, high-traction surfaces and challenging terrain <Paper corpusId=\"277059508\" paperTitle=\"(Sathyamoorthy et al., 2022)\" isShortName></Paper>.\n\nOther approaches explore different feedback signals. Wellhausen et al. employed force-torque measurements from sensorized feet on a legged robot, projecting foothold positions into camera images to generate training data for terrain property prediction <Paper corpusId=\"277824204\" paperTitle=\"(Bu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"67871681\" paperTitle=\"(Wellhausen et al., 2019)\" isShortName></Paper>. Bekhti et al. proposed using acceleration sensor measurements as cost indicators, along with texture-based prediction to handle uneven terrains with varying surface characteristics <Paper corpusId=\"214296448\" paperTitle=\"(Bekhti et al., 2020)\" isShortName></Paper>.\n\nRecent advancements focus on enhancing generalization capabilities. METAVerse combines self-supervised learning with meta-learning to train a global traversability prediction model across diverse environments, generating dense cost maps from sparse LiDAR point clouds <Paper corpusId=\"261617172\" paperTitle=\"(Seo et al., 2023)\" isShortName></Paper>. This approach minimizes estimation uncertainty and enables rapid online adaptation to new environments. Schmid et al. introduced an anomaly detection method using autoencoders that learn from successfully traversed paths and evaluate traversability through reconstruction error <Paper corpusId=\"277824204\" paperTitle=\"(Bu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"251252961\" paperTitle=\"(Schmid et al., 2022)\" isShortName></Paper>.\n\nThe self-supervised paradigm offers significant advantages: it eliminates manual annotation costs, adapts to robot-specific capabilities, and continuously improves with accumulated experience. Various signal sources (inertial features, odometry errors, force-torque values) serve as effective self-supervision signals for learning traversability costmaps that can guide navigation decisions <Paper corpusId=\"259305206\" paperTitle=\"(Karnan et al., 2023)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Zurn et al., 2019)",
              "snippets": [
                "Self-supervised learning of terrain classification and terrain properties for mobile robots has been investigated intensively in recent years. Early works by Sofman et al. (Sofman et al., 2006) proposed a self-supervised online learning approach that relies on overhead imagery such as satellite images to learn a traversability costmap for outdoor off-road robots. Happold et al. [13] trained a neural network offline on hand-labeled geometric features computed from stereo data for online traversability analysis using the predictions of the trained network. Later works by Hadsell et al. [2], [14] and Konolige et al"
              ],
              "metadata": null
            },
            {
              "id": "(Castro et al., 2022)",
              "snippets": [
                "Estimating terrain traversability in off-road environments requires reasoning about complex interaction dynamics between the robot and these terrains. However, it is challenging to create informative labels to learn a model in a supervised manner for these interactions. We propose a method that learns to predict traversability costmaps by combining exteroceptive environmental information with proprioceptive terrain interaction feedback in a self-supervised manner."
              ],
              "metadata": null
            },
            {
              "id": "(Bu et al., 2025)",
              "snippets": [
                "Given the dependency of supervised learning on data annotation, self-supervised learning methods have gained significant attention, avoiding the high cost of manual annotation and utilizing the driving experience of vehicles to learn terrain traversability. These approaches acquire knowledge of environmental traversability via self-annotation. For instance, Castro et al. (Castro et al., 2022) utilized IMU z-axis measurements as traversability scores, whereas Wellhausen et al. (Wellhausen et al., 2019) applied mechanical sensors to assess the mechanical feedback from robot-terrain interaction, adopting it as a criterion for traversability analysis. Schmid et al. (Schmid et al., 2022) proposed an anomaly detection method based on an autoencoder, which calculates the loss only for trajectories successfully traversed by the vehicle, and evaluates terrain traversability through reconstruction error during inference."
              ],
              "metadata": null
            },
            {
              "id": "(Sathyamoorthy et al., 2022)",
              "snippets": [
                "We present TerraPN, a novel method that learns the surface properties (traction, bumpiness, deformability, etc.) of complex outdoor terrains directly from robot-terrain interactions through self-supervised learning, and uses it for autonomous robot navigation. Our method uses RGB images of terrain surfaces and the robot's velocities as inputs, and the IMU vibrations and odometry errors experienced by the robot as labels for self-supervision. Our method computes a surface cost map that differentiates smooth, high-traction surfaces (low navigation costs) from bumpy, slippery, deformable surfaces (high navigation costs)."
              ],
              "metadata": null
            },
            {
              "id": "(Wellhausen et al., 2019)",
              "snippets": [
                "Legged robots have the potential to traverse diverse and rugged terrain. To find a safe and efficient navigation path and to carefully select individual footholds, it is useful to be able to predict properties of the terrain ahead of the robot. In this letter, we propose a method to collect data from robot-terrain interaction and associate it to images. Using sparse data acquired in teleoperation experiments with a quadrupedal robot, we train a neural network to generate a dense prediction of the terrain properties in front of the robot. To generate training data, we project the foothold positions from the robot trajectory into on-board camera images. We then attach labels to these footholds by identifying the dominant features of the force\u2013torque signal measured with sensorized feet. We show that data collected in this fashion can be used to train a convolutional network for terrain property prediction as well as weakly supervised semantic segmentation. Finally, we show that the predicted terrain properties can be used for autonomous navigation of the ANYmal quadruped robot."
              ],
              "metadata": null
            },
            {
              "id": "(Bekhti et al., 2020)",
              "snippets": [
                "In this paper, we propose a framework to enable mobile robots to autonomously learn terrain traversability using only their sensors in a self-supervised manner. Terrain traversability is predicted as a cost measured by an acceleration sensor equipped by the mobile robot. Focusing on the usage of image features for terrains with small-scale unevenness, a texture-based prediction of traversability was proposed in (Bekhti et al., 2015). However, the applicability of the texture information in 2D images has not been sufficiently investigated because, in natural environments, terrains are not always uniform and often are non-uniform, therein containing spiky materials such as relatively large stones and roots of trees. This non-uniformity causes traversability cost prediction to be challenging. In this paper, a detection of such non-uniformity in terrains is proposed based on multiscale local image features. It is shown that we can improve the prediction performance prediction performance of the texture-based approach. An advantage of the texture-based approach is that the sensor is affordable and can still detect motion features of the traversing robot without high-cost 3D sensing of the terrain geometry."
              ],
              "metadata": null
            },
            {
              "id": "(Seo et al., 2023)",
              "snippets": [
                "This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences."
              ],
              "metadata": null
            },
            {
              "id": "(Schmid et al., 2022)",
              "snippets": [
                "Navigating off-road with a fast autonomous vehicle depends on a robust perception system that differentiates traversable from non-traversable terrain. Typically, this depends on a semantic understanding which is based on supervised learning from images annotated by a human expert. This requires a significant investment in human time, assumes correct expert classification, and small details can lead to misclassification. To address these challenges, we propose a method for predicting high- and low-risk terrains from only past vehicle experience in a self-supervised fashion. First, we develop a tool that projects the vehicle trajectory into the front camera image. Second, occlusions in the 3D representation of the terrain are filtered out. Third, an autoencoder trained on masked vehicle trajectory regions identifies low- and high-risk terrains based on the reconstruction error. We evaluated our approach with two models and different bottleneck sizes with two different training and testing sites with a four-wheeled off-road vehicle. Comparison with two independent test sets of semantic labels from similar terrain as training sites demonstrates the ability to separate the ground as low-risk and the vegetation as high-risk with 81.1% and 85.1% accuracy."
              ],
              "metadata": null
            },
            {
              "id": "(Karnan et al., 2023)",
              "snippets": [
                "To alleviate the need for extensive human labeling, self-supervised learning methods have been proposed to either learn terrain representations or costs from data gathered onboard a mobile robot.\n\nCost Learning: Several methods have applied self-supervision to assign traversability costs for the downstream off-road navigation task [7,38,26,39,40,41,42]. Specifically, these methods rely on inertial spectral features [7], future predictive models [26], inertial-odometry errors [38], or forcetorque values from foothold positions [39,43] as self-supervision signals to learn a traversability cost map, used to evaluate candidate actions. More recently, Frey et al. [44] have proposed an online traversability estimation approach inspired by the above self-supervision schemes."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Learning from Demonstration Methods",
          "text": "\nLearning from Demonstration (LfD) represents a powerful paradigm for teaching robots to navigate complex terrains by leveraging human expertise. Unlike self-supervised approaches that learn from the robot's own experiences, LfD methods extract traversability knowledge directly from human demonstrations of successful navigation behaviors <Paper corpusId=\"113688878\" paperTitle=\"(Brock et al., 2009)\" isShortName></Paper> <Paper corpusId=\"14305634\" paperTitle=\"(Silver et al., 2008)\" isShortName></Paper>.\n\nThe core principle behind LfD is to develop a function that maps environmental sensor data to traversal costs such that when these costs are used by a planner, the resulting robot behavior closely mimics expert demonstrations <Paper corpusId=\"113688878\" paperTitle=\"(Brock et al., 2009)\" isShortName></Paper> <Paper corpusId=\"14305634\" paperTitle=\"(Silver et al., 2008)\" isShortName></Paper>. This approach has become widely used to transfer human navigational expertise to robots, enabling them to achieve human-level performance in navigating challenging environments <Paper corpusId=\"230435654\" paperTitle=\"(Siva et al., 2021)\" isShortName></Paper>.\n\nSeveral implementations of LfD have shown promising results in terrain traversability learning. For instance, Ollis et al. combined Bayesian estimates with geometric information from stereo vision to calculate terrain costs that indicate obstacle presence probability, determining traversability based on feature values <Paper corpusId=\"248377546\" paperTitle=\"(Sevastopoulos et al., 2022)\" isShortName></Paper>. Similarly, Suger et al. generated training data from safe journeys guided by human operators and applied a Positive Naive Bayes classifier to estimate traversability probabilities without presuming correlations between features and traversability <Paper corpusId=\"248377546\" paperTitle=\"(Sevastopoulos et al., 2022)\" isShortName></Paper>.\n\nMore recent approaches have integrated LfD with other learning techniques. Siva et al. developed a unified framework that combines representation learning with imitation learning to generate appropriate motion on different types of outdoor unstructured terrain <Paper corpusId=\"247159052\" paperTitle=\"(Xiao et al., 2020)\" isShortName></Paper> <Paper corpusId=\"197678996\" paperTitle=\"(Siva et al., 2019)\" isShortName></Paper>. Their approach integrates representation and apprenticeship learning under a regularized optimization framework that automatically identifies discriminative feature modalities, improving the robustness of robot adaptation <Paper corpusId=\"197678996\" paperTitle=\"(Siva et al., 2019)\" isShortName></Paper>.\n\nInverse Reinforcement Learning (IRL) has also been applied within the LfD framework. Wigness et al. used IRL to learn cost functions that enable human-like outdoor navigation behaviors, such as navigating on roads while staying close to grass or avoiding open areas <Paper corpusId=\"247159052\" paperTitle=\"(Xiao et al., 2020)\" isShortName></Paper> <Paper corpusId=\"52029494\" paperTitle=\"(Wigness et al., 2018)\" isShortName></Paper>. Their approach combines learned visual perception with inverse optimal control trained on minimal human supervisory examples, allowing fast field adaptation in dynamic environments <Paper corpusId=\"52029494\" paperTitle=\"(Wigness et al., 2018)\" isShortName></Paper>.\n\nLfD approaches have proven particularly valuable for autonomous mobile robots deployed in off-road field environments for applications like disaster response, infrastructure inspection, and planetary exploration <Paper corpusId=\"251105215\" paperTitle=\"(Siva et al., 2022)\" isShortName></Paper> <Paper corpusId=\"114175564\" paperTitle=\"(Lattanzi et al., 2017)\" isShortName></Paper> <Paper corpusId=\"54867483\" paperTitle=\"(Schuster et al., 2019)\" isShortName></Paper>. By imitating expert demonstrators, these methods enable robots to navigate safely through environments where traditional geometric approaches might fail to correctly assess traversability.",
          "citations": [
            {
              "id": "(Brock et al., 2009)",
              "snippets": [
                "Recently, a class of machine learning techniques have been developed that rely upon expert human demonstration to develop a function mapping overhead data to traversal cost. These algorithms choose the cost function so that planner behavior mimics an expert's demonstration as closely as possible."
              ],
              "metadata": null
            },
            {
              "id": "(Silver et al., 2008)",
              "snippets": [
                "Recently, a class of machine learning techniques have been developed that rely upon expert human demonstration to develop a function mapping overhead data to traversal cost. These algorithms choose the cost function so that planner behavior mimics an expert's demonstration as closely as possible."
              ],
              "metadata": null
            },
            {
              "id": "(Siva et al., 2021)",
              "snippets": [
                "Previous learning-based methods can be divided into two broad categories: terrain classification and terrain adaptation. The first category of approaches apply a robot's exteroceptive and proprioceptive sensory data to classify terrain types and estimate traversability for a robot to navigate over the terrain (Bermudez et al., 2012), (Brooks et al., 2005), (Devjanin et al., 1983), (DuPont et al., 2008), (Hudjakov et al., 2009). This category also includes methods that model terrain complexity for planning robot navigation tasks (Silver et al., 2010), (Peynot et al., 2014). The second category of methods focus on directly generating adaptive navigational behaviors according to terrain in order to successfully complete navigation tasks (Silver et al., 2010), (Han et al., 2017), (Pastor et al., 2009), (Wigness et al., 2018). Specifically, learning from demonstration (LfD) is widely used to transfer human expertise to robots in order to achieve human-level robot navigational control [4], [13], [14]."
              ],
              "metadata": null
            },
            {
              "id": "(Sevastopoulos et al., 2022)",
              "snippets": [
                "In the work of Ollis et al. (Ollis et al., 2007) the robot learns from human demonstration to calculate terrain costs, which indicate the probability of an obstacle's presence. Through the combination of using Bayesian estimates and geometric information collected by stereo vision, the final terrain costs follow a certain distribution and thus it can be determined whether the path is traversable by articulating that those cells with higher values of features ought to be less traversable. Using the human interference in a similar manner but excluding any presuming correlation between the features and the traversability, in (Suger et al., 2015) training data is generated throughout the safe journey that the human operator drives the robot throughout. Thereupon, the notion of learning is addressed with the application of the Positive Naive Bayes (NB) classifier that estimates the frequencies of observed features by finding the parameters of the probability distribution for the traversability."
              ],
              "metadata": null
            },
            {
              "id": "(Xiao et al., 2020)",
              "snippets": [
                "For example, Siva, et al. (Siva et al., 2019) used a human demonstration of navigation on different types of outdoor unstructured terrain, and then combined representation learning and IL to generate appropriate motion on specific terrain. Using IRL, Wigness, et al. (Wigness et al., 2018) learned cost functions to enable outdoor human-like navigational behaviors, such as navigating on a road but staying close to grass, or keeping out of more visible and open areas. Another approach for terrain-based navigation leveraged real interactions with the physical world to discover non-geometric features for navigation (Kahn et al., 2020): traversing over tall grass has very low cost, while the cost of going through uneven terrain is relatively high. The learned inverse kindodynamics model conditioned on inertia embeddings by Xiao,et al. [100] was able to capture unknown world states and therefore enable accurate, high-speed, off-road navigation."
              ],
              "metadata": null
            },
            {
              "id": "(Siva et al., 2019)",
              "snippets": [
                "\u2014When a mobile robot is deployed in a \ufb01eld environment, e.g., during a disaster response application, the capability of adapting its navigational behaviors to unstructured terrains is essential for effective and safe robot navigation. In this paper, we introduce a novel joint terrain representation and apprenticeship learning approach to implement robot adaptation to unstructured terrains. Different from conventional learning-based adaptation techniques, our approach provides a uni\ufb01ed problem formulation that integrates representation and apprenticeship learning under a uni\ufb01ed regularized optimization framework, instead of treating them as separate and independent procedures. Our approach also has the capability to automatically identify discriminative feature modalities, which can improve the robustness of robot adaptation. In addition, we implement a new optimization algorithm to solve the formulated problem, which provides a theoretical guarantee to converge to the global optimal solution. In the experiments, we extensively evaluate the proposed approach in real-world scenarios, in which a mobile robot navigates on familiar and unfamiliar unstructured terrains. Experimental results have shown that the proposed approach is able to transfer human expertise to robots with small errors, achieve superior performance compared with previous and baseline methods, and provide intuitive insights on the importance of terrain feature modalities."
              ],
              "metadata": null
            },
            {
              "id": "(Wigness et al., 2018)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Siva et al., 2022)",
              "snippets": [
                "In recent years, autonomous mobile robots have been increasingly deployed in off-road field environments to carry out tasks related to disaster response, infrastructure inspection, and subterranean and planetary exploration (Lattanzi et al., 2017), (Schuster et al., 2019), (Chiang et al., 2019)",
                "In general, terrain adaptation has been addressed using three broad categories of methods. The first category, classic control-based methods, use mathematical tools from control theory [6], [7], [8] to design robot models that achieve the desired robot behavior and perform robust ground maneuvers in various environments. The second category, learning-based methods, use data-driven formulations to either imitate an expert demonstrator [5], [9], [10], learn from trialand-error in a reinforcement learning setting [11], [12], [13], or use online learning to continuously learn and adapt in an environment [14], [15], [16]. Finally, the third category, machine-learning-based control, exploits the advantage of integrating machine learning into control theory to learn accurate robot dynamics and accordingly adapt navigational behaviors [17], [18], [19]."
              ],
              "metadata": null
            },
            {
              "id": "(Lattanzi et al., 2017)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Schuster et al., 2019)",
              "snippets": [
                "Planetary exploration poses many challenges for a robot system: From weight and size constraints to extraterrestrial environment conditions, which constrain the suitable sensors and actuators. As the distance to other planets introduces a significant communication delay, the efficient operation of a robot system requires a high level of autonomy. In this work, we present our Lightweight Rover Unit (LRU), a small and agile rover prototype that we designed for the challenges of planetary exploration. Its locomotion system with individually steered wheels allows for high maneuverability in rough terrain and stereo cameras as its main sensors ensure the applicability to space missions. We implemented software components for self-localization in GPS-denied environments, autonomous exploration and mapping as well as computer vision, planning and control modules for the autonomous localization, pickup and assembly of objects with its manipulator. Additional high-level mission control components facilitate both autonomous behavior and remote monitoring of the system state over a delayed communication link. We successfully demonstrated the autonomous capabilities of our LRU at the SpaceBotCamp challenge, a national robotics contest with focus on autonomous planetary exploration. A robot had to autonomously explore an unknown Moon-like rough terrain, locate and collect two objects and assemble them after transport to a third object \u2013 which the LRU did on its first try, in half of the time and fully autonomously. The next milestone for our ongoing LRU development is an upcoming planetary exploration analogue mission to perform scientific experiments at a Moon analogue site located on a volcano."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Inverse Reinforcement Learning Techniques",
          "text": "\nInverse Reinforcement Learning (IRL) has emerged as a powerful approach for terrain traversability modeling, particularly for autonomous vehicles operating in complex environments. Unlike methods that rely on predefined metrics or manual feature engineering, IRL aims to learn traversability costs directly from navigation behaviors, eliminating the need for explicit cost function design <Paper corpusId=\"250292279\" paperTitle=\"(Gan et al., 2022)\" isShortName></Paper>.\n\nThe Maximum Entropy Deep IRL (MEDIRL) framework has been particularly successful in this domain, exploiting the representational capacity of neural networks for reward function approximation <Paper corpusId=\"250292279\" paperTitle=\"(Gan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"2996148\" paperTitle=\"(Wulfmeier et al., 2017)\" isShortName></Paper>. Wulfmeier et al. demonstrated that this approach can directly map from raw input data to traversability costs, bypassing manual pipeline design while effectively handling complex urban environments with over 25,000 demonstration trajectories extracted from more than 120 km of urban driving <Paper corpusId=\"2996148\" paperTitle=\"(Wulfmeier et al., 2017)\" isShortName></Paper>. Their results showed that IRL-based methods produce cost representations with better robustness and scalability than predetermined costs or those learned through supervised methods <Paper corpusId=\"250292279\" paperTitle=\"(Gan et al., 2022)\" isShortName></Paper>.\n\nFor off-road environments, Zhu et al. proposed a method using Deep Maximum Entropy IRL that incorporates vehicle kinematics into convolutional neural networks. Their system generates scene maps using 3D LiDAR data and learns cost functions from either real driving trajectories or synthesized demonstrations representing specific behaviors like crossing negative obstacles <Paper corpusId=\"250292279\" paperTitle=\"(Gan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"202578026\" paperTitle=\"(Zhu et al., 2019)\" isShortName></Paper>.\n\nRecent advances have extended IRL approaches to incorporate multiple contextual factors. Jung et al. developed a system that predicts traversability maps while considering inertial, environmental, and social contexts without requiring expensive prior information like HD maps. Their method uses a novel deep neural network to extract contextual cues from sensor data and incorporates them into a reward map that represents the probability distribution of plausible and socially acceptable trajectories <Paper corpusId=\"250292279\" paperTitle=\"(Gan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"232236704\" paperTitle=\"(Jung et al., 2021)\" isShortName></Paper>.\n\nOther notable developments include TerrainNet by Meng et al., which learns a cost function for traversing diverse and uneven surfaces. Their approach uses a multi-headed output representation to capture fine and coarse-grained terrain features necessary for estimating traversability, achieving accurate depth estimation through self-supervised depth completion with multi-view RGB and stereo inputs <Paper corpusId=\"272705831\" paperTitle=\"(Achterhold et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257771816\" paperTitle=\"(Meng et al., 2023)\" isShortName></Paper>.\n\nWhat distinguishes IRL techniques from other methods is their ability to learn comprehensive cost functions directly from demonstrations while maintaining real-time performance through efficient network architectures. This makes them particularly valuable for autonomous navigation in complex, unstructured environments where traditional approaches might struggle.",
          "citations": [
            {
              "id": "(Gan et al., 2022)",
              "snippets": [
                "Recently, Inverse Reinforcement Learning (IRL) for terrain traversability modeling has received increased interests, particularly for autonomous vehicles (Wulfmeier et al., 2017)[15](Zhu et al., 2019)(Jung et al., 2021). Instead of using a predetermined metric to define traversability cost and associating it with terrain features, IRL-based methods aim to learn this cost directly from a robot's (traversing) behaviors. Doing so bypasses the effort and incompleteness of manually designing the cost function and parts of the learning pipeline. The resulting cost representations of IRL-based methods have also shown better robustness and scalability than the predetermined cost and the cost learned in supervised methods (Wulfmeier et al., 2017)[18]",
                "Maximum Entropy Deep IRL (MEDIRL) exploits the representational capacity of neural networks for reward function approximation and has been successfully applied to traversability cost learning and trajectory forecasting for autonomous vehicles [14-17, 19, 20]."
              ],
              "metadata": null
            },
            {
              "id": "(Wulfmeier et al., 2017)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Zhu et al., 2019)",
              "snippets": [
                "Terrain traversability analysis is a fundamental issue to achieve the autonomy of a robot at off-road environments. Geometry-based and appearance-based methods have been studied in decades, while behavior-based methods exploiting learning from demonstration (LfD) are new trends. Behavior-based methods learn cost functions that guide trajectory planning in compliance with experts' demonstrations, which can be more scalable to various scenes and driving behaviors. This research proposes a method of off-road traversability analysis and trajectory planning using Deep Maximum Entropy Inverse Reinforcement Learning. To incorporate the vehicle's kinematics while solving the problem of exponential increase of state-space complexity, two convolutional neural networks, i.e., RL ConvNet and Svf ConvNet, are developed to encode kinematics into convolution kernels and achieve efficient forward reinforcement learning. We conduct experiments in off-road environments. Scene maps are generated using 3D LiDAR data, and expert demonstrations are either the vehicle's real driving trajectories at the scene or synthesized ones to represent specific behaviors such as crossing negative obstacles. Different cost functions of traversability analysis are learned and tested at various scenes of capability in guiding the trajectory planning of different behaviors. We also demonstrate the peformance and computation efficiency of the proposed method."
              ],
              "metadata": null
            },
            {
              "id": "(Jung et al., 2021)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Achterhold et al., 2024)",
              "snippets": [
                "In Zhu et al. [5] the terrain traversal costs are learned from human demonstrations by inverse reinforcement learning. TerrainNet (Meng et al., 2023) learns a cost function for traversing diverse and uneven surfaces. These methods, however, do not learn the dynamical properties of the robot on the terrain classes explicitly like our method."
              ],
              "metadata": null
            },
            {
              "id": "(Meng et al., 2023)",
              "snippets": [
                "Effective use of camera-based vision systems is essential for robust performance in autonomous off-road driving, particularly in the high-speed regime. Despite success in structured, on-road settings, current end-to-end approaches for scene prediction have yet to be successfully adapted for complex outdoor terrain. To this end, we present TerrainNet, a vision-based terrain perception system for semantic and geometric terrain prediction for aggressive, off-road navigation. The approach relies on several key insights and practical considerations for achieving reliable terrain modeling. The network includes a multi-headed output representation to capture fine- and coarse-grained terrain features necessary for estimating traversability. Accurate depth estimation is achieved using self-supervised depth completion with multi-view RGB and stereo inputs. Requirements for real-time performance and fast inference speeds are met using efficient, learned image feature projections. Furthermore, the model is trained on a large-scale, real-world off-road dataset collected across a variety of diverse outdoor environments. We show how TerrainNet can also be used for costmap prediction and provide a detailed framework for integration into a planning module. We demonstrate the performance of TerrainNet through extensive comparison to current state-of-the-art baselines for camera-only scene prediction. Finally, we showcase the effectiveness of integrating TerrainNet within a complete autonomous-driving stack by conducting a real-world vehicle test in a challenging off-road scenario."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Proprioceptive Sensor-Based Approaches",
          "text": "\nProprioceptive sensor-based approaches leverage the robot's internal sensing capabilities to learn terrain traversability directly from physical interactions with the environment. Unlike methods that rely primarily on visual or geometric features, these approaches capture the actual effects of terrain on robot mobility, providing direct feedback on traversability costs.\n\nA common approach is to use acceleration and inertial measurement unit (IMU) data to quantify terrain difficulty. TerraPN, for example, uses IMU vibrations and odometry errors experienced by the robot as self-supervision labels to compute surface cost maps that differentiate between smooth, high-traction surfaces and challenging terrain <Paper corpusId=\"277059508\" paperTitle=\"(Sathyamoorthy et al., 2022)\" isShortName></Paper>. Similarly, Castro et al. developed a method that combines exteroceptive environmental information with proprioceptive feedback in a self-supervised manner, using z-axis IMU measurements as traversability scores <Paper corpusId=\"252439156\" paperTitle=\"(Castro et al., 2022)\" isShortName></Paper> <Paper corpusId=\"272367062\" paperTitle=\"(Endo et al., 2024)\" isShortName></Paper>.\n\nOther approaches use different proprioceptive signals as traversability indicators. Bekhti et al. proposed using acceleration sensor measurements as cost indicators, along with texture-based prediction to handle uneven terrains with varying surface characteristics <Paper corpusId=\"214296448\" paperTitle=\"(Bekhti et al., 2020)\" isShortName></Paper>. Wellhausen et al. employed force-torque measurements from sensorized feet on a legged robot, projecting foothold positions into camera images to generate training data for terrain property prediction <Paper corpusId=\"67871681\" paperTitle=\"(Wellhausen et al., 2019)\" isShortName></Paper> <Paper corpusId=\"272367062\" paperTitle=\"(Endo et al., 2024)\" isShortName></Paper>. Espino et al. demonstrated a system that uses measures of energy expenditure by wheels as one of three cost factors for adaptive path planning <Paper corpusId=\"269330151\" paperTitle=\"(Espino et al., 2024)\" isShortName></Paper>.\n\nRecent advancements include more sophisticated combinations of exteroceptive and proprioceptive information. Zhu et al. proposed a learning-based method that uses semantic and geometric features along with robot velocity as inputs for learning a continuous cost supervised by processed IMU data, combining CNN and LSTM architectures to handle the concatenated features <Paper corpusId=\"270391944\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. Margolis et al. introduced Active Sensing Motor Policies (ASMP), which are trained to explore locomotion behaviors that increase the accuracy of estimating physical parameters, such as having a quadruped robot swipe its foot against the ground to estimate friction coefficient <Paper corpusId=\"264935029\" paperTitle=\"(Margolis et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nControl performance metrics also serve as effective proprioceptive indicators for traversability estimation. These include tracking errors of velocity commands and IMU variance, which can be used as quantitative metrics for supervised learning of environment representations <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Other approaches predict traversal time, specific environmental physics parameters, or collision and fall indicators from the robot's proprioceptive feedback history <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nThese proprioceptive sensor-based approaches offer significant advantages by adapting to robot-specific physical capabilities and continuously improving with accumulated experience. The direct use of the robot's own sensing capabilities provides a more accurate representation of terrain traversability specific to a particular robot's embodiment, rather than relying on human-engineered or generic metrics <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252368380\" paperTitle=\"(Lee et al., 2022)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Sathyamoorthy et al., 2022)",
              "snippets": [
                "We present TerraPN, a novel method that learns the surface properties (traction, bumpiness, deformability, etc.) of complex outdoor terrains directly from robot-terrain interactions through self-supervised learning, and uses it for autonomous robot navigation. Our method uses RGB images of terrain surfaces and the robot's velocities as inputs, and the IMU vibrations and odometry errors experienced by the robot as labels for self-supervision. Our method computes a surface cost map that differentiates smooth, high-traction surfaces (low navigation costs) from bumpy, slippery, deformable surfaces (high navigation costs)."
              ],
              "metadata": null
            },
            {
              "id": "(Castro et al., 2022)",
              "snippets": [
                "Estimating terrain traversability in off-road environments requires reasoning about complex interaction dynamics between the robot and these terrains. However, it is challenging to create informative labels to learn a model in a supervised manner for these interactions. We propose a method that learns to predict traversability costmaps by combining exteroceptive environmental information with proprioceptive terrain interaction feedback in a self-supervised manner."
              ],
              "metadata": null
            },
            {
              "id": "(Endo et al., 2024)",
              "snippets": [
                "ML-based traversability prediction is a key component of off-road autonomy, where visual and geometric cues are essential. Visual cues capture the diverse geological semantic features that influence robot behavior. Thus, classification methods use the semantic segmentation of appearance imagery to categorize symbolic terrain types and identify traversable areas (Filitchkin et al., 2012), (Rothrock et al., 2016). This appearance-based classification is often combined with geometry-based regression methods to provide further variation in traversability within each class. Classify-then-regress methods model the traversability for each terrain class by correlating the degree of wheel slip with the terrain inclination (Cunningham et al., 2017)- (Inotsume et al., 2021). Mixture-of-experts (MoE) methods further account for terrain class likelihoods for slip regressors (Angelova et al., 2007), (Endo et al., 2023). Although these combined approaches using visual and geometric cues are effective, they require separate training processes for the classifier and regressor models, which involve human supervision. \n\nUsing the measured traverse data for supervision, learning traversability from experience has become a viable alternative. This approach provides a direct interpretation of robot-terrain interactions and eliminates the need for laborintensive manual annotation. Previous studies often used past driving trajectories to auto-generate ground-truth traversability, which assumes scene familiarity with robots (Schmid et al., 2022)- (Jung et al., 2023). Proprioceptive signals, such as force-torque (Wellhausen et al., 2019) and linear acceleration (Castro et al., 2022), define traversability in continuous-valued regression models. The absence of manual annotation also facilitates model adaptation through incremental learning with in-situ measurements, improving the robustness to novel situations [20], (Frey et al., 2023)."
              ],
              "metadata": null
            },
            {
              "id": "(Bekhti et al., 2020)",
              "snippets": [
                "In this paper, we propose a framework to enable mobile robots to autonomously learn terrain traversability using only their sensors in a self-supervised manner. Terrain traversability is predicted as a cost measured by an acceleration sensor equipped by the mobile robot. Focusing on the usage of image features for terrains with small-scale unevenness, a texture-based prediction of traversability was proposed in (Bekhti et al., 2015). However, the applicability of the texture information in 2D images has not been sufficiently investigated because, in natural environments, terrains are not always uniform and often are non-uniform, therein containing spiky materials such as relatively large stones and roots of trees. This non-uniformity causes traversability cost prediction to be challenging. In this paper, a detection of such non-uniformity in terrains is proposed based on multiscale local image features. It is shown that we can improve the prediction performance prediction performance of the texture-based approach. An advantage of the texture-based approach is that the sensor is affordable and can still detect motion features of the traversing robot without high-cost 3D sensing of the terrain geometry."
              ],
              "metadata": null
            },
            {
              "id": "(Wellhausen et al., 2019)",
              "snippets": [
                "Legged robots have the potential to traverse diverse and rugged terrain. To find a safe and efficient navigation path and to carefully select individual footholds, it is useful to be able to predict properties of the terrain ahead of the robot. In this letter, we propose a method to collect data from robot-terrain interaction and associate it to images. Using sparse data acquired in teleoperation experiments with a quadrupedal robot, we train a neural network to generate a dense prediction of the terrain properties in front of the robot. To generate training data, we project the foothold positions from the robot trajectory into on-board camera images. We then attach labels to these footholds by identifying the dominant features of the force\u2013torque signal measured with sensorized feet. We show that data collected in this fashion can be used to train a convolutional network for terrain property prediction as well as weakly supervised semantic segmentation. Finally, we show that the predicted terrain properties can be used for autonomous navigation of the ANYmal quadruped robot."
              ],
              "metadata": null
            },
            {
              "id": "(Espino et al., 2024)",
              "snippets": [
                "We present a neurorobotic navigation system that utilizes a Spiking Neural Network (SNN) Wavefront Planner and E-prop learning to concurrently map and plan paths in a large and complex environment. We incorporate a novel method for mapping which, when combined with the Spiking Wavefront Planner (SWP), allows for adaptive planning by selectively considering any combination of costs. The system is tested on a mobile robot platform in an outdoor environment with obstacles and varying terrain. Results indicate that the system is capable of discerning features in the environment using three measures of cost, (1) energy expenditure by the wheels, (2) time spent in the presence of obstacles, and (3) terrain slope. In just twelve hours of online training, E-prop learns and incorporates traversal costs into the path planning maps by updating the delays in the SWP."
              ],
              "metadata": null
            },
            {
              "id": "(Zhu et al., 2024)",
              "snippets": [
                "We propose a learning-based method to predict traversability costmaps that present risk-aware influence of the terrains with respect to the robot navigation. Exteroceptive information including semantic and geometric features and interoceptive information as the robot velocity are the inputs for learning a continuous cost which is supervised by processed IMU data. We combine a Convolutional Neural Network (CNN) backbone to extract features from exteroceptive information, a neural network to process interoceptive information and a Long Short-Term Memory (LSTM) to handle the concatenate features as our learning architecture."
              ],
              "metadata": null
            },
            {
              "id": "(Margolis et al., 2023)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al., 2024)",
              "snippets": [
                "To estimate the terrain traversability cost, some previous work directly extracts features from the environment. These methods usually define traversability by empirically judging the cost from meshes [4], point clouds [5], or depth images [6]. Others try to classify the terrains from semantic categories [7][8][9], often mapping semantic features to traversability cost based on human-provided labels. These methods typically rely on human prior knowledge rather than the robot's own experiences [10]. While these estimators are robot-agnostic and easily transferable across different embodiments, they fail to consider the robot's real interaction experiences. For example, some terrains may be difficult for lightweight robots but manageable for heavier ones, and vice versa.\n\nTo consider the robot's experience while traversing different terrains, previous work has suggested various metrics. The most straightforward approach is to consider control performance metrics such as tracking error of commands (e.g., velocity) [11] or IMU variance [12]. These quantitative metrics are used as labels for supervised learning of environment representations. Other approaches extend this research by learning estimators to predict traversal time [13], selected environment physics parameters [14], or collision and fall indicators [15] from a history of robot's proprioception feedback."
              ],
              "metadata": null
            },
            {
              "id": "(Lee et al., 2022)",
              "snippets": [
                "Recent learning-based methods can avoid such heuristics as they exploit the collected data to learn the traversability cost instead of manual cost assignment [11]- (Gasparino et al., 2022), (Nardi et al., 2019), (Waibel et al., 2022). In (Manderson et al., 2020), (Nardi et al., 2019), (Waibel et al., 2022), the roughness of the terrains is learned based on the sensor data, and they plan a smoother path for the vehicle. In [12], a neural network represents traversability as the achievable maximum speed distribution of vehicles on different terrains. Moreover, end-to-end learning approaches have been proposed to avoid geometric obstacles as well as untraversable terrains by analyzing the executed commands and the realized trajectories [11], [13]."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Meta-Learning and Online Adaptation",
          "text": "\nMeta-learning has emerged as a powerful approach for developing terrain traversability models that can quickly adapt to new environments. Unlike traditional learning methods that treat each environment independently, meta-learning frameworks aim to extract generalizable knowledge across diverse terrains that can be rapidly fine-tuned during deployment. METAVerse exemplifies this approach, training a global model to predict terrain traversability from sparse LiDAR point clouds across multiple environments while minimizing estimation uncertainty. During deployment, the system performs online adaptation by exploiting recent interaction experiences to quickly adjust to local terrain conditions. <Paper corpusId=\"261617172\" paperTitle=\"(Seo et al., 2023)\" isShortName></Paper>\n\nModel-Agnostic Meta-Learning (MAML) provides a particularly effective foundation for online adaptation of traversability models. This technique finds initial network parameters that enable rapid adaptation through just a few gradient descent steps, allowing the model to generalize effectively to current circumstances. Seo et al. implemented this approach to incorporate driving data collected across various environments for learning traverse costs from geometric properties without confusion between different terrain types. Their system trains on meta-data from past timesteps to predict traverse costs for future timesteps, enabling the model to adapt online during deployment in a self-supervised manner, without requiring manual labeling. <Paper corpusId=\"259252467\" paperTitle=\"(Seo et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"6719686\" paperTitle=\"(Finn et al., 2017)\" isShortName></Paper>\n\nWhat distinguishes meta-learning approaches from other traversability learning methods is their explicit focus on rapid adaptation. By training networks to be \"easy to fine-tune,\" these systems can quickly incorporate new experiences encountered during deployment to improve their traversability predictions for the current environment. This capability is particularly valuable for robots operating in highly variable or previously unseen terrains, where pre-trained models alone might struggle to accurately predict traversability costs. The combination of self-supervised learning (for generating training data automatically) with meta-learning (for enabling rapid adaptation) creates systems that continuously improve their traversability models through accumulated experience while maintaining robust performance across diverse environments. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
          "citations": [
            {
              "id": "(Seo et al., 2023)",
              "snippets": [
                "This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences."
              ],
              "metadata": null
            },
            {
              "id": "(Seo et al._1, 2023)",
              "snippets": [
                "We use Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) for online adaptation of the traversability model. It aims to find the initial parameters of the network so that adaptation with a few gradient descent steps from this initialization leads to effective generalization to current circumstances. By doing so, the model can incorporate driving data collected in various environments for learning traverse cost from geometric properties without confusion",
                "The traversability model is trained to adapt using the meta-train data of M past timesteps, \u03c4 (t \u2212 M, t), to predict the traverse cost of the next K timesteps, \u03c4 (t, t + K), as follows:",
                "the model can online-adapt during the deployment phase using automatically generated meta-train data in a self-supervised manner."
              ],
              "metadata": null
            },
            {
              "id": "(Finn et al., 2017)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Neural Network Architectures for Traversibility Learning",
          "text": "\nNeural network architectures for terrain traversability learning have expanded significantly in recent years, with diverse approaches tailored to the unique challenges of robotics. A key development is the integration of convolutional neural networks (CNNs) with recurrent neural networks to handle multi-modal input data. Zhu et al. proposed an architecture that combines a CNN backbone to extract features from exteroceptive information (semantic and geometric features) with a separate neural network for processing interoceptive information (robot velocity), and then uses a Long Short-Term Memory (LSTM) network to handle the concatenated features <Paper corpusId=\"270391944\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This architecture enables the prediction of continuous traversability costs supervised by processed IMU data, creating a more comprehensive model of terrain traversability.\n\nFor handling uncertainty in traversability prediction, Fan et al. introduced a neural network architecture focused on robustly learning the distribution of traversability costs rather than point estimates. Their approach addresses the problem from a risk-aware perspective, specifically learning tail-risks using conditional value-at-risk (CVaR) to preserve robot safety in potentially hazardous terrain <Paper corpusId=\"236429026\" paperTitle=\"(Fan et al., 2021)\" isShortName></Paper>. This represents a significant advancement over classical geometric analysis approaches, which can be prone to modeling errors and computationally expensive.\n\nDeep learning approaches for traversability have evolved to combine visual and geometric cues in increasingly sophisticated ways. Early methods used semantic segmentation to categorize terrain types and identify traversable areas <Paper corpusId=\"131853020\" paperTitle=\"(Rothrock et al., 2016)\" isShortName></Paper>, often followed by geometry-based regression to provide variation in traversability within each class <Paper corpusId=\"245319283\" paperTitle=\"(Inotsume et al., 2021)\" isShortName></Paper>. More recent approaches have moved beyond this \"classify-then-regress\" paradigm toward end-to-end learning that directly maps sensor inputs to traversability costs <Paper corpusId=\"272367062\" paperTitle=\"(Endo et al., 2024)\" isShortName></Paper>.\n\nSelf-supervised learning approaches have become increasingly popular for neural network training in this domain. These methods leverage the robot's past experiences to generate training data automatically, eliminating the need for labor-intensive manual annotation. Schmid et al. developed an autoencoder trained on masked vehicle trajectory regions to identify low- and high-risk terrains based on reconstruction error <Paper corpusId=\"251252961\" paperTitle=\"(Schmid et al., 2022)\" isShortName></Paper>. Other approaches use proprioceptive signals like force-torque measurements <Paper corpusId=\"67871681\" paperTitle=\"(Wellhausen et al., 2019)\" isShortName></Paper> and linear acceleration <Paper corpusId=\"252439156\" paperTitle=\"(Castro et al., 2022)\" isShortName></Paper> to define traversability in continuous-valued regression models.\n\nRecent innovations include specialized neural architectures like the Spiking Neural Network (SNN) Wavefront Planner proposed by Espino et al. This system combines an SNN with E-prop learning to concurrently map and plan paths in complex environments. The architecture incorporates a novel mapping method that allows for adaptive planning by considering multiple cost factors: energy expenditure by wheels, time spent near obstacles, and terrain slope. Their implementation demonstrated effective learning of traversal costs in just twelve hours of online training <Paper corpusId=\"269330151\" paperTitle=\"(Espino et al., 2024)\" isShortName></Paper>.\n\nSome of the most recent advances leverage pre-trained foundation models to improve generalization. Jung et al. introduced a self-supervised learning method for traversability prediction that employs contrastive representation learning with a state-of-the-art vision foundation model, demonstrating impressive out-of-distribution performance and generalization capabilities <Paper corpusId=\"266551467\" paperTitle=\"(Jung et al., 2023)\" isShortName></Paper>. Similarly, Frey et al. proposed a system called Wild Visual Navigation that leverages high-dimensional features from self-supervised visual transformer models with an online supervision generation scheme, allowing robots to navigate in complex outdoor terrains with minimal training time <Paper corpusId=\"258685355\" paperTitle=\"(Frey et al., 2023)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Zhu et al., 2024)",
              "snippets": [
                "We propose a learning-based method to predict traversability costmaps that present risk-aware influence of the terrains with respect to the robot navigation. Exteroceptive information including semantic and geometric features and interoceptive information as the robot velocity are the inputs for learning a continuous cost which is supervised by processed IMU data. We combine a Convolutional Neural Network (CNN) backbone to extract features from exteroceptive information, a neural network to process interoceptive information and a Long Short-Term Memory (LSTM) to handle the concatenate features as our learning architecture."
              ],
              "metadata": null
            },
            {
              "id": "(Fan et al., 2021)",
              "snippets": [
                "Classical approaches to this problem rely on geometric analysis of the surrounding terrain, which can be prone to modeling errors and can be computationally expensive. Moreover, modeling the distribution of uncertain traversability costs is a difficult task, compounded by the various error sources mentioned above. In this work, we take a principled learning approach to this problem. We introduce a neural network architecture for robustly learning the distribution of traversability costs. Because we are motivated by preserving the life of the robot, we tackle this learning problem from the perspective of learning tail-risks, i.e. the conditional value-at-risk (CVaR)."
              ],
              "metadata": null
            },
            {
              "id": "(Rothrock et al., 2016)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Inotsume et al., 2021)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Endo et al., 2024)",
              "snippets": [
                "ML-based traversability prediction is a key component of off-road autonomy, where visual and geometric cues are essential. Visual cues capture the diverse geological semantic features that influence robot behavior. Thus, classification methods use the semantic segmentation of appearance imagery to categorize symbolic terrain types and identify traversable areas (Filitchkin et al., 2012), (Rothrock et al., 2016). This appearance-based classification is often combined with geometry-based regression methods to provide further variation in traversability within each class. Classify-then-regress methods model the traversability for each terrain class by correlating the degree of wheel slip with the terrain inclination (Cunningham et al., 2017)- (Inotsume et al., 2021). Mixture-of-experts (MoE) methods further account for terrain class likelihoods for slip regressors (Angelova et al., 2007), (Endo et al., 2023). Although these combined approaches using visual and geometric cues are effective, they require separate training processes for the classifier and regressor models, which involve human supervision. \n\nUsing the measured traverse data for supervision, learning traversability from experience has become a viable alternative. This approach provides a direct interpretation of robot-terrain interactions and eliminates the need for laborintensive manual annotation. Previous studies often used past driving trajectories to auto-generate ground-truth traversability, which assumes scene familiarity with robots (Schmid et al., 2022)- (Jung et al., 2023). Proprioceptive signals, such as force-torque (Wellhausen et al., 2019) and linear acceleration (Castro et al., 2022), define traversability in continuous-valued regression models. The absence of manual annotation also facilitates model adaptation through incremental learning with in-situ measurements, improving the robustness to novel situations [20], (Frey et al., 2023)."
              ],
              "metadata": null
            },
            {
              "id": "(Schmid et al., 2022)",
              "snippets": [
                "Navigating off-road with a fast autonomous vehicle depends on a robust perception system that differentiates traversable from non-traversable terrain. Typically, this depends on a semantic understanding which is based on supervised learning from images annotated by a human expert. This requires a significant investment in human time, assumes correct expert classification, and small details can lead to misclassification. To address these challenges, we propose a method for predicting high- and low-risk terrains from only past vehicle experience in a self-supervised fashion. First, we develop a tool that projects the vehicle trajectory into the front camera image. Second, occlusions in the 3D representation of the terrain are filtered out. Third, an autoencoder trained on masked vehicle trajectory regions identifies low- and high-risk terrains based on the reconstruction error. We evaluated our approach with two models and different bottleneck sizes with two different training and testing sites with a four-wheeled off-road vehicle. Comparison with two independent test sets of semantic labels from similar terrain as training sites demonstrates the ability to separate the ground as low-risk and the vegetation as high-risk with 81.1% and 85.1% accuracy."
              ],
              "metadata": null
            },
            {
              "id": "(Wellhausen et al., 2019)",
              "snippets": [
                "Legged robots have the potential to traverse diverse and rugged terrain. To find a safe and efficient navigation path and to carefully select individual footholds, it is useful to be able to predict properties of the terrain ahead of the robot. In this letter, we propose a method to collect data from robot-terrain interaction and associate it to images. Using sparse data acquired in teleoperation experiments with a quadrupedal robot, we train a neural network to generate a dense prediction of the terrain properties in front of the robot. To generate training data, we project the foothold positions from the robot trajectory into on-board camera images. We then attach labels to these footholds by identifying the dominant features of the force\u2013torque signal measured with sensorized feet. We show that data collected in this fashion can be used to train a convolutional network for terrain property prediction as well as weakly supervised semantic segmentation. Finally, we show that the predicted terrain properties can be used for autonomous navigation of the ANYmal quadruped robot."
              ],
              "metadata": null
            },
            {
              "id": "(Castro et al., 2022)",
              "snippets": [
                "Estimating terrain traversability in off-road environments requires reasoning about complex interaction dynamics between the robot and these terrains. However, it is challenging to create informative labels to learn a model in a supervised manner for these interactions. We propose a method that learns to predict traversability costmaps by combining exteroceptive environmental information with proprioceptive terrain interaction feedback in a self-supervised manner."
              ],
              "metadata": null
            },
            {
              "id": "(Espino et al., 2024)",
              "snippets": [
                "We present a neurorobotic navigation system that utilizes a Spiking Neural Network (SNN) Wavefront Planner and E-prop learning to concurrently map and plan paths in a large and complex environment. We incorporate a novel method for mapping which, when combined with the Spiking Wavefront Planner (SWP), allows for adaptive planning by selectively considering any combination of costs. The system is tested on a mobile robot platform in an outdoor environment with obstacles and varying terrain. Results indicate that the system is capable of discerning features in the environment using three measures of cost, (1) energy expenditure by the wheels, (2) time spent in the presence of obstacles, and (3) terrain slope. In just twelve hours of online training, E-prop learns and incorporates traversal costs into the path planning maps by updating the delays in the SWP."
              ],
              "metadata": null
            },
            {
              "id": "(Jung et al., 2023)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Frey et al., 2023)",
              "snippets": [
                "Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we propose Wild Visual Navigation (WVN), an online self-supervised learning system for traversability estimation which uses only vision. The system is able to continuously adapt from a short human demonstration in the field. It leverages high-dimensional features from self-supervised visual transformer models, with an online scheme for supervision generation that runs in real-time on the robot. We demonstrate the advantages of our approach with experiments and ablation studies in challenging environments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex outdoor terrains - negotiating obstacles in high grass as well as a 1.4 km footpath following. While our experiments were executed with a quadruped robot, ANYmal, the approach presented can generalize to any ground robot."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Visual and Geometric Feature-Based Methods",
          "text": "\nVisual and geometric feature-based methods have been foundational in terrain traversability estimation, offering the advantage of predicting traversability before physical interaction with the terrain. Early approaches focused on extracting geometric features from stereo data, with Happold et al. training neural networks on hand-labeled features for traversability analysis <Paper corpusId=\"208857799\" paperTitle=\"(Zurn et al., 2019)\" isShortName></Paper>. Similarly, Sofman et al. proposed using overhead imagery like satellite images to learn traversability costmaps for outdoor off-road robots <Paper corpusId=\"208857799\" paperTitle=\"(Zurn et al., 2019)\" isShortName></Paper>.\n\nTexture-based approaches have proven effective for terrain traversability prediction, particularly for surfaces with small-scale unevenness. Bekhti et al. developed a framework that uses multiscale local image features to detect non-uniformity in terrains (such as stones and tree roots), improving traversability prediction without requiring expensive 3D sensing equipment <Paper corpusId=\"214296448\" paperTitle=\"(Bekhti et al., 2020)\" isShortName></Paper>. This approach has the advantage of being affordable while still detecting motion features of the traversing robot, though it faces challenges in natural environments where terrains are often non-uniform.\n\nSemantic segmentation has emerged as another powerful technique for distinguishing between terrain types and preventing misclassification. Several researchers have trained classifiers to learn what terrain is traversable <Paper corpusId=\"244426453\" paperTitle=\"(Polevoy et al., 2021)\" isShortName></Paper> <Paper corpusId=\"214571441\" paperTitle=\"(Martinez et al., 2020)\" isShortName></Paper>. These approaches use semantic information to categorize terrain into traversable and non-traversable regions, often combining this with physical property estimation. Wellhausen et al. proposed simultaneously learning traversability prediction and semantic segmentation by using data from force-torque sensors embedded in a legged robot's feet during teleoperated runs <Paper corpusId=\"244426453\" paperTitle=\"(Polevoy et al., 2021)\" isShortName></Paper> <Paper corpusId=\"67871681\" paperTitle=\"(Wellhausen et al., 2019)\" isShortName></Paper>.\n\nRecent advancements in visual feature-based methods have moved beyond discrete terrain classification toward continuous representation of traversability. Sikand et al. introduced Visual Representation Learning for Preference-Aware Path Planning (VRL-PAP), which leverages unlabeled human demonstrations to learn visual representations of terrain that are viewpoint invariant and encode terrain types in a continuous representation space <Paper corpusId=\"272424135\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"237571896\" paperTitle=\"(Sikand et al., 2021)\" isShortName></Paper>. This approach overcomes key limitations of semantic segmentation by eliminating the need for pre-enumeration of discrete terrain types, handling hybrid terrain types, and reducing the requirement for expensive labeled data.\n\nCombining visual features with other sensor modalities has proven particularly effective. Some approaches extract features from meshes <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>, point clouds <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"14102368\" paperTitle=\"(Bradley et al., 2015)\" isShortName></Paper>, or depth images <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"210928244\" paperTitle=\"(Brandao et al., 2020)\" isShortName></Paper>. Chavez-Garcia et al. addressed traversability estimation as a heightmap classification problem, building a convolutional neural network that predicts whether a robot can traverse a terrain patch based on its heightmap <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"3666270\" paperTitle=\"(Chavez-Garcia et al., 2017)\" isShortName></Paper>. This approach can be trained for specific robot models (wheeled, tracked, legged, snake-like) using simulation data and then applied to unseen large heightmaps.\n\nRecent work has focused on learning-based methods that combine visual and geometric features for more accurate traversability prediction. Cai et al. showed that compared to hand-crafting planning costs based on terrain features, directly learning traversability models from data requires less manual labeling and results in more accurate assessment of vehicle-terrain interaction <Paper corpusId=\"272424135\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper>. Building on this idea, researchers have explored visual and visual-inertial representation learning <Paper corpusId=\"272424135\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258967822\" paperTitle=\"(Seo et al._2, 2023)\" isShortName></Paper>, self-training with pseudo-labels for unvisited terrain <Paper corpusId=\"272424135\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268403350\" paperTitle=\"(Cho et al., 2024)\" isShortName></Paper>, and temporal fusion of robot states and sensor measurements <Paper corpusId=\"272424135\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267364840\" paperTitle=\"(Gasparino et al., 2024)\" isShortName></Paper>.\n\nRisk-aware approaches have also been integrated with visual feature-based methods. Cai et al. proposed a representation of traversability based exclusively on robot speed that can be learned from data, using conditional value at risk (CVaR) to convert learned speed distribution maps into costmaps <Paper corpusId=\"272424135\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247748785\" paperTitle=\"(Cai et al., 2022)\" isShortName></Paper>. This risk-aware planning approach led to faster average time-to-goals compared to methods that only consider expected behavior.",
          "citations": [
            {
              "id": "(Zurn et al., 2019)",
              "snippets": [
                "Self-supervised learning of terrain classification and terrain properties for mobile robots has been investigated intensively in recent years. Early works by Sofman et al. (Sofman et al., 2006) proposed a self-supervised online learning approach that relies on overhead imagery such as satellite images to learn a traversability costmap for outdoor off-road robots. Happold et al. [13] trained a neural network offline on hand-labeled geometric features computed from stereo data for online traversability analysis using the predictions of the trained network. Later works by Hadsell et al. [2], [14] and Konolige et al"
              ],
              "metadata": null
            },
            {
              "id": "(Bekhti et al., 2020)",
              "snippets": [
                "In this paper, we propose a framework to enable mobile robots to autonomously learn terrain traversability using only their sensors in a self-supervised manner. Terrain traversability is predicted as a cost measured by an acceleration sensor equipped by the mobile robot. Focusing on the usage of image features for terrains with small-scale unevenness, a texture-based prediction of traversability was proposed in (Bekhti et al., 2015). However, the applicability of the texture information in 2D images has not been sufficiently investigated because, in natural environments, terrains are not always uniform and often are non-uniform, therein containing spiky materials such as relatively large stones and roots of trees. This non-uniformity causes traversability cost prediction to be challenging. In this paper, a detection of such non-uniformity in terrains is proposed based on multiscale local image features. It is shown that we can improve the prediction performance prediction performance of the texture-based approach. An advantage of the texture-based approach is that the sensor is affordable and can still detect motion features of the traversing robot without high-cost 3D sensing of the terrain geometry."
              ],
              "metadata": null
            },
            {
              "id": "(Polevoy et al., 2021)",
              "snippets": [
                "Recently, there has been significant research into using machine learning to aid or replace portions of the navigation pipeline to help overcome these limitations (Guastella et al., 2020), [10]. Several have trained classifiers to learn what terrain is traversable (Mart\u00ednez et al., 2020), (Ahtiainen et al., 2017). An alternative is to use semantic segmentation (Jackel et al., 2006), (Maturana et al., 2017) to distinguish types of terrain and prevent misclassification. Wellhausen et. al. (Wellhausen et al., 2019) propose simultaneously learning traversability prediction and semantic segmentation by using data from force-torque sensors embedded in a legged robot's feet during teleoperated runs. This traversability information is then used to update an underlying cost map."
              ],
              "metadata": null
            },
            {
              "id": "(Martinez et al., 2020)",
              "snippets": [
                "Autonomous navigation of ground vehicles on natural environments requires looking for traversable terrain continuously. This paper develops traversability classifiers for the three-dimensional (3D) point clouds acquired by the mobile robot Andabata on non-slippery solid ground. To this end, different supervised learning techniques from the Python library Scikit-learn are employed. Training and validation are performed with synthetic 3D laser scans that were labelled point by point automatically with the robotic simulator Gazebo. Good prediction results are obtained for most of the developed classifiers, which have also been tested successfully on real 3D laser scans acquired by Andabata in motion."
              ],
              "metadata": null
            },
            {
              "id": "(Wellhausen et al., 2019)",
              "snippets": [
                "Legged robots have the potential to traverse diverse and rugged terrain. To find a safe and efficient navigation path and to carefully select individual footholds, it is useful to be able to predict properties of the terrain ahead of the robot. In this letter, we propose a method to collect data from robot-terrain interaction and associate it to images. Using sparse data acquired in teleoperation experiments with a quadrupedal robot, we train a neural network to generate a dense prediction of the terrain properties in front of the robot. To generate training data, we project the foothold positions from the robot trajectory into on-board camera images. We then attach labels to these footholds by identifying the dominant features of the force\u2013torque signal measured with sensorized feet. We show that data collected in this fashion can be used to train a convolutional network for terrain property prediction as well as weakly supervised semantic segmentation. Finally, we show that the predicted terrain properties can be used for autonomous navigation of the ANYmal quadruped robot."
              ],
              "metadata": null
            },
            {
              "id": "(Cai et al., 2024)",
              "snippets": [
                "The field of traversability analysis studies how to infer suitability of terrain for navigation (see survey (Papadakis, 2013)). Compared to hand-crafting planning costs based on terrain features, directly learning traversability models from data requires less manual labeling and results in a more accurate assessment of vehicle-terrain interaction. Based on navigation data, visual and/or geometric features of the visited terrain can be used to train a traversability predictor based on estimated traversability values (Frey et al., 2023), (Datar et al., 2024), (Datar et al., 2023), (Cai et al., 2022), (Sikand et al., 2021). Building upon this basic idea, visual and visual-inertial representation learning (Seo et al., 2023), [23], self-training with pseudo-labels for unvisited terrain (Cho et al., 2024), temporal fusion of robot states and sensor measurements (Gasparino et al., 2024), and data augmentation via vision foundation models (Jung et al., 2023) can all improve learning accuracy."
              ],
              "metadata": null
            },
            {
              "id": "(Sikand et al., 2021)",
              "snippets": [
                "Autonomous mobile robots deployed in outdoor environments must reason about different types of terrain for both safety (e.g., prefer dirt over mud) and deployer preferences (e.g., prefer dirt path over flower beds). Most existing solutions to this preference-aware path planning problem use semantic segmentation to classify terrain types from camera images, and then ascribe costs to each type. Unfortunately, there are three key limitations of such approaches - they 1) require preenumeration of the discrete terrain types, 2) are unable to handle hybrid terrain types (e.g., grassy dirt), and 3) require expensive labelled data to train visual semantic segmentation. We introduce Visual Representation Learning for Preference-Aware Path Planning (VRL-PAP), an alternative approach that overcomes all three limitations: VRL-PAP leverages un-labelled human demonstrations of navigation to autonomously generate triplets for learning visual representations of terrain that are viewpoint invariant and encode terrain types in a continuous representation space. The learned representations are then used along with the same unlabelled human navigation demonstrations to learn a mapping from the representation space to terrain costs. At run time, VRL-PAP maps from images to representations and then representations to costs to perform preference-aware path planning. We present empirical results from challenging outdoor settings that demonstrate VRL-PAP 1) is successfully able to pick paths that reflect demonstrated preferences, 2) is comparable in execution to geometric navigation with a highly detailed manually annotated map (without requiring such annotations), 3) is able to generalize to novel terrain types with minimal additional unlabeled demonstrations."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al., 2024)",
              "snippets": [
                "To estimate the terrain traversability cost, some previous work directly extracts features from the environment. These methods usually define traversability by empirically judging the cost from meshes [4], point clouds [5], or depth images [6]. Others try to classify the terrains from semantic categories [7][8][9], often mapping semantic features to traversability cost based on human-provided labels. These methods typically rely on human prior knowledge rather than the robot's own experiences [10]. While these estimators are robot-agnostic and easily transferable across different embodiments, they fail to consider the robot's real interaction experiences. For example, some terrains may be difficult for lightweight robots but manageable for heavier ones, and vice versa.\n\nTo consider the robot's experience while traversing different terrains, previous work has suggested various metrics. The most straightforward approach is to consider control performance metrics such as tracking error of commands (e.g., velocity) [11] or IMU variance [12]. These quantitative metrics are used as labels for supervised learning of environment representations. Other approaches extend this research by learning estimators to predict traversal time [13], selected environment physics parameters [14], or collision and fall indicators [15] from a history of robot's proprioception feedback."
              ],
              "metadata": null
            },
            {
              "id": "(Bradley et al., 2015)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Brandao et al., 2020)",
              "snippets": [
                "Long-range locomotion planning is an important problem for the deployment of legged robots to real scenarios. Current methods used for legged locomotion planning often do not exploit the flexibility of legged robots, and do not scale well with environment size. In this letter we propose the use of navigation meshes for deployment in large-scale multi-floor sites. We leverage this representation to improve long-term locomotion plans in terms of success rates, path costs and reasoning about which gait-controller to use when. We show that NavMeshes have higher planning success rates than sampling-based planners, but are 400x faster to construct and at least 100x faster to plan with. The performance gap further increases when considering multi-floor environments. We present both a procedure for building controller-aware NavMeshes and a full navigation system that adapts to changes to the environment. We demonstrate the capabilities of the system in simulation experiments and in field trials at a real-world oil rig facility."
              ],
              "metadata": null
            },
            {
              "id": "(Chavez-Garcia et al., 2017)",
              "snippets": [
                "Mobile ground robots operating on unstructured terrain must predict which areas of the environment they are able to pass in order to plan feasible paths. We address traversability estimation as a heightmap classification problem: we build a convolutional neural network that, given an image representing the heightmap of a terrain patch, predicts whether the robot will be able to traverse such patch from left to right. The classifier is trained for a specific robot model (wheeled, tracked, legged, snake-like) using simulation data on procedurally generated training terrains; the trained classifier can be applied to unseen large heightmaps to yield oriented traversability maps, and then plan traversable paths. We extensively evaluate the approach in simulation on six real-world elevation dataset, and run a real-robot validation in one indoor and one outdoor environment."
              ],
              "metadata": null
            },
            {
              "id": "(Seo et al._2, 2023)",
              "snippets": [
                "Estimating the traversability of terrain should be reliable and accurate in diverse conditions for autonomous driving in off-road environments. However, learning-based approaches often yield unreliable results when confronted with unfamiliar contexts, and it is challenging to obtain manual annotations frequently for new circumstances. In this letter, we introduce a method for learning traversability from images that utilizes only self-supervision and no manual labels, enabling it to easily learn traversability in new circumstances. To this end, we first generate self-supervised traversability labels from past driving trajectories by labeling regions traversed by the vehicle as highly traversable. Using the self-supervised labels, we then train a neural network that identifies terrains that are safe to traverse from an image using a one-class classification algorithm. Additionally, we supplement the limitations of self-supervised labels by incorporating methods of self-supervised learning of visual representations. To conduct a comprehensive evaluation, we collect data in a variety of driving environments and perceptual conditions and show that our method produces reliable estimations in various environments. In addition, the experimental results validate that our method outperforms other self-supervised traversability estimation methods and achieves comparable performances with supervised learning methods trained on manually labeled data."
              ],
              "metadata": null
            },
            {
              "id": "(Cho et al., 2024)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Gasparino et al., 2024)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Cai et al., 2022)",
              "snippets": [
                "Motion planning in off-road environments re-quires reasoning about both the geometry and semantics of the scene (e.g., a robot may be able to drive through soft bushes but not a fallen log). In many recent works, the world is classified into a finite number of semantic categories that often are not sufficient to capture the ability (i.e., the speed) with which a robot can traverse off-road terrain. Instead, this work proposes a new representation of traversability based exclusively on robot speed that can be learned from data, offers interpretability and intuitive tuning, and can be easily integrated with a variety of planning paradigms in the form of a costmap. Specifically, given a dataset of experienced trajectories, the proposed algorithm learns to predict a distribution of speeds the robot could achieve, conditioned on the environment semantics and commanded speed. The learned speed distribution map is converted into costmaps with a risk-aware cost term based on conditional value at risk (CVaR). Numerical simulations demonstrate that the proposed risk-aware planning algorithm leads to faster average time-to-goals compared to a method that only considers expected behavior, and the planner can be tuned for slightly slower, but less variable behavior. Furthermore, the approach is integrated into a full autonomy stack and demonstrated in a high-fidelity Unity environment and is shown to provide a 30% improvement in the success rate of navigation."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Terrain Classification and Semantic Approaches",
          "text": "\nTerrain classification and semantic approaches represent a significant category of methods for learning traversability costs in robotics. These approaches focus on identifying and categorizing different terrain types from sensor data to estimate traversability for safe robot navigation. Early classification methods applied exteroceptive and proprioceptive sensory data to classify terrain types and estimate traversability for navigation <Paper corpusId=\"230435654\" paperTitle=\"(Siva et al., 2021)\" isShortName></Paper> <Paper corpusId=\"244102998\" paperTitle=\"(Siva et al._1, 2021)\" isShortName></Paper>. This approach has evolved to include terrain complexity modeling for planning navigation tasks <Paper corpusId=\"230435654\" paperTitle=\"(Siva et al., 2021)\" isShortName></Paper> <Paper corpusId=\"16458472\" paperTitle=\"(Silver et al., 2010)\" isShortName></Paper>.\n\nSemantic segmentation has emerged as a powerful technique for distinguishing between terrain types and preventing misclassification. Several researchers have trained classifiers to learn what terrain is traversable <Paper corpusId=\"244426453\" paperTitle=\"(Polevoy et al., 2021)\" isShortName></Paper> <Paper corpusId=\"214571441\" paperTitle=\"(Martinez et al., 2020)\" isShortName></Paper>. This approach uses semantic information to categorize terrain into traversable and non-traversable regions. Wellhausen et al. proposed simultaneously learning traversability prediction and semantic segmentation by using data from force-torque sensors embedded in a legged robot's feet during teleoperated runs <Paper corpusId=\"244426453\" paperTitle=\"(Polevoy et al., 2021)\" isShortName></Paper> <Paper corpusId=\"67871681\" paperTitle=\"(Wellhausen et al., 2019)\" isShortName></Paper>. This traversability information is then used to update an underlying cost map for navigation planning.\n\nModern terrain classification approaches have evolved beyond simple binary traversability assessment. Visual cues capture diverse geological semantic features that influence robot behavior, with classification methods using semantic segmentation of appearance imagery to categorize terrain types and identify traversable areas <Paper corpusId=\"272367062\" paperTitle=\"(Endo et al., 2024)\" isShortName></Paper>. This appearance-based classification is often combined with geometry-based regression methods to provide further variation in traversability within each terrain class. \"Classify-then-regress\" methods model traversability for each terrain class by correlating wheel slip with terrain inclination <Paper corpusId=\"272367062\" paperTitle=\"(Endo et al., 2024)\" isShortName></Paper>.\n\nSome approaches combine visual features with other sensor modalities, extracting features from meshes, point clouds, or depth images <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Chavez-Garcia et al. addressed traversability estimation as a heightmap classification problem, building a convolutional neural network that predicts whether a robot can traverse a terrain patch based on its heightmap <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"3666270\" paperTitle=\"(Chavez-Garcia et al., 2017)\" isShortName></Paper>. This approach can be trained for specific robot models (wheeled, tracked, legged, snake-like) using simulation data and then applied to unseen large heightmaps.\n\nRecent advancements include approaches that consider control performance metrics such as velocity command tracking errors or IMU variance as quantitative metrics for supervised learning of environment representations <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Other approaches extend this research by learning estimators to predict traversal time, specific environment physics parameters, or collision and fall indicators from a history of robot proprioception feedback <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nWhile terrain classification and semantic approaches provide a structured way to assess traversability, many traditional methods rely on human prior knowledge rather than the robot's own experiences <Paper corpusId=\"273345537\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This can be a limitation since some terrains may be difficult for lightweight robots but manageable for heavier ones, and vice versa. To address this, more recent approaches are integrating robot-specific factors and actual interaction experiences into semantic traversability assessment, providing a more nuanced understanding of terrain traversability specific to each robot's capabilities.",
          "citations": [
            {
              "id": "(Siva et al., 2021)",
              "snippets": [
                "Previous learning-based methods can be divided into two broad categories: terrain classification and terrain adaptation. The first category of approaches apply a robot's exteroceptive and proprioceptive sensory data to classify terrain types and estimate traversability for a robot to navigate over the terrain (Bermudez et al., 2012), (Brooks et al., 2005), (Devjanin et al., 1983), (DuPont et al., 2008), (Hudjakov et al., 2009). This category also includes methods that model terrain complexity for planning robot navigation tasks (Silver et al., 2010), (Peynot et al., 2014). The second category of methods focus on directly generating adaptive navigational behaviors according to terrain in order to successfully complete navigation tasks (Silver et al., 2010), (Han et al., 2017), (Pastor et al., 2009), (Wigness et al., 2018). Specifically, learning from demonstration (LfD) is widely used to transfer human expertise to robots in order to achieve human-level robot navigational control [4], [13], [14]."
              ],
              "metadata": null
            },
            {
              "id": "(Siva et al._1, 2021)",
              "snippets": [
                "Learning-based approaches can learn terrain complexity and estimate the possibility of traversing terrains through terrain recognition (Bermudez et al., 2012); (Peynot et al., 2014); (Hudjakov et al., 2009))."
              ],
              "metadata": null
            },
            {
              "id": "(Silver et al., 2010)",
              "snippets": [
                "This paper does not directly address methods for learning terrain traversibility costs automatically from robot experience. While it mentions learning from demonstration for navigation, it doesn't clearly outline \"leading methods\" for learning traversibility costs from robot experience specifically. The focus appears to be on learning from expert examples rather than the robot's own experience.\n\nNone"
              ],
              "metadata": null
            },
            {
              "id": "(Polevoy et al., 2021)",
              "snippets": [
                "Recently, there has been significant research into using machine learning to aid or replace portions of the navigation pipeline to help overcome these limitations (Guastella et al., 2020), [10]. Several have trained classifiers to learn what terrain is traversable (Mart\u00ednez et al., 2020), (Ahtiainen et al., 2017). An alternative is to use semantic segmentation (Jackel et al., 2006), (Maturana et al., 2017) to distinguish types of terrain and prevent misclassification. Wellhausen et. al. (Wellhausen et al., 2019) propose simultaneously learning traversability prediction and semantic segmentation by using data from force-torque sensors embedded in a legged robot's feet during teleoperated runs. This traversability information is then used to update an underlying cost map."
              ],
              "metadata": null
            },
            {
              "id": "(Martinez et al., 2020)",
              "snippets": [
                "Autonomous navigation of ground vehicles on natural environments requires looking for traversable terrain continuously. This paper develops traversability classifiers for the three-dimensional (3D) point clouds acquired by the mobile robot Andabata on non-slippery solid ground. To this end, different supervised learning techniques from the Python library Scikit-learn are employed. Training and validation are performed with synthetic 3D laser scans that were labelled point by point automatically with the robotic simulator Gazebo. Good prediction results are obtained for most of the developed classifiers, which have also been tested successfully on real 3D laser scans acquired by Andabata in motion."
              ],
              "metadata": null
            },
            {
              "id": "(Wellhausen et al., 2019)",
              "snippets": [
                "Legged robots have the potential to traverse diverse and rugged terrain. To find a safe and efficient navigation path and to carefully select individual footholds, it is useful to be able to predict properties of the terrain ahead of the robot. In this letter, we propose a method to collect data from robot-terrain interaction and associate it to images. Using sparse data acquired in teleoperation experiments with a quadrupedal robot, we train a neural network to generate a dense prediction of the terrain properties in front of the robot. To generate training data, we project the foothold positions from the robot trajectory into on-board camera images. We then attach labels to these footholds by identifying the dominant features of the force\u2013torque signal measured with sensorized feet. We show that data collected in this fashion can be used to train a convolutional network for terrain property prediction as well as weakly supervised semantic segmentation. Finally, we show that the predicted terrain properties can be used for autonomous navigation of the ANYmal quadruped robot."
              ],
              "metadata": null
            },
            {
              "id": "(Endo et al., 2024)",
              "snippets": [
                "ML-based traversability prediction is a key component of off-road autonomy, where visual and geometric cues are essential. Visual cues capture the diverse geological semantic features that influence robot behavior. Thus, classification methods use the semantic segmentation of appearance imagery to categorize symbolic terrain types and identify traversable areas (Filitchkin et al., 2012), (Rothrock et al., 2016). This appearance-based classification is often combined with geometry-based regression methods to provide further variation in traversability within each class. Classify-then-regress methods model the traversability for each terrain class by correlating the degree of wheel slip with the terrain inclination (Cunningham et al., 2017)- (Inotsume et al., 2021). Mixture-of-experts (MoE) methods further account for terrain class likelihoods for slip regressors (Angelova et al., 2007), (Endo et al., 2023). Although these combined approaches using visual and geometric cues are effective, they require separate training processes for the classifier and regressor models, which involve human supervision. \n\nUsing the measured traverse data for supervision, learning traversability from experience has become a viable alternative. This approach provides a direct interpretation of robot-terrain interactions and eliminates the need for laborintensive manual annotation. Previous studies often used past driving trajectories to auto-generate ground-truth traversability, which assumes scene familiarity with robots (Schmid et al., 2022)- (Jung et al., 2023). Proprioceptive signals, such as force-torque (Wellhausen et al., 2019) and linear acceleration (Castro et al., 2022), define traversability in continuous-valued regression models. The absence of manual annotation also facilitates model adaptation through incremental learning with in-situ measurements, improving the robustness to novel situations [20], (Frey et al., 2023)."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al., 2024)",
              "snippets": [
                "To estimate the terrain traversability cost, some previous work directly extracts features from the environment. These methods usually define traversability by empirically judging the cost from meshes [4], point clouds [5], or depth images [6]. Others try to classify the terrains from semantic categories [7][8][9], often mapping semantic features to traversability cost based on human-provided labels. These methods typically rely on human prior knowledge rather than the robot's own experiences [10]. While these estimators are robot-agnostic and easily transferable across different embodiments, they fail to consider the robot's real interaction experiences. For example, some terrains may be difficult for lightweight robots but manageable for heavier ones, and vice versa.\n\nTo consider the robot's experience while traversing different terrains, previous work has suggested various metrics. The most straightforward approach is to consider control performance metrics such as tracking error of commands (e.g., velocity) [11] or IMU variance [12]. These quantitative metrics are used as labels for supervised learning of environment representations. Other approaches extend this research by learning estimators to predict traversal time [13], selected environment physics parameters [14], or collision and fall indicators [15] from a history of robot's proprioception feedback."
              ],
              "metadata": null
            },
            {
              "id": "(Chavez-Garcia et al., 2017)",
              "snippets": [
                "Mobile ground robots operating on unstructured terrain must predict which areas of the environment they are able to pass in order to plan feasible paths. We address traversability estimation as a heightmap classification problem: we build a convolutional neural network that, given an image representing the heightmap of a terrain patch, predicts whether the robot will be able to traverse such patch from left to right. The classifier is trained for a specific robot model (wheeled, tracked, legged, snake-like) using simulation data on procedurally generated training terrains; the trained classifier can be applied to unseen large heightmaps to yield oriented traversability maps, and then plan traversable paths. We extensively evaluate the approach in simulation on six real-world elevation dataset, and run a real-robot validation in one indoor and one outdoor environment."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "In robotics, what are the leading methods for learning terrain traversibility costs automatically from robot experience?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should define terrain traversability learning and its importance in robotics.",
            "weight": 0.0923076923076923,
            "evidence": [
              "The autonomous exploration of unstructured environments by mobile robots is witnessing increased interest, as it helps to accomplish diverse tasks such as search and rescue, surveying and data collection (e.g., [1] with unmanned ground vehicles and [2] with unmanned aerial vehicle), and surveillance. Performing these missions by autonomous mobile robots presents several advantages. It allows the avoidance of human intervention in hazardous area [3], as well as overcomes difficulties that may arise in the case of remotely controlled mobile robots such as a loss of connectivity or degraded latency. With robots being utilized in a wide spectrum of unstructured environments, the need for systems to build reliable representations of the environment is critical. Therefore, the estimation of the traversability of terrains is of great importance."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain supervised or self-supervised learning methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "Supervised (learning) method: a data-driven method that learns from provided examples, i.e., using training data that include ground truth labels, usually provided by experts, prior to training",
              "In this context, a recent study by Martinez et al. [85] introduced the supervised learning method, which uses labelled data for training quantifying part of terrains such as ground, grass (of height below 5 cm), and the side-walk portion to label it as traversable area via the use of Lidar sensing.",
              "Another supervised learning-based terrain classification approach has been studied by the authors in [161]. Here, a Microsoft Kinect V2 visual sensor able to supply infrared (IR), colour and depth stream features was used to predict terrain types categorised into five groups, i.e. gravel, sand, pavement, grass and litterfall & straw. After manual labelling and pre-processing, IR, colour and depth features were combined for the terrain recognition model with a Support Vector Machine (SVM) classifier.",
              "In this work, we proposed a self-supervised terrain classification framework that exploits the training signal from an unsupervised proprioceptive terrain classifier to learn an exteroceptive classifier for pixel-wise semantic terrain segmentation.",
              "This paper introduces a self-supervised traversability estimation method that can learn traversability in varied environments without manual annotations. Using the self-supervised labels only, the network is trained to predict traversable regions using a one-class classification method.",
              "We pose the traversability estimation as a vector regression task over vertical bands of the observed frame. The model is pre-trained through self-supervision to reduce the distribution shift between synthetic and real data and encourage shared feature learning. Then, supervised training on synthetic videos is carried out, while employing an unsupervised domain adaptation loss to improve its generalization capabilities on real scenes."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should explain unsupervised learning methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "Unsupervised (learning) method: an approach in which the learning model is given a dataset without specific instructions on what the output should be. The method then aims at automatically extracting patterns or structure in the data by finding meaningful features",
              "Within the review of recent traversability-related literature, [124] proposed an automatic labelling method as a part of unsupervised learning that learns samples from unlabelled data using 2D maps obtained from the Gazebo environment and converted to a 3D point cloud."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should explain reinforcement learning and inverse reinforcement learning methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "Reinforcement Learning (RL) and Inverse Reinforcement Learning (IRL) methods are being applied to learn terrain traversability costs from robot experiences and human demonstrations. These approaches enable robots to adapt to complex terrains and mimic specific navigation styles.Reinforcement Learning (RL) and Inverse Reinforcement Learning (IRL) have emerged as powerful techniques for learning terrain traversability costs in robotics. These methods offer the advantage of learning directly from robot experiences or human demonstrations, allowing for more adaptive and nuanced navigation strategies.IRL has gained significant attention for modeling terrain traversability, particularly in autonomous vehicles  (11, Grizzle et al., 2022). Unlike traditional approaches that use predefined metrics, IRL aims to learn traversability costs directly from a robot's traversing behaviors. This method allows for a more flexible and context-aware understanding of terrain challenges.",
              "Authors in [171] proposed a traversability map created via a fully convolutional network (FCN) based method whereby a feature map generated using Lidar has been used as input to the FCN. Also, trajectory planning has been implemented using Inverse Reinforcement Learning (IRL), and different methods under various scenes have been analysed",
              "In the study performed by the authors in [168], the depth images, the orientation of the robot and the elevation map which represent the obstacle around the robot have been used as inputs in deep reinforcement learning.",
              "Furthermore, research [167] proposed a path-planning algorithm based on deep-reinforcement learning, using a reward function relating to terrain slope.",
              "Another recent study, i.e. in [55], proposed a deep reinforcement learning-based navigation method that predicted the navigation actions: forward, tiny left, tiny right, hard left and hard right using images as inputs."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should explain online and Incremental learning methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "TLDR: Online and incremental learning techniques allow robots to continuously update their terrain traversability models based on real-time experiences. These methods enable robots to adapt to changing environments and improve their navigation capabilities over time.Online and incremental learning approaches have gained significant attention in robotics for learning terrain traversability costs. These methods allow robots to update their models in real-time as they encounter new terrains, making them particularly useful for long-term autonomous operation in dynamic environments.Pragr et al. developed a robotic system that incrementally learns to predict power consumption-based traversal costs experienced by robots over various terrains (7, Pragr et al., 2019). This system combines autonomous spatial exploration with simultaneous learning of the underlying traversal cost model, enabling continuous adaptation to new terrain types."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should explain transfer learning and multi-robot methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "TLDR: Transfer learning and multi-robot approaches enable the sharing of terrain traversability knowledge between different robots and scenarios. These methods improve adaptability and efficiency in learning traversability costs across diverse robotic platforms and environments.Transfer learning and multi-robot approaches have emerged as promising techniques for enhancing the learning of terrain traversability costs in robotics. These methods address the challenge of adapting learned models across different robot types and environments, potentially reducing the time and resources required for training.Zelinka et al. proposed a transfer learning approach based on convolutional neural networks to create cost assessment models for individual robots  (137, Zelinka et al., 2021). Their results indicate that terrain cost assessment can be improved through transfer learning, demonstrating the viability of sharing traversability knowledge between different robotic platforms."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should describe the main challenges in learning terrain traversability costs.",
            "weight": 0.04615384615384615,
            "evidence": [
              "There is a significant growth in autonomy level in off-road ground vehicles. However, unknown off-road environ-ments are often challenging due to their unstructured and roughnature. To find a path that the robot can move smoothly toits destination, it needs to analyse the surrounding terrain."
            ]
          }
        ]
      }
    }
  },
  {
    "question": "What is the known evidence for pervasiveness of gaming of citation metrics like citation count and h-index?",
    "response": {
      "sections": [
        {
          "title": "Introduction/Background: Citation Metrics and Their Importance",
          "text": "\nCitation metrics have emerged as the dominant method for quantifying academic impact and influence across scientific disciplines. The most prevalent metrics include citation counts, h-index, and journal impact factors, which are extensively used to evaluate individual researchers, institutions, and publications. <Paper corpusId=\"234469699\" paperTitle=\"(Lagopoulos et al., 2021)\" isShortName></Paper> These quantitative indicators have largely replaced qualitative assessments because they appear more objective and are easier to formulate and implement. <Paper corpusId=\"234469699\" paperTitle=\"(Lagopoulos et al., 2021)\" isShortName></Paper> <Paper corpusId=\"165059828\" paperTitle=\"(Copiello, 2019)\" isShortName></Paper>\n\nThe importance of citation metrics in academia cannot be overstated. Researchers with high citation counts and h-indices are more likely to receive promotions, awards, and recognition, while journals with high impact factors are perceived as more prestigious and desirable publication venues. <Paper corpusId=\"234469699\" paperTitle=\"(Lagopoulos et al., 2021)\" isShortName></Paper> The widespread adoption of these metrics has fundamentally transformed how scientific contributions are evaluated and rewarded. <Paper corpusId=\"270764785\" paperTitle=\"(Evdaimon et al., 2024)\" isShortName></Paper>\n\nHowever, this reliance on quantitative metrics has created a system vulnerable to manipulation. As Goodhart's Law suggests, \"when a measure becomes a target, it ceases to be a good measure,\" and citation metrics are no exception. <Paper corpusId=\"411534\" paperTitle=\"(Shah et al., 2015)\" isShortName></Paper> Researchers may adopt practices specifically designed to inflate their metrics rather than focus on producing quality research. A 2010 survey found that 71% of respondents feared colleagues could \"game\" or \"cheat\" their way to better evaluations at their institutions. <Paper corpusId=\"263979729\" paperTitle=\"(Edwards et al., 2017)\" isShortName></Paper>\n\nThe potential for manipulation is particularly concerning in environments with limited oversight, such as Google Scholar (GS), where studies have demonstrated how easily citation metrics can be artificially inflated. <Paper corpusId=\"2345759\" paperTitle=\"(Lopez-Cozar et al., 2013)\" isShortName></Paper> <Paper corpusId=\"2578647\" paperTitle=\"(Bartneck et al., 2010)\" isShortName></Paper> Even in more controlled databases, research has identified various forms of citation manipulation, including self-citation strategies specifically designed to increase h-index values. <Paper corpusId=\"215574614\" paperTitle=\"(Szomszor et al., 2020)\" isShortName></Paper> <Paper corpusId=\"221141814\" paperTitle=\"(Wren et al., 2020)\" isShortName></Paper>\n\nThis vulnerability raises serious concerns about whether citation metrics genuinely reflect research quality and impact or merely reward those most adept at manipulating the system. <Paper corpusId=\"270764785\" paperTitle=\"(Evdaimon et al., 2024)\" isShortName></Paper> If rewards disproportionately benefit individuals who game their metrics, the quantitative system may ultimately create more problems than the subjective evaluation paradigms it was designed to replace. <Paper corpusId=\"263979729\" paperTitle=\"(Edwards et al., 2017)\" isShortName></Paper>",
          "citations": [
            {
              "id": "(Lagopoulos et al., 2021)",
              "snippets": [
                "Different forms of self-citation can be observed in scientific publishing. Direct self-citation, where authors cite their previous work, are the ones most associated with the term. Co-author and collaborative self-citation come as an extension to direct self-citation adding more complexity to the self-citation network. Coercive citation, where reviewers ask authors to include references to their work, are considered the most difficult to detect, and along with the journal/institution self-citations they complete the list of the most common types of self-citation [Ioannidis, 2015].\n\nThe scientific impact of an individual, an institution, or a publisher is typically determined by quantitative metrics and indices [Wren and Georgescu, 2020]. Citation count is the prevalent metric of recognition in science, while qualitative metrics are discarded as hard to formulate and employ [Fortunato et al., 2018]. Promotions and awards are granted to highly cited researchers, and journals with high impact factors are considered to be the most appealing and prestigious ones. Yet, most of the metrics such as citation count, h-index, and impact factor do not investigate self-citations. Thus, efforts to game or hack such metrics are made to increase the visibility of a research piece and consequently to act as advertisement and recognition for the author/publisher/institution [Szomszor et al., 2020, Copiello, 2019]."
              ],
              "metadata": null
            },
            {
              "id": "(Copiello, 2019)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Evdaimon et al., 2024)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Shah et al., 2015)",
              "snippets": [
                "When a metric is introduced as a quantifier, its value as a measure immediately begins to decline. This consequence was initially formulated in the economic context and is known as Goodhart's Law, which states \"when a measure becomes a target, it ceases to be a good measure,\" in reference to the response of investors to act in ways which they seek to benefit from economic regulations. Analogously, as impact metrics are proposed, researchers will seek to adopt practices which enable them to capitalize from the metric and better their rankings. Thus, it is important that impact metrics are robust, or difficult to rig or game by means of disreputable practices (such as unwarranted self-citation, double publication and citation trading)."
              ],
              "metadata": null
            },
            {
              "id": "(Edwards et al., 2017)",
              "snippets": [
                "Ultimately, the well-intentioned use of quantitative metrics may create inequities and outcomes worse than the systems they replaced. Specifically, if rewards are disproportionally given to individuals manipulating their metrics, problems of the old subjective paradigms (e.g., old-boys' networks) may be tame by comparison. In a 2010 survey, 71% of respondents stated that they feared colleagues can ''game'' or ''cheat'' their way into better evaluations at their institutions (Abbott et al., 2010), demonstrating that scientists are acutely attuned to the possibility of abuses in the current system."
              ],
              "metadata": null
            },
            {
              "id": "(Lopez-Cozar et al., 2013)",
              "snippets": [
                "The inclusion of bibliometric tools applied in an uncontrolled environment as GS proposes, has led to another type of critical studies experimenting on their capacity to discern academic content from faked content. At this point we must refer to the study undertaken by (132836892) and his inexistent researcher Ike Antkare who proved how easily computer generated tools for research evaluation can be manipulated. In similar studies, Beel, Gipp & Wilde (2010) and Beel & Gipp (2010) tested different procedures with which to influence GS' results and obtain higher ranking positions and hence, more visibility. Among others, they made use of the Scigen software (http://pdos.csail.mit.edu/scigen/) to create fake papers, they included modifications of previously published papers adding new references, as well as duplicates of other papers. Such papers alerted on the ease to manipulate the GS search engine",
                "Although malpractices have also been reported in other databases due to the inclusion of the so-called 'predatory publishers' (Harzing, 2012) or simply by manipulating journals' Impact Factor (Opatrn\u00fd, 2008), the lack of any type of control or transparency of GS is certainly worrying as this tool is becoming more and more popular within the research community (Bartneck et al., 2010)",
                "This exposes the vulnerability of the product if editors and researchers are tempted to do \"citations engineering\" and modify their hindex by excessively self-citing their papers. Indeed, as (Bartneck et al., 2010) proved, the h-index can be easily inflated by means of three possible strategies: random self-citation, recent self-citation or, in a most refined way, sending citations only to the hot zone of their publications, that is, to those which can influence this indicator. Devious editors can easily modify their journals' h-index, having a significant effect especially for those journals with small figures on which the slightest variation can have a great impact on their ranking positioning."
              ],
              "metadata": null
            },
            {
              "id": "(Bartneck et al., 2010)",
              "snippets": [
                "The h-index has received an enormous attention for being an indicator that measures the quality of researchers and organizations. We investigate to what degree authors can inflate their h-index through strategic self-citations with the help of a simulation. We extended Burrell\u2019s publication model with a procedure for placing self-citations, following three different strategies: random self-citation, recent self-citations and h-manipulating self-citations. The results show that authors can considerably inflate their h-index through self-citations. We propose the q-index as an indicator for how strategically an author has placed self-citations, and which serves as a tool to detect possible manipulation of the h-index. The results also show that the best strategy for an high h-index is publishing papers that are highly cited by others. The productivity has also a positive effect on the h-index."
              ],
              "metadata": null
            },
            {
              "id": "(Szomszor et al., 2020)",
              "snippets": [
                "Citations can be an indicator of publication significance, utility, attention, visibility or short-term impact but analysts need to confirm whether a high citation count for an individual is a genuine reflection of influence or a consequence of extraordinary, even excessive, self-citation. It has recently been suggested there may be increasing misrepresentation of research performance by individuals who self-cite inordinately to achieve scores and win rewards. In this paper we consider self-referencing and self-citing, describe the typical shape of self-citation patterns for carefully curated publication sets authored by 3517 Highly Cited Researchers and quantify the variance in the distribution of self-citation rates within and between all 21 Essential Science Indicators\u2019 fields. We describe both a generic level of median self-referencing rates, common to most fields, and a graphical, distribution-driven assessment of excessive self-citation that demarcates a threshold not dependent on statistical tests or percentiles (since for some fields all values are within a central \u2018normal\u2019 range). We describe this graphical procedure for identifying exceptional self-citation rates but emphasize the necessity for expert interpretation of the citation profiles of specific individuals, particularly in fields with atypical self-citation patterns."
              ],
              "metadata": null
            },
            {
              "id": "(Wren et al., 2020)",
              "snippets": [
                "Although citations are used as a quantifiable, objective metric of academic influence, cases have been documented whereby references were added to a paper solely to inflate the perceived influence of a body of research. This reference list manipulation (RLM) could take place during the peer-review process (e.g., coercive citation from editors or reviewers), or prior to it (e.g., a quid-pro-quo between authors). Surveys have estimated how many people may have been affected by coercive RLM at one time or another, but it is not known how many authors engage in RLM, nor to what degree. Examining a subset of active, highly published authors (n=20,803) in PubMed, we find the frequency of non-self citations (NSC) to one author coming from one paper approximates Zipf\u2019s law. We propose the Gini Index as a simple means of quantifying skew in this distribution and test it against a series of \"red flag\" metrics that are expected to result from RLM attempts. We estimate between 81 (FDR <0.05) and 231 (FDR<0.10) authors are outliers on the curve, suggestive of chronic, repeated RLM. Based upon the distribution, we estimate approximately 3,284 (16%) of all authors may have engaged in RLM to some degree, possibly opportunistically. Finally, we find authors who use 18% or more of their references for self-citation are significantly more likely to have NSC Gini distortions, suggesting their desire to see their work cited carries over into their peer-review activity."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Types of Citation Metric Gaming",
          "text": "\nCitation metric gaming encompasses a wide range of deliberate strategies designed to artificially inflate metrics such as citation counts, h-index, and journal impact factors. The most prevalent forms include:\n\n## Self-citation Practices\n- **Direct self-citation**: Authors excessively cite their own previous work to boost citation metrics. Studies have demonstrated that researchers can significantly inflate their h-index through strategic self-citations, particularly when citations are deliberately placed to boost specific metrics rather than contribute to scientific discourse. <Paper corpusId=\"2578647\" paperTitle=\"(Bartneck et al., 2010)\" isShortName></Paper> <Paper corpusId=\"440416\" paperTitle=\"(Senanayake et al., 2015)\" isShortName></Paper>\n- **Strategic h-index manipulation**: Authors specifically target their \"hot zone\" publications (those which can influence the h-index) with self-citations. Simulations have shown that authors can considerably inflate their h-index through such targeted citation practices. <Paper corpusId=\"2345759\" paperTitle=\"(Lopez-Cozar et al., 2013)\" isShortName></Paper> <Paper corpusId=\"2578647\" paperTitle=\"(Bartneck et al., 2010)\" isShortName></Paper>\n- **Co-author self-citation**: Authors cite publications of their co-authors, creating a wider network of self-citation that is harder to detect. <Paper corpusId=\"234469699\" paperTitle=\"(Lagopoulos et al., 2021)\" isShortName></Paper>\n\n## Coordinated Citation Manipulation\n- **Citation cartels/farms**: Groups of authors who cite each other disproportionately more than they cite other groups working on the same subjects. These cartels can involve authors from different institutions and countries. <Paper corpusId=\"264998042\" paperTitle=\"(Ioannidis et al., 2023)\" isShortName></Paper> <Paper corpusId=\"15814739\" paperTitle=\"(Fister et al., 2016)\" isShortName></Paper>\n- **Journal citation stacking**: Journals collude to cite one another excessively, artificially boosting their impact factors. In some documented cases, such practices increased journal impact factors by up to 94%. <Paper corpusId=\"16041842\" paperTitle=\"(2015)\" isShortName></Paper> <Paper corpusId=\"246748933\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>\n- **Reciprocal citation**: Authors preferentially cite publications of people who cite their work rather than those who don't. <Paper corpusId=\"246748933\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>\n\n## Coercive Citation Practices\n- **Editorial coercion**: Journal editors pressure authors to add citations to their journal before accepting papers for publication. <Paper corpusId=\"64211621\" paperTitle=\"(Malone et al., 2016)\" isShortName></Paper>\n- **Reviewer coercion**: Reviewers demand authors include citations to their work as a condition for recommending publication. <Paper corpusId=\"234469699\" paperTitle=\"(Lagopoulos et al., 2021)\" isShortName></Paper>\n\n## Platform Exploitation\n- **Google Scholar manipulation**: Researchers have demonstrated how easily citation metrics in Google Scholar can be artificially inflated due to its lack of control and transparency. Methods include uploading computer-generated fake papers, modifying previously published papers with added references, and creating duplicate papers. <Paper corpusId=\"2345759\" paperTitle=\"(Lopez-Cozar et al., 2013)\" isShortName></Paper> <Paper corpusId=\"7247536\" paperTitle=\"(Beel et al., 2010)\" isShortName></Paper>\n- **Research networking site manipulation**: Recent evidence shows citation counts can be artificially inflated through paid services that generate fake papers on platforms like ResearchGate, which then get indexed by Google Scholar. <Paper corpusId=\"277452706\" paperTitle=\"(Al-Sinani et al., 2025)\" isShortName></Paper>\n\n## Publication Strategy Manipulation\n- **Survey and review article proliferation**: Some researchers focus on publishing review articles, which typically attract more citations than original research, to boost their metrics. <Paper corpusId=\"277626613\" paperTitle=\"(Aguilar-Ruiz, 2025)\" isShortName></Paper>\n- **Reference list manipulation (RLM)**: Authors add unnecessary references to papers solely to inflate perceived influence, occurring either during peer review or prior to submission. One study examining 20,803 active authors estimated that approximately 16% may have engaged in RLM to some degree. <Paper corpusId=\"252168696\" paperTitle=\"(Wren et al., 2022)\" isShortName></Paper>\n\nThe range and sophistication of these gaming tactics pose a serious challenge to the integrity of citation-based evaluation systems in academia. The ability to identify and mitigate these practices is crucial as citation metrics continue to influence career advancement, funding decisions, and research priorities. <Paper corpusId=\"51867785\" paperTitle=\"(Chakraborty et al., 2018)\" isShortName></Paper> <Paper corpusId=\"264998042\" paperTitle=\"(Ioannidis et al., 2023)\" isShortName></Paper>",
          "citations": [
            {
              "id": "(Bartneck et al., 2010)",
              "snippets": [
                "The h-index has received an enormous attention for being an indicator that measures the quality of researchers and organizations. We investigate to what degree authors can inflate their h-index through strategic self-citations with the help of a simulation. We extended Burrell\u2019s publication model with a procedure for placing self-citations, following three different strategies: random self-citation, recent self-citations and h-manipulating self-citations. The results show that authors can considerably inflate their h-index through self-citations. We propose the q-index as an indicator for how strategically an author has placed self-citations, and which serves as a tool to detect possible manipulation of the h-index. The results also show that the best strategy for an high h-index is publishing papers that are highly cited by others. The productivity has also a positive effect on the h-index."
              ],
              "metadata": null
            },
            {
              "id": "(Senanayake et al., 2015)",
              "snippets": [
                "The manipulative authors had 70.6 papers per author on average and 256.2 citations per author on average, while the corresponding numbers for non-manipulative authors were 35.4 and 130.2. Therefore, it is clear that the manipulative authors had a clear advantage in terms of paper and citation counts. Indeed, as portrayed in Fig 3, the spread of the h-index for nonmanipulative authors and manipulative authors provide evidence to the fact that manipulative authors can indeed massage their h-index by publishing low impact papers with the sole purpose of referencing their previous work."
              ],
              "metadata": null
            },
            {
              "id": "(Lopez-Cozar et al., 2013)",
              "snippets": [
                "The inclusion of bibliometric tools applied in an uncontrolled environment as GS proposes, has led to another type of critical studies experimenting on their capacity to discern academic content from faked content. At this point we must refer to the study undertaken by (132836892) and his inexistent researcher Ike Antkare who proved how easily computer generated tools for research evaluation can be manipulated. In similar studies, Beel, Gipp & Wilde (2010) and Beel & Gipp (2010) tested different procedures with which to influence GS' results and obtain higher ranking positions and hence, more visibility. Among others, they made use of the Scigen software (http://pdos.csail.mit.edu/scigen/) to create fake papers, they included modifications of previously published papers adding new references, as well as duplicates of other papers. Such papers alerted on the ease to manipulate the GS search engine",
                "Although malpractices have also been reported in other databases due to the inclusion of the so-called 'predatory publishers' (Harzing, 2012) or simply by manipulating journals' Impact Factor (Opatrn\u00fd, 2008), the lack of any type of control or transparency of GS is certainly worrying as this tool is becoming more and more popular within the research community (Bartneck et al., 2010)",
                "This exposes the vulnerability of the product if editors and researchers are tempted to do \"citations engineering\" and modify their hindex by excessively self-citing their papers. Indeed, as (Bartneck et al., 2010) proved, the h-index can be easily inflated by means of three possible strategies: random self-citation, recent self-citation or, in a most refined way, sending citations only to the hot zone of their publications, that is, to those which can influence this indicator. Devious editors can easily modify their journals' h-index, having a significant effect especially for those journals with small figures on which the slightest variation can have a great impact on their ranking positioning."
              ],
              "metadata": null
            },
            {
              "id": "(Lagopoulos et al., 2021)",
              "snippets": [
                "Different forms of self-citation can be observed in scientific publishing. Direct self-citation, where authors cite their previous work, are the ones most associated with the term. Co-author and collaborative self-citation come as an extension to direct self-citation adding more complexity to the self-citation network. Coercive citation, where reviewers ask authors to include references to their work, are considered the most difficult to detect, and along with the journal/institution self-citations they complete the list of the most common types of self-citation [Ioannidis, 2015].\n\nThe scientific impact of an individual, an institution, or a publisher is typically determined by quantitative metrics and indices [Wren and Georgescu, 2020]. Citation count is the prevalent metric of recognition in science, while qualitative metrics are discarded as hard to formulate and employ [Fortunato et al., 2018]. Promotions and awards are granted to highly cited researchers, and journals with high impact factors are considered to be the most appealing and prestigious ones. Yet, most of the metrics such as citation count, h-index, and impact factor do not investigate self-citations. Thus, efforts to game or hack such metrics are made to increase the visibility of a research piece and consequently to act as advertisement and recognition for the author/publisher/institution [Szomszor et al., 2020, Copiello, 2019]."
              ],
              "metadata": null
            },
            {
              "id": "(Ioannidis et al., 2023)",
              "snippets": [
                "However, misuse and gaming of metrics are rampant (Ioannidis et al., 2020)(Fire et al., 2018). The urge to \"publish or perish\" (or even \"get cited or perish\") creates an environment where gaming of metrics is amply incentivized. A whole generation of old and new tricks try to make CVs and their impact look good and impactful-better and more impactful than they really are. Many of these gaming tricks can reach extravagant levels, as in the case of paper mills, massive self-citations, or citation cartels (Christopher, 2021)(Noorden et al., 2019)(Fister et al., 2016)",
                ".Self-citation practices may take also complex forms. Occasionally, the authors may collude to cite each other's works, even though they are not co-authors. Such citation cartels (citation farms) usually involve a small number of authors. Flow of citations may not necessarily be equally towards all members of the cartel. For instance, one or a few members may be cited, while the others may enjoy other repayments. The members of the citation farm may be in different institutions and countries. Again, quantitative metrics is the best way to diagnose a cartel. Usually, a large number of scientists cite one author's work and citations from each citing author account for a very small portion of the total citations. Conversely, in a citation farm, a handful of citing authors may account for > 50% or even > 80% of the citations received.\n\nSome of the inappropriate self-citing or citation farming behavior may even aim to inflate selectively some specific citation metric considered most important. For example, the Hirsch h-index has enjoyed inordinate popularity since its introduction in 2005 [35]. H-index can be more easily gamed than the number of total citations",
                ".many of the gaming practices used for single scientists have equivalents for gaming at the journal level, e.g., coercive journal self-citation (requests by the editor to cite other papers published by the journal) and citation cartels involving journals rather than single authors (\"citation stocking\") (Noorden, 2013)."
              ],
              "metadata": null
            },
            {
              "id": "(Fister et al., 2016)",
              "snippets": [
                "In this perspective, our goal is to present and elucidate a thus far largely overlooked problem that is arising in scientific publishing, namely the identification and discovery of citation cartels in citation networks. Taking from the well-known definition of a community in the realm of network science, namely that people within a community share significantly more links with each other as they do outside of this community, we propose that citation cartels are defined as groups of authors that cite each other disproportionately more than they do other groups of authors that work on the same subject. Evidently, the identification of citation cartels is somewhat different, although similar to the identification of communities in networks. We systematically expose the problem, provide theoretical examples, and outline an algorithmic guide on how to approach the subject."
              ],
              "metadata": null
            },
            {
              "id": "(2015)",
              "snippets": [
                "The increasing importance of this measure has spawned a whole range of tips and tricks for artificially boosting citation scores (Falagas et al., 2008), including, recently, the formation of \"citation cartels\" (Davis, 2012;(1999) in which journals collude to cite one another. In an extreme example, a single scholar's questionable editorial practices were enough to catapult the University of Alexandria-\"not even the best university in Alexandria\" into the Times Higher Education top 200 rankings (Guttenplan, 2010, para. 6). For every extreme case like this, there are likely many more going undetected."
              ],
              "metadata": null
            },
            {
              "id": "(Liu et al., 2022)",
              "snippets": [
                "It is well known that citation count is an important index to measure academic entities such as papers, journals, and scholars (Xia et al., 2017), (Kong et al., 2020). Indeed, references in the paper are often overlooked and there is also randomness to a certain extent in citation behavior. Prabha (Prabha, 1983) has pointed out that only less than 1/3 references in each paper are mandatory. Moustafa (Moustafa, 2016) introduces the idea that there are multiple inherent biases in citation practice, which will make citation-based bibliometric measures strongly flawed and defective. For example, authors can easily inflate their h-indices and distort scientific knowledge towards more conformism by manipulating selfcitations (Bartneck et al., 2010). Inappropriate co-author self-citations and collaborative self-citations can also mislead and distort scientific literature, thereby challenging scientific fairness. Reciprocal citations indicate that authors prefer to cite publications of people who cite their own work rather than those who do not (Corbyn, 2010). Moreover, to inflate the journal impact factor, some editors have encouraged authors to cite the journal's own papers, which will lead to journal citation stacking. Previous studies have explored the relationships between journal impact factors and self-citations (Mimouni et al., 2016). Another similar kind of citations is called coercive induced self-citation (Ioannidis, 2015), which may occur in reviewers, grants, advisors, etc. \n\nUnder pressure to ensure that journals/authors maintain high-value bibliometric indicators, i.e., JIF and h-index, some editors have artificially generated citations for their journals/papers by forcing authors to submit papers or by writing reviews. \"Citation cartel\" refers to the behavior of journal groups exchanging citations with excessive frequency (1999). Increasing competition in the academic publishing market has led to such practice becoming more common in recent years. Many instances of cartels have been reported. For instance, Clarivate Analytics, the publisher of the annual Journal Citation Report (JCR), has featured and suspended journals which distort JIF caused by different anomalous citation patterns 1 . The Scholarly Kitchen also reports citation cartel cases 2 . Recipient journals can enhance their JIF by up to 94% by receiving citations from donor journals in the JIF years."
              ],
              "metadata": null
            },
            {
              "id": "(Malone et al., 2016)",
              "snippets": [
                "Criticisms of the JIF include \"gaming\" the numbers through self-citation; compulsory citations imposed at the behest of journals; questions of mathematical validity; failures to replicate the metric's calculations; and lack of comparability between disciplines (Bollen et al., 2005)(Brown, 2014)Carpenter, 2014;(Neylon et al., 2009)."
              ],
              "metadata": null
            },
            {
              "id": "(Beel et al., 2010)",
              "snippets": [
                "One reason researchers might perform academic search engine spam may be to increase citation counts of their articles and hence enhance their reputations. Citation counts are commonly used to evaluate the impact and performance of researchers and their articles. In the past, citation counts were amassed by organizations such as ISI's Web of Science. Direct manipulation of Web of Science would be difficult, as ISI checks citations in 10,000 journals from the reference lists in those journals from 1900 to the present (and throws out duplicate references in a single article). Nevertheless, some researchers are said to manipulate their citation counts with citation circles, inappropriate self-citations, etc."
              ],
              "metadata": null
            },
            {
              "id": "(Al-Sinani et al., 2025)",
              "snippets": [
                "Citation-based metrics such as the H-index and i10-index are commonly used to assess a researcher's academic impact. The H-index is defined as the maximum value h such that the researcher has published h papers each of which has been cited at least h times. It attempts to capture both productivity and citation impact in a single number. The i10-index, introduced by Google Scholar, is a simpler metric that counts the number of a researcher's publications with at least ten citations. While these metrics are convenient and widely adopted in academic evaluations, they are also vulnerable to manipulation-especially in open publishing environments where content vetting is minimal",
                ".Recent work has demonstrated that citation counts on Google Scholar can be artificially inflated through paid services, highlighting a new and concerning form of metric manipulation [11]. This practice contaminates academic databases and misleads those who rely on citation metrics for scholarly evaluation",
                ".The central risk lies in the ability of fake papers to distort citation-based impact indicators. Since ResearchGate assigns DOIs to uploaded papers -and these DOIs are often indexed by third-party platforms such as Google Scholar-citations from these questionable documents contribute directly to the H-index and i10-index of the cited authors, regardless of the citing paper's authenticity.\n\nThis phenomenon was clearly observed in the case of Anwar Mohammed, whose Google Scholar profile shows a disproportionate spike in citations originating from suspicious ResearchGate uploads, most dated 2024-2025. These documents follow consistent structural and stylistic patterns and frequently cite Mohammed's earlier work, despite lacking any thematic relevance or scholarly depth.\n\nSuch gaming of citation metrics not only misrepresents the academic influence of certain individuals but also skews global rankings, hiring decisions, grant evaluations, and editorial opportunities, many of which still rely heavily on bibliometric indicators."
              ],
              "metadata": null
            },
            {
              "id": "(Aguilar-Ruiz, 2025)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Wren et al., 2022)",
              "snippets": [
                "Although citations are used as a quantifiable, objective metric of academic influence, references could be added to a paper solely to inflate the perceived influence of a body of research. This reference list manipulation (RLM) could take place during the peer-review process, or prior to it. Surveys have estimated how many people may have been affected by coercive RLM at one time or another, but it is not known how many authors engage in RLM, nor to what degree. By examining a subset of active, highly published authors (n = 20,803) in PubMed, we find the frequency of non-self-citations (NSC) to one author coming from a single paper approximates Zipf's law. Author-centric deviations from it are approximately normally distributed, permitting deviations to be quantified statistically",
                "Between 81 (0.4%, FDR < 0.05) and 231 (1.1%, FDR < 0.10) authors are outliers on the curve, suggestive of chronic, repeated RLM. Approximately 16% of all authors may have engaged in RLM to some degree. Authors who use 18% or more of their references for self-citation are significantly more likely to have NSC Gini distortions, suggesting a potential willingness to coerce others to cite them."
              ],
              "metadata": null
            },
            {
              "id": "(Chakraborty et al., 2018)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Evidence of Citation Gaming Prevalence",
          "text": "\n## Scope and Scale of Citation Gaming\n\nEvidence suggests that citation gaming is pervasive across various platforms and academic disciplines. Statistical analyses of citation patterns have identified that approximately 16% of authors may have engaged in reference list manipulation (RLM) to some degree. <Paper corpusId=\"252168696\" paperTitle=\"(Wren et al., 2022)\" isShortName></Paper> More concerning, even among this large sample of over 20,000 active, highly published authors, between 0.4-1.1% were statistical outliers suggesting chronic, repeated manipulation of reference lists. <Paper corpusId=\"252168696\" paperTitle=\"(Wren et al., 2022)\" isShortName></Paper>\n\nThe problem appears particularly acute in less controlled environments such as Google Scholar, where multiple studies have demonstrated how remarkably easy it is to manipulate citation metrics. In one striking experiment, researchers created false documents with fabricated citations to real papers, uploading them to a university domain website. This simple manipulation resulted in 774 new citations across 129 papers, significantly increasing both author and journal h-indices with minimal effort. <Paper corpusId=\"268198\" paperTitle=\"(Lopez-Cozar et al., 2012)\" isShortName></Paper> Similar experiments have consistently demonstrated Google Scholar's vulnerability to metric manipulation through computer-generated papers, modified versions of previously published work, and duplicate papers. <Paper corpusId=\"2345759\" paperTitle=\"(Lopez-Cozar et al., 2013)\" isShortName></Paper>\n\n## Documented Cases and Their Impact\n\nCitation gaming has been observed at both individual and institutional levels. Analyses of citation patterns reveal that manipulative authors tend to have significantly more papers (70.6 vs 35.4) and citations (256.2 vs 130.2) on average compared to non-manipulative authors, demonstrating the tangible advantages gained through gaming practices. <Paper corpusId=\"440416\" paperTitle=\"(Senanayake et al., 2015)\" isShortName></Paper> \n\nInstitutional-level manipulation has also been documented. In one extreme case, questionable editorial practices by a single scholar were enough to propel the University of Alexandria\u2014described as \"not even the best university in Alexandria\"\u2014into the prestigious Times Higher Education top 200 rankings. <Paper corpusId=\"16041842\" paperTitle=\"(2015)\" isShortName></Paper> This suggests that for every exposed case of manipulation, many more likely go undetected.\n\nThe prevalence of journal-level manipulation is similarly concerning. In 2011, Thomson Reuters (now Clarivate Analytics) suspended 33 journals from its Journal Citation Report due to excessive self-citations, which in some cases contributed to as much as 90% of their Journal Impact Factor (JIF). <Paper corpusId=\"219124184\" paperTitle=\"(Jolly et al., 2020)\" isShortName></Paper> In another notable instance, the \"Brazilian citation scheme\" uncovered in 2013 led to the expulsion of many highly-ranked Brazilian journals from the Web of Science database. <Paper corpusId=\"159448771\" paperTitle=\"(Strielkowski et al., 2018)\" isShortName></Paper> Through citation cartels, recipient journals have been shown to enhance their JIF by up to 94%. <Paper corpusId=\"246748933\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>\n\n## Motivations and Factors Driving Prevalence\n\nThe widespread gaming of citation metrics appears to be driven by several factors. First, the evolution from \"publish or perish\" to \"top cited or perish\" syndromes has intensified pressure on researchers to not just publish but to be highly cited. <Paper corpusId=\"231861790\" paperTitle=\"(Abramo et al., 2021)\" isShortName></Paper> <Paper corpusId=\"264998042\" paperTitle=\"(Ioannidis et al., 2023)\" isShortName></Paper> This has been exacerbated by national research assessment exercises, funding decisions, and promotion committees that increasingly rely on citation metrics. <Paper corpusId=\"231861790\" paperTitle=\"(Abramo et al., 2021)\" isShortName></Paper>\n\nIn some countries, direct financial incentives further motivate citation gaming. Authors may receive bonuses for highly cited articles, with Chinese universities offering first authors an average of $43,000 for publishing in prestigious journals like Science or Nature, with top rewards reaching $165,000. <Paper corpusId=\"231861790\" paperTitle=\"(Abramo et al., 2021)\" isShortName></Paper> <Paper corpusId=\"35245045\" paperTitle=\"(Abritis et al., 2017)\" isShortName></Paper>\n\n## Breadth of Gaming Techniques in Practice\n\nRecent research has uncovered increasingly sophisticated methods of metric manipulation. Beyond traditional self-citation, scholars now engage in orchestrated citation patterns that can be small-scale (involving a single author and close collaborators) or large-scale (involving extensive collaborations among many co-authors). <Paper corpusId=\"270764785\" paperTitle=\"(Evdaimon et al., 2024)\" isShortName></Paper> \n\nA particularly concerning emerging trend involves paid services that generate fake papers with citations to legitimate authors. These papers are uploaded to platforms like ResearchGate, which assigns them DOIs that are subsequently indexed by Google Scholar, artificially inflating authors' metrics. <Paper corpusId=\"277452706\" paperTitle=\"(Al-Sinani et al., 2025)\" isShortName></Paper> This phenomenon has been described as \"citation engineering,\" where authors publish low-impact papers with the sole purpose of referencing their previous work. <Paper corpusId=\"440416\" paperTitle=\"(Senanayake et al., 2015)\" isShortName></Paper>\n\nThe research community appears widely aware of these potential abuses. A 2010 survey found that 71% of respondents feared colleagues could \"game\" or \"cheat\" their way to better evaluations at their institutions. <Paper corpusId=\"263979729\" paperTitle=\"(Edwards et al., 2017)\" isShortName></Paper> This widespread concern suggests that the gaming of citation metrics is not just possible but is perceived to be common practice in academia.\n\nOverall, the evidence indicates that citation gaming is not limited to isolated incidents but represents a systemic challenge across academic publishing. The phenomenon has been characterized as \"metric gaming up to the point of absurdity,\" with fraudsters publishing nonsensical algorithmically generated papers featuring genuine references. <Paper corpusId=\"236374033\" paperTitle=\"(Cabanac et al., 2021)\" isShortName></Paper> This underscores the need for more robust evaluation systems that can detect and discourage such practices.",
          "citations": [
            {
              "id": "(Wren et al., 2022)",
              "snippets": [
                "Although citations are used as a quantifiable, objective metric of academic influence, references could be added to a paper solely to inflate the perceived influence of a body of research. This reference list manipulation (RLM) could take place during the peer-review process, or prior to it. Surveys have estimated how many people may have been affected by coercive RLM at one time or another, but it is not known how many authors engage in RLM, nor to what degree. By examining a subset of active, highly published authors (n = 20,803) in PubMed, we find the frequency of non-self-citations (NSC) to one author coming from a single paper approximates Zipf's law. Author-centric deviations from it are approximately normally distributed, permitting deviations to be quantified statistically",
                "Between 81 (0.4%, FDR < 0.05) and 231 (1.1%, FDR < 0.10) authors are outliers on the curve, suggestive of chronic, repeated RLM. Approximately 16% of all authors may have engaged in RLM to some degree. Authors who use 18% or more of their references for self-citation are significantly more likely to have NSC Gini distortions, suggesting a potential willingness to coerce others to cite them."
              ],
              "metadata": null
            },
            {
              "id": "(Lopez-Cozar et al., 2012)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Lopez-Cozar et al., 2013)",
              "snippets": [
                "The inclusion of bibliometric tools applied in an uncontrolled environment as GS proposes, has led to another type of critical studies experimenting on their capacity to discern academic content from faked content. At this point we must refer to the study undertaken by (132836892) and his inexistent researcher Ike Antkare who proved how easily computer generated tools for research evaluation can be manipulated. In similar studies, Beel, Gipp & Wilde (2010) and Beel & Gipp (2010) tested different procedures with which to influence GS' results and obtain higher ranking positions and hence, more visibility. Among others, they made use of the Scigen software (http://pdos.csail.mit.edu/scigen/) to create fake papers, they included modifications of previously published papers adding new references, as well as duplicates of other papers. Such papers alerted on the ease to manipulate the GS search engine",
                "Although malpractices have also been reported in other databases due to the inclusion of the so-called 'predatory publishers' (Harzing, 2012) or simply by manipulating journals' Impact Factor (Opatrn\u00fd, 2008), the lack of any type of control or transparency of GS is certainly worrying as this tool is becoming more and more popular within the research community (Bartneck et al., 2010)",
                "This exposes the vulnerability of the product if editors and researchers are tempted to do \"citations engineering\" and modify their hindex by excessively self-citing their papers. Indeed, as (Bartneck et al., 2010) proved, the h-index can be easily inflated by means of three possible strategies: random self-citation, recent self-citation or, in a most refined way, sending citations only to the hot zone of their publications, that is, to those which can influence this indicator. Devious editors can easily modify their journals' h-index, having a significant effect especially for those journals with small figures on which the slightest variation can have a great impact on their ranking positioning."
              ],
              "metadata": null
            },
            {
              "id": "(Senanayake et al., 2015)",
              "snippets": [
                "The manipulative authors had 70.6 papers per author on average and 256.2 citations per author on average, while the corresponding numbers for non-manipulative authors were 35.4 and 130.2. Therefore, it is clear that the manipulative authors had a clear advantage in terms of paper and citation counts. Indeed, as portrayed in Fig 3, the spread of the h-index for nonmanipulative authors and manipulative authors provide evidence to the fact that manipulative authors can indeed massage their h-index by publishing low impact papers with the sole purpose of referencing their previous work."
              ],
              "metadata": null
            },
            {
              "id": "(2015)",
              "snippets": [
                "The increasing importance of this measure has spawned a whole range of tips and tricks for artificially boosting citation scores (Falagas et al., 2008), including, recently, the formation of \"citation cartels\" (Davis, 2012;(1999) in which journals collude to cite one another. In an extreme example, a single scholar's questionable editorial practices were enough to catapult the University of Alexandria-\"not even the best university in Alexandria\" into the Times Higher Education top 200 rankings (Guttenplan, 2010, para. 6). For every extreme case like this, there are likely many more going undetected."
              ],
              "metadata": null
            },
            {
              "id": "(Jolly et al., 2020)",
              "snippets": [
                "citation indices which are more foolproof to collusion using inappropriate selfcitations. Bartneck and Kokkelmans (Bartneck et al., 2010) discussed how h-index can be inflated by authors by manipulating self-citation. W. Flatt et al. (Flatt et al., 2017), in their study on analyzing the effect of excessive self-citation, argued for a new metric based on more transparency to help curb excessive self-citations, which may have a negative effect on our ability to truly analyze the impact and contributions of a scientific research in its research domain. However, limited work has been done in the field of analyzing citation stack.\n\nIn 2011, Thomson Reuters [5] (currently known as Clarivate Analytics) suspended 33 journals from its Journal Citation Report due to a very high rate of self-citations, which contributed to as much as 90% of the JIF of those journals. Therefore, anomalous citation detection is an important task in bibliometrics."
              ],
              "metadata": null
            },
            {
              "id": "(Strielkowski et al., 2018)",
              "snippets": [
                "In 2013, a so-called \"Brazilian citation scheme\" was uncovered by Thomson Reuters (the owner of Web of Science database at the time) which produced a big scandal and led to the expulsion of many Brazilian highly-ranked journals from Web of Science database (Noorden, 2013)."
              ],
              "metadata": null
            },
            {
              "id": "(Liu et al., 2022)",
              "snippets": [
                "It is well known that citation count is an important index to measure academic entities such as papers, journals, and scholars (Xia et al., 2017), (Kong et al., 2020). Indeed, references in the paper are often overlooked and there is also randomness to a certain extent in citation behavior. Prabha (Prabha, 1983) has pointed out that only less than 1/3 references in each paper are mandatory. Moustafa (Moustafa, 2016) introduces the idea that there are multiple inherent biases in citation practice, which will make citation-based bibliometric measures strongly flawed and defective. For example, authors can easily inflate their h-indices and distort scientific knowledge towards more conformism by manipulating selfcitations (Bartneck et al., 2010). Inappropriate co-author self-citations and collaborative self-citations can also mislead and distort scientific literature, thereby challenging scientific fairness. Reciprocal citations indicate that authors prefer to cite publications of people who cite their own work rather than those who do not (Corbyn, 2010). Moreover, to inflate the journal impact factor, some editors have encouraged authors to cite the journal's own papers, which will lead to journal citation stacking. Previous studies have explored the relationships between journal impact factors and self-citations (Mimouni et al., 2016). Another similar kind of citations is called coercive induced self-citation (Ioannidis, 2015), which may occur in reviewers, grants, advisors, etc. \n\nUnder pressure to ensure that journals/authors maintain high-value bibliometric indicators, i.e., JIF and h-index, some editors have artificially generated citations for their journals/papers by forcing authors to submit papers or by writing reviews. \"Citation cartel\" refers to the behavior of journal groups exchanging citations with excessive frequency (1999). Increasing competition in the academic publishing market has led to such practice becoming more common in recent years. Many instances of cartels have been reported. For instance, Clarivate Analytics, the publisher of the annual Journal Citation Report (JCR), has featured and suspended journals which distort JIF caused by different anomalous citation patterns 1 . The Scholarly Kitchen also reports citation cartel cases 2 . Recipient journals can enhance their JIF by up to 94% by receiving citations from donor journals in the JIF years."
              ],
              "metadata": null
            },
            {
              "id": "(Abramo et al., 2021)",
              "snippets": [
                "In recent years, the \"publish or perish\" syndrome (Fanelli, 2010)(Dalen et al., 2012)Neill, 2018) has evolved into the more sophisticated \"top cited or perish\" syndrome (Chandler et al., 2002)(Gill, 2016)(S\u00e1 et al., 2013). Researchers and managers are immediately aware when countries introduce national research assessment exercises using citation metrics, and attentive to the fact that funding agencies and tenure committees use citations in decision making. In some nations, authors are even paid bonuses for highly cited articles (Abritis et al., 2017)(Stephan et al., 2017)(Tonta et al., 2019). This has led observers and scholars to criticize and question the potential phenomenon of self-citation gaming by scientists (Biagioli, 2016;Scarpa, Bianco, & Tagliafico, 2018;Seeber, Cattaneo, Meoli, & Malighetti, 2019;Baccini, De Nicolao, & Petrovich, 2019;D'Antuono & Ciavarella, 2019)."
              ],
              "metadata": null
            },
            {
              "id": "(Ioannidis et al., 2023)",
              "snippets": [
                "However, misuse and gaming of metrics are rampant (Ioannidis et al., 2020)(Fire et al., 2018). The urge to \"publish or perish\" (or even \"get cited or perish\") creates an environment where gaming of metrics is amply incentivized. A whole generation of old and new tricks try to make CVs and their impact look good and impactful-better and more impactful than they really are. Many of these gaming tricks can reach extravagant levels, as in the case of paper mills, massive self-citations, or citation cartels (Christopher, 2021)(Noorden et al., 2019)(Fister et al., 2016)",
                ".Self-citation practices may take also complex forms. Occasionally, the authors may collude to cite each other's works, even though they are not co-authors. Such citation cartels (citation farms) usually involve a small number of authors. Flow of citations may not necessarily be equally towards all members of the cartel. For instance, one or a few members may be cited, while the others may enjoy other repayments. The members of the citation farm may be in different institutions and countries. Again, quantitative metrics is the best way to diagnose a cartel. Usually, a large number of scientists cite one author's work and citations from each citing author account for a very small portion of the total citations. Conversely, in a citation farm, a handful of citing authors may account for > 50% or even > 80% of the citations received.\n\nSome of the inappropriate self-citing or citation farming behavior may even aim to inflate selectively some specific citation metric considered most important. For example, the Hirsch h-index has enjoyed inordinate popularity since its introduction in 2005 [35]. H-index can be more easily gamed than the number of total citations",
                ".many of the gaming practices used for single scientists have equivalents for gaming at the journal level, e.g., coercive journal self-citation (requests by the editor to cite other papers published by the journal) and citation cartels involving journals rather than single authors (\"citation stocking\") (Noorden, 2013)."
              ],
              "metadata": null
            },
            {
              "id": "(Abritis et al., 2017)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Evdaimon et al., 2024)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Al-Sinani et al., 2025)",
              "snippets": [
                "Citation-based metrics such as the H-index and i10-index are commonly used to assess a researcher's academic impact. The H-index is defined as the maximum value h such that the researcher has published h papers each of which has been cited at least h times. It attempts to capture both productivity and citation impact in a single number. The i10-index, introduced by Google Scholar, is a simpler metric that counts the number of a researcher's publications with at least ten citations. While these metrics are convenient and widely adopted in academic evaluations, they are also vulnerable to manipulation-especially in open publishing environments where content vetting is minimal",
                ".Recent work has demonstrated that citation counts on Google Scholar can be artificially inflated through paid services, highlighting a new and concerning form of metric manipulation [11]. This practice contaminates academic databases and misleads those who rely on citation metrics for scholarly evaluation",
                ".The central risk lies in the ability of fake papers to distort citation-based impact indicators. Since ResearchGate assigns DOIs to uploaded papers -and these DOIs are often indexed by third-party platforms such as Google Scholar-citations from these questionable documents contribute directly to the H-index and i10-index of the cited authors, regardless of the citing paper's authenticity.\n\nThis phenomenon was clearly observed in the case of Anwar Mohammed, whose Google Scholar profile shows a disproportionate spike in citations originating from suspicious ResearchGate uploads, most dated 2024-2025. These documents follow consistent structural and stylistic patterns and frequently cite Mohammed's earlier work, despite lacking any thematic relevance or scholarly depth.\n\nSuch gaming of citation metrics not only misrepresents the academic influence of certain individuals but also skews global rankings, hiring decisions, grant evaluations, and editorial opportunities, many of which still rely heavily on bibliometric indicators."
              ],
              "metadata": null
            },
            {
              "id": "(Edwards et al., 2017)",
              "snippets": [
                "Ultimately, the well-intentioned use of quantitative metrics may create inequities and outcomes worse than the systems they replaced. Specifically, if rewards are disproportionally given to individuals manipulating their metrics, problems of the old subjective paradigms (e.g., old-boys' networks) may be tame by comparison. In a 2010 survey, 71% of respondents stated that they feared colleagues can ''game'' or ''cheat'' their way into better evaluations at their institutions (Abbott et al., 2010), demonstrating that scientists are acutely attuned to the possibility of abuses in the current system."
              ],
              "metadata": null
            },
            {
              "id": "(Cabanac et al., 2021)",
              "snippets": [
                "This work reveals metric gaming up to the point of absurdity: fraudsters publish nonsensical algorithmically generated papers featuring genuine references. It stresses the need to screen papers for nonsense before peer\u2010review and chase citation manipulation in published papers. Overall, this is yet another illustration of the harmful effects of the pressure to publish or perish."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Impact and Consequences of Citation Gaming",
          "text": "\nCitation gaming has far-reaching consequences that extend beyond just inflating numbers, affecting both individual researchers and the broader scientific enterprise. These impacts can be categorized into several key areas:\n\n## Career Advancement and Resource Allocation Inequities\n\nThe manipulation of citation metrics creates significant career advantages for those who engage in gaming practices. In academic environments where evaluations increasingly rely on citation-based indicators, researchers who artificially inflate their metrics gain unfair advantages in hiring, promotion, and tenure decisions <Paper corpusId=\"263979729\" paperTitle=\"(Edwards et al., 2017)\" isShortName></Paper>. This concern is widespread, with a 2010 survey finding that 71% of respondents feared colleagues could \"game\" or \"cheat\" their way to better evaluations at their institutions <Paper corpusId=\"263979729\" paperTitle=\"(Edwards et al., 2017)\" isShortName></Paper>.\n\nThe stakes are particularly high when direct financial incentives are tied to citation metrics. In some countries, authors receive substantial bonuses for highly cited articles, with Chinese universities offering first authors an average of $43,000 for publishing in prestigious journals like Science or Nature, and top rewards reaching $165,000 <Paper corpusId=\"231861790\" paperTitle=\"(Abramo et al., 2021)\" isShortName></Paper> <Paper corpusId=\"35245045\" paperTitle=\"(Abritis et al., 2017)\" isShortName></Paper>. These incentives further motivate citation manipulation tactics.\n\nGaming practices also distort the allocation of research funding and academic recognition. When funding agencies and tenure committees use citations in decision making <Paper corpusId=\"231861790\" paperTitle=\"(Abramo et al., 2021)\" isShortName></Paper> <Paper corpusId=\"154806755\" paperTitle=\"(Sa et al., 2013)\" isShortName></Paper>, those who engage in strategic citation practices receive disproportionate resources. This diverts research funding and opportunities away from researchers producing genuinely impactful work but who are less skilled at manipulating metrics.\n\n## Distortion of Scientific Knowledge Development\n\nCitation gaming practices significantly impair the knowledge development process that citations are meant to facilitate. By artificially inflating certain citation paths, gaming creates a distorted map of scientific influence that misrepresents the actual intellectual lineage of ideas <Paper corpusId=\"248512893\" paperTitle=\"(Nishioka et al., 2022)\" isShortName></Paper>. This distortion is particularly problematic because citations serve as a fundamental mechanism for tracing the evolution of science <Paper corpusId=\"243933321\" paperTitle=\"(Ghosal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"26784280\" paperTitle=\"(Camacho-Minano et al., 2009)\" isShortName></Paper>.\n\nThe prevalence of citation bias\u2014where papers are cited based on non-scholarly attributes rather than scientific merit\u2014further compounds this distortion <Paper corpusId=\"248512893\" paperTitle=\"(Nishioka et al., 2022)\" isShortName></Paper>. For example, gender-based citation gaps persist even in more diverse academic fields, where work by women tends to be undercited <Paper corpusId=\"248512893\" paperTitle=\"(Nishioka et al., 2022)\" isShortName></Paper> <Paper corpusId=\"5002988\" paperTitle=\"(Caplar et al., 2016)\" isShortName></Paper> <Paper corpusId=\"52265401\" paperTitle=\"(Dion et al., 2018)\" isShortName></Paper>.\n\n## Institutional Reputation and Ranking Manipulation\n\nCitation gaming extends beyond individual researchers to affect institutional rankings. In one extreme case, questionable editorial practices by a single scholar propelled the University of Alexandria into the prestigious Times Higher Education top 200 rankings, despite it being \"not even the best university in Alexandria\" <Paper corpusId=\"16041842\" paperTitle=\"(2015)\" isShortName></Paper>. This suggests that for every detected case of manipulation, many more likely remain undiscovered, undermining the credibility of institutional ranking systems.\n\n## Compromise of Research Quality and Integrity\n\nThe pressure to accumulate citations has evolved from the \"publish or perish\" syndrome to the more sophisticated \"top cited or perish\" syndrome <Paper corpusId=\"231861790\" paperTitle=\"(Abramo et al., 2021)\" isShortName></Paper> <Paper corpusId=\"205439505\" paperTitle=\"(Dalen et al., 2012)\" isShortName></Paper> <Paper corpusId=\"313222\" paperTitle=\"(Fanelli, 2010)\" isShortName></Paper>. This shift encourages researchers to focus on strategies for obtaining citations rather than on conducting rigorous research, potentially compromising research quality.\n\nWhile self-citation can be legitimate when building on previous work, the strategic use of self-citation to advance careers <Paper corpusId=\"212743070\" paperTitle=\"(Kacem et al., 2019)\" isShortName></Paper> distorts evaluation systems. Even when metrics are \"curated\" to remove direct self-citations, this doesn't prevent indirect manipulation, as each self-citation appears to increase citations from others by approximately one after a year and by about three after five years <Paper corpusId=\"212743070\" paperTitle=\"(Kacem et al., 2019)\" isShortName></Paper>.\n\n## Erosion of Trust in Academic Evaluation and Publication\n\nThe prevalence of citation gaming threatens the integrity of scientific quality assessment and diminishes the credibility of academic evaluation systems <Paper corpusId=\"51867785\" paperTitle=\"(Chakraborty et al., 2018)\" isShortName></Paper>. Problems such as coercive citations, anomalous citations, citation manipulation, and discriminatory citation practices have \"infested\" the academic community <Paper corpusId=\"243933321\" paperTitle=\"(Ghosal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"2578647\" paperTitle=\"(Bartneck et al., 2010)\" isShortName></Paper> <Paper corpusId=\"506530\" paperTitle=\"(Bai et al., 2016)\" isShortName></Paper>.\n\nThe growing sophistication of gaming tactics, such as citation cartels between journals that can increase impact factors by up to 94% <Paper corpusId=\"264998042\" paperTitle=\"(Ioannidis et al., 2023)\" isShortName></Paper>, undermines faith in publication metrics. Recent developments include paid services generating fake papers with citations to legitimate authors, which are uploaded to platforms like ResearchGate and subsequently indexed by Google Scholar <Paper corpusId=\"277452706\" paperTitle=\"(Al-Sinani et al., 2025)\" isShortName></Paper>.\n\n## Skewed Research Focus and Publication Strategies\n\nCitation gaming has influenced publication strategies, with researchers increasingly producing specific types of papers more likely to attract citations. The proliferation of survey and review articles, which typically receive more citations than original research, has skewed citation metrics like impact factor and h-index <Paper corpusId=\"277626613\" paperTitle=\"(Aguilar-Ruiz, 2025)\" isShortName></Paper>. This shift diverts scholarly effort away from original research toward producing publication types optimized for citation accumulation.\n\nUltimately, the well-intentioned use of quantitative metrics may create outcomes worse than the subjective evaluation systems they were designed to replace <Paper corpusId=\"263979729\" paperTitle=\"(Edwards et al., 2017)\" isShortName></Paper>. When rewards disproportionately benefit those who manipulate metrics, the scientific community faces a fundamental challenge to its integrity and effectiveness <Paper corpusId=\"263979729\" paperTitle=\"(Edwards et al., 2017)\" isShortName></Paper> <Paper corpusId=\"52336477\" paperTitle=\"(Fire et al., 2018)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Edwards et al., 2017)",
              "snippets": [
                "Ultimately, the well-intentioned use of quantitative metrics may create inequities and outcomes worse than the systems they replaced. Specifically, if rewards are disproportionally given to individuals manipulating their metrics, problems of the old subjective paradigms (e.g., old-boys' networks) may be tame by comparison. In a 2010 survey, 71% of respondents stated that they feared colleagues can ''game'' or ''cheat'' their way into better evaluations at their institutions (Abbott et al., 2010), demonstrating that scientists are acutely attuned to the possibility of abuses in the current system."
              ],
              "metadata": null
            },
            {
              "id": "(Abramo et al., 2021)",
              "snippets": [
                "In recent years, the \"publish or perish\" syndrome (Fanelli, 2010)(Dalen et al., 2012)Neill, 2018) has evolved into the more sophisticated \"top cited or perish\" syndrome (Chandler et al., 2002)(Gill, 2016)(S\u00e1 et al., 2013). Researchers and managers are immediately aware when countries introduce national research assessment exercises using citation metrics, and attentive to the fact that funding agencies and tenure committees use citations in decision making. In some nations, authors are even paid bonuses for highly cited articles (Abritis et al., 2017)(Stephan et al., 2017)(Tonta et al., 2019). This has led observers and scholars to criticize and question the potential phenomenon of self-citation gaming by scientists (Biagioli, 2016;Scarpa, Bianco, & Tagliafico, 2018;Seeber, Cattaneo, Meoli, & Malighetti, 2019;Baccini, De Nicolao, & Petrovich, 2019;D'Antuono & Ciavarella, 2019)."
              ],
              "metadata": null
            },
            {
              "id": "(Abritis et al., 2017)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Sa et al., 2013)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Nishioka et al., 2022)",
              "snippets": [
                "Citing is an important aspect of scientific discourse and important for quantifying the scientific impact quantification of researchers. Widely used importance metrics, such as the citation count and the h-index (Hirsch, 2005), are based on citations. They are sometimes used to judge the quality of research presented by an article (263083144). However, several works have observed that publications are cited not only based on the pure scholarly contributions but also based on nonscholarly attributes such as gender, author affiliation, and funding. For instance, articles authored by women might be under-cited (Caplar et al., 2016)(Dion et al., 2018)[35]. Such distortions concerning citations-also called \"citation bias\"-can distort the perception of available scholarly contributions among users of publications (Jannot et al., 2013)."
              ],
              "metadata": null
            },
            {
              "id": "(Ghosal et al., 2021)",
              "snippets": [
                "Problems such as coercive citations (Wilhite et al., 2012), anomalous citations (Bai et al., 2016), citation manipulation (Bartneck et al., 2010), rich get richer effects (Ronda-Pupo et al., 2017), and discriminatory citation practices (Camacho\u2010Mi\u00f1ano et al., 2009)) have infested the academic community. However, in spite of all these known issues, citation counts and h-indices still remain the measures of research impact and tools for academic incentives, though long-debated by many (Cerd\u00e1, Nieto, & Campos, 2009;Lalo\u00eb & Mosseri, 2009)."
              ],
              "metadata": null
            },
            {
              "id": "(Camacho-Minano et al., 2009)",
              "snippets": [
                "Why authors choose some references in preference to others is a question that is still not wholly answered despite its being of interest to scientists.The relevance of references is twofold: They are a mechanism for tracing the evolution of science, and because they enhance the image of the cited authors, citations are a widely known and used indicator of scientific endeavor. Following an extensive review of the literature, we selected all papers that seek to answer the central question and demonstrate that the existing theories are not sufficient: Neither citation nor indicator theory provides a complete and convincing answer. Some perspectives in this arena remain, which are isolated from the core literature. The purpose of this article is to offer a fresh perspective on a 30-year-old problem by extending the context of the discussion. We suggest reviving the discussion about citation theories with a new perspective, that of the readers, by layers or phases, in the final choice of references, allowing for a new classification in which any paper, to date, could be included."
              ],
              "metadata": null
            },
            {
              "id": "(Caplar et al., 2016)",
              "snippets": [
                "Numerous studies across different research fields have shown that both male and female referees consistently give higher scores to work done by men than to identical work done by women1,2,3. In addition, women are under-represented in prestigious publications and authorship positions4,5 and women receive ~10% fewer citations6,7. In astronomy, similar biases have been measured in conference participation8,9 and success rates for telescope proposals10,11. Even though the number of doctorate degrees awarded to women is constantly increasing, women still tend to be under-represented in faculty positions12. Spurred by these findings, we measure the role of gender in the number of citations that papers receive in astronomy. To account for the fact that the properties of papers written by men and women differ intrinsically, we use a random forest algorithm to control for the non-gender-specific properties of these papers. Here we show that papers authored by women receive 10.4 \u00b1 0.9% fewer citations than would be expected if the papers with the same non-gender-specific properties were written by men. Gender discrimination is very much an issue in academia generally and in astronomy specifically. Through machine learning techniques, astronomy papers authored by women are shown to have 10% systematically fewer citations than those authored by men."
              ],
              "metadata": null
            },
            {
              "id": "(Dion et al., 2018)",
              "snippets": [
                "Accumulated evidence identifies discernible gender gaps across many dimensions of professional academic careers including salaries, publication rates, journal placement, career progress, and academic service. Recent work in political science also reveals gender gaps in citations, with articles written by men citing work by other male scholars more often than work by female scholars. This study estimates the gender gap in citations across political science subfields and across methodological subfields within political science, sociology, and economics. The research design captures variance across research areas in terms of the underlying distribution of female scholars. We expect that subfields within political science and social science disciplines with more women will have smaller gender citation gaps, a reduction of the \"Matthew effect\" where men\u2019s research is viewed as the most central and important in a field. However, gender citation gaps may persist if a \"Matilda effect\" occurs whereby women\u2019s research is viewed as less important or their ideas are attributed to male scholars, even as a field becomes more diverse. Analysing all articles published from 2007\u20132016 in several journals, we find that female scholars are significantly more likely than mixed gender or male author teams to cite research by their female peers, but that these citation rates vary depending on the overall distribution of women in their field. More gender diverse subfields and disciplines produce smaller gender citation gaps, consistent with a reduction in the \"Matthew effect\". However, we also observe undercitation of work by women, even in journals that publish mostly female authors. While improvements in gender diversity in academia increase the visibility and impact of scholarly work by women, implicit biases in citation practices in the social sciences persist."
              ],
              "metadata": null
            },
            {
              "id": "(2015)",
              "snippets": [
                "The increasing importance of this measure has spawned a whole range of tips and tricks for artificially boosting citation scores (Falagas et al., 2008), including, recently, the formation of \"citation cartels\" (Davis, 2012;(1999) in which journals collude to cite one another. In an extreme example, a single scholar's questionable editorial practices were enough to catapult the University of Alexandria-\"not even the best university in Alexandria\" into the Times Higher Education top 200 rankings (Guttenplan, 2010, para. 6). For every extreme case like this, there are likely many more going undetected."
              ],
              "metadata": null
            },
            {
              "id": "(Dalen et al., 2012)",
              "snippets": [
                "How does publication pressure in modern-day universities affect the intrinsic and extrinsic rewards in science? By using a worldwide survey among demographers in developed and developing countries, the authors show that the large majority perceive the publication pressure as high, but more so in Anglo-Saxon countries and to a lesser extent in Western Europe. However, scholars see both the pros (upward mobility) and cons (excessive publication and uncitedness, neglect of policy issues, etc.) of the so-called publish-or-perish culture. By measuring behavior in terms of reading and publishing, and perceived extrinsic rewards and stated intrinsic rewards of practicing science, it turns out that publication pressure negatively affects the orientation of demographers towards policy and knowledge sharing. There are no signs that the pressure affects reading and publishing outside the core discipline. \u00a9 2012 Wiley Periodicals, Inc."
              ],
              "metadata": null
            },
            {
              "id": "(Fanelli, 2010)",
              "snippets": [
                "The growing competition and \"publish or perish\" culture in academia might conflict with the objectivity and integrity of research, because it forces scientists to produce \"publishable\" results at all costs. Papers are less likely to be published and to be cited if they report \"negative\" results (results that fail to support the tested hypothesis). Therefore, if publication pressures increase scientific bias, the frequency of \"positive\" results in the literature should be higher in the more competitive and \"productive\" academic environments. This study verified this hypothesis by measuring the frequency of positive results in a large random sample of papers with a corresponding author based in the US. Across all disciplines, papers were more likely to support a tested hypothesis if their corresponding authors were working in states that, according to NSF data, produced more academic papers per capita. The size of this effect increased when controlling for state's per capita R&D expenditure and for study characteristics that previous research showed to correlate with the frequency of positive results, including discipline and methodology. Although the confounding effect of institutions' prestige could not be excluded (researchers in the more productive universities could be the most clever and successful in their experiments), these results support the hypothesis that competitive academic environments increase not only scientists' productivity but also their bias. The same phenomenon might be observed in other countries where academic competition and pressures to publish are high."
              ],
              "metadata": null
            },
            {
              "id": "(Kacem et al., 2019)",
              "snippets": [
                "Without question, this creates a real career motivation to strategically use self-citation (Seeber et al. 2019), but this does not in any way diminish the value of self-cites that result from productive, sustained, leadingedge efforts (Cooke and Donaldson 2014). When used appropriately, self-cites are equally important as cites from the surrounding community, and without tracking them it is impossible to see how scholars build on their own work.\n\nDespite this, many favor a curated form of the h-index as a response to the gaming problem. Curation involves hacking away at the citation data to neatly remove all occurrences of self-citation. While such treatment effectively silences direct attempts to boost citation scores, it does not prevent indirect manipulation and also produces undesired side effects. For example, curation ignores when authors use self-citation to attract cites from others, which is alarming given that each self-citation appears to increase the number of citations from others by about one after a year, and by about three after 5 years (Fowler and Aksnes 2007)."
              ],
              "metadata": null
            },
            {
              "id": "(Chakraborty et al., 2018)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Bartneck et al., 2010)",
              "snippets": [
                "The h-index has received an enormous attention for being an indicator that measures the quality of researchers and organizations. We investigate to what degree authors can inflate their h-index through strategic self-citations with the help of a simulation. We extended Burrell\u2019s publication model with a procedure for placing self-citations, following three different strategies: random self-citation, recent self-citations and h-manipulating self-citations. The results show that authors can considerably inflate their h-index through self-citations. We propose the q-index as an indicator for how strategically an author has placed self-citations, and which serves as a tool to detect possible manipulation of the h-index. The results also show that the best strategy for an high h-index is publishing papers that are highly cited by others. The productivity has also a positive effect on the h-index."
              ],
              "metadata": null
            },
            {
              "id": "(Bai et al., 2016)",
              "snippets": [
                "Evaluating the impact of a scholarly article is of great significance and has attracted great attentions. Although citation-based evaluation approaches have been widely used, these approaches face limitations e.g. in identifying anomalous citations patterns. This negligence would inevitably cause unfairness and inaccuracy to the article impact evaluation. In this study, in order to discover the anomalous citations and ensure the fairness and accuracy of research outcome evaluation, we investigate the citation relationships between articles using the following factors: collaboration times, the time span of collaboration, citing times and the time span of citing to weaken the relationship of Conflict of Interest (COI) in the citation network. Meanwhile, we study a special kind of COI, namely suspected COI relationship. Based on the COI relationship, we further bring forward the COIRank algorithm, an innovative scheme for accurately assessing the impact of an article. Our method distinguishes the citation strength, and utilizes PageRank and HITS algorithms to rank scholarly articles comprehensively. The experiments are conducted on the American Physical Society (APS) dataset. We find that about 80.88% articles contain contributed citations by co-authors in 26,366 articles and 75.55% articles among these articles are cited by the authors belonging to the same affiliation, indicating COI and suspected COI should not be ignored for evaluating impact of scientific papers objectively. Moreover, our experimental results demonstrate COIRank algorithm significantly outperforms the state-of-art solutions. The validity of our approach is verified by using the probability of Recommendation Intensity."
              ],
              "metadata": null
            },
            {
              "id": "(Ioannidis et al., 2023)",
              "snippets": [
                "However, misuse and gaming of metrics are rampant (Ioannidis et al., 2020)(Fire et al., 2018). The urge to \"publish or perish\" (or even \"get cited or perish\") creates an environment where gaming of metrics is amply incentivized. A whole generation of old and new tricks try to make CVs and their impact look good and impactful-better and more impactful than they really are. Many of these gaming tricks can reach extravagant levels, as in the case of paper mills, massive self-citations, or citation cartels (Christopher, 2021)(Noorden et al., 2019)(Fister et al., 2016)",
                ".Self-citation practices may take also complex forms. Occasionally, the authors may collude to cite each other's works, even though they are not co-authors. Such citation cartels (citation farms) usually involve a small number of authors. Flow of citations may not necessarily be equally towards all members of the cartel. For instance, one or a few members may be cited, while the others may enjoy other repayments. The members of the citation farm may be in different institutions and countries. Again, quantitative metrics is the best way to diagnose a cartel. Usually, a large number of scientists cite one author's work and citations from each citing author account for a very small portion of the total citations. Conversely, in a citation farm, a handful of citing authors may account for > 50% or even > 80% of the citations received.\n\nSome of the inappropriate self-citing or citation farming behavior may even aim to inflate selectively some specific citation metric considered most important. For example, the Hirsch h-index has enjoyed inordinate popularity since its introduction in 2005 [35]. H-index can be more easily gamed than the number of total citations",
                ".many of the gaming practices used for single scientists have equivalents for gaming at the journal level, e.g., coercive journal self-citation (requests by the editor to cite other papers published by the journal) and citation cartels involving journals rather than single authors (\"citation stocking\") (Noorden, 2013)."
              ],
              "metadata": null
            },
            {
              "id": "(Al-Sinani et al., 2025)",
              "snippets": [
                "Citation-based metrics such as the H-index and i10-index are commonly used to assess a researcher's academic impact. The H-index is defined as the maximum value h such that the researcher has published h papers each of which has been cited at least h times. It attempts to capture both productivity and citation impact in a single number. The i10-index, introduced by Google Scholar, is a simpler metric that counts the number of a researcher's publications with at least ten citations. While these metrics are convenient and widely adopted in academic evaluations, they are also vulnerable to manipulation-especially in open publishing environments where content vetting is minimal",
                ".Recent work has demonstrated that citation counts on Google Scholar can be artificially inflated through paid services, highlighting a new and concerning form of metric manipulation [11]. This practice contaminates academic databases and misleads those who rely on citation metrics for scholarly evaluation",
                ".The central risk lies in the ability of fake papers to distort citation-based impact indicators. Since ResearchGate assigns DOIs to uploaded papers -and these DOIs are often indexed by third-party platforms such as Google Scholar-citations from these questionable documents contribute directly to the H-index and i10-index of the cited authors, regardless of the citing paper's authenticity.\n\nThis phenomenon was clearly observed in the case of Anwar Mohammed, whose Google Scholar profile shows a disproportionate spike in citations originating from suspicious ResearchGate uploads, most dated 2024-2025. These documents follow consistent structural and stylistic patterns and frequently cite Mohammed's earlier work, despite lacking any thematic relevance or scholarly depth.\n\nSuch gaming of citation metrics not only misrepresents the academic influence of certain individuals but also skews global rankings, hiring decisions, grant evaluations, and editorial opportunities, many of which still rely heavily on bibliometric indicators."
              ],
              "metadata": null
            },
            {
              "id": "(Aguilar-Ruiz, 2025)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Fire et al., 2018)",
              "snippets": [
                "Abstract Background The academic publishing world is changing significantly, with ever-growing numbers of publications each year and shifting publishing patterns. However, the metrics used to measure academic success, such as the number of publications, citation number, and impact factor, have not changed for decades. Moreover, recent studies indicate that these metrics have become targets and follow Goodhart\u2019s Law, according to which, \"when a measure becomes a target, it ceases to be a good measure.\" Results In this study, we analyzed >120 million papers to examine how the academic publishing world has evolved over the last century, with a deeper look into the specific field of biology. Our study shows that the validity of citation-based measures is being compromised and their usefulness is lessening. In particular, the number of publications has ceased to be a good metric as a result of longer author lists, shorter papers, and surging publication numbers. Citation-based metrics, such citation number and h-index, are likewise affected by the flood of papers, self-citations, and lengthy reference lists. Measures such as a journal\u2019s impact factor have also ceased to be good metrics due to the soaring numbers of papers that are published in top journals, particularly from the same pool of authors. Moreover, by analyzing properties of >2,600 research fields, we observed that citation-based metrics are not beneficial for comparing researchers in different fields, or even in the same department. Conclusions Academic publishing has changed considerably; now we need to reconsider how we measure success."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Detection and Prevention Measures",
          "text": "\n## Detecting Citation Manipulation\n\nSeveral approaches have been developed to identify citation gaming practices. One significant tool is the q-index, which measures how strategically an author has placed self-citations, serving as an indicator to detect potential h-index manipulation <Paper corpusId=\"2578647\" paperTitle=\"(Bartneck et al., 2010)\" isShortName></Paper>. This index helps distinguish between legitimate self-citation (building on one's previous work) and strategic citation patterns designed specifically to game metrics.\n\nQuantitative analysis of citation patterns has proven effective in identifying citation cartels or \"citation farms.\" Typical citation networks show a large number of scientists citing an author's work with each citing author accounting for a small portion of total citations. In contrast, citation cartels display distinctive patterns where \"a handful of citing authors may account for > 50% or even > 80% of the citations received\" <Paper corpusId=\"264998042\" paperTitle=\"(Ioannidis et al., 2023)\" isShortName></Paper>. These statistical anomalies serve as red flags for potential manipulation.\n\nJournal-level detection has also advanced. In 2011, Thomson Reuters (now Clarivate Analytics) suspended 33 journals from its Journal Citation Report after identifying excessively high self-citation rates that contributed to as much as 90% of their Journal Impact Factors <Paper corpusId=\"219124184\" paperTitle=\"(Jolly et al., 2020)\" isShortName></Paper>. This demonstrates that anomalous citation detection has become an important aspect of bibliometric quality control.\n\nRecent research has further focused on detecting paper mill products by examining raw data associated with suspicious manuscripts. Given that paper mills need to streamline production of fake manuscripts, they often create alleged raw data using templates, which can be identified through careful examination <Paper corpusId=\"235660251\" paperTitle=\"(Christopher, 2021)\" isShortName></Paper>.\n\n## Prevention Strategies\n\nSeveral approaches have been proposed to reduce citation gaming:\n\n1. **Curated Metrics**: Some researchers advocate for \"curated\" versions of citation metrics that remove self-citations. However, this approach has limitations, as it does not prevent indirect manipulation where self-citations attract citations from others. Research shows that \"each self-citation appears to increase the number of citations from others by about one after a year, and by about three after 5 years\" <Paper corpusId=\"212743070\" paperTitle=\"(Kacem et al., 2019)\" isShortName></Paper>.\n\n2. **Alternative Metrics**: Researchers have called for new metrics centered on transparency to help curb self-promotion practices in citation <Paper corpusId=\"985526\" paperTitle=\"(Flatt et al., 2017)\" isShortName></Paper>. These alternatives aim to maintain the value of legitimate self-citation while discouraging manipulative practices.\n\n3. **Comprehensive Evaluation Frameworks**: Moving beyond singular metrics toward multi-dimensional evaluation frameworks can reduce vulnerability to gaming. This approach incorporates qualitative assessments alongside quantitative metrics to provide a more holistic picture of research impact.\n\n4. **Pre-publication Screening**: There is increasing recognition of the need to \"screen papers for nonsense before peer-review and chase citation manipulation in published papers\" <Paper corpusId=\"236374033\" paperTitle=\"(Cabanac et al., 2021)\" isShortName></Paper>. This is particularly important given the emergence of algorithmically generated papers featuring genuine references.\n\n5. **Algorithmic Detection**: New computational approaches attempt to identify citation cartels by analyzing citation networks for disproportionate citation patterns. These methods define citation cartels as \"groups of authors that cite each other disproportionately more than they do other groups of authors that work on the same subject\" <Paper corpusId=\"15814739\" paperTitle=\"(Fister et al., 2016)\" isShortName></Paper>.\n\n## Effectiveness of Current Measures\n\nDespite concerns about citation gaming, some research suggests certain forms of manipulation may have limited impact on summary citation metrics. Studies into citation patterns indicate that issues such as self-citations, \"rubbish citations\" from low-quality non-academic sources, or citations for errors have \"little impact on summary citation metrics such as the h-index or the g-index\" <Paper corpusId=\"55298872\" paperTitle=\"(Soutar et al., 2015)\" isShortName></Paper> <Paper corpusId=\"14600895\" paperTitle=\"(Harzing et al., 2008)\" isShortName></Paper>.\n\nHowever, this research predates the more sophisticated gaming strategies that have emerged in recent years. The increasing complexity of citation manipulation tactics, including paper mills and citation cartels, suggests that older detection methods may no longer be sufficient. As gaming techniques evolve, so too must the countermeasures designed to identify and discourage them.\n\nThere's growing recognition that addressing citation gaming requires a multi-faceted approach that combines technical solutions with cultural change within academia. The fundamental issue remains that \"when a measure becomes a target, it ceases to be a good measure\" <Paper corpusId=\"52336477\" paperTitle=\"(Fire et al., 2018)\" isShortName></Paper>, suggesting that as long as citation metrics continue to drive career advancement and funding, gaming behaviors will persist.",
          "citations": [
            {
              "id": "(Bartneck et al., 2010)",
              "snippets": [
                "The h-index has received an enormous attention for being an indicator that measures the quality of researchers and organizations. We investigate to what degree authors can inflate their h-index through strategic self-citations with the help of a simulation. We extended Burrell\u2019s publication model with a procedure for placing self-citations, following three different strategies: random self-citation, recent self-citations and h-manipulating self-citations. The results show that authors can considerably inflate their h-index through self-citations. We propose the q-index as an indicator for how strategically an author has placed self-citations, and which serves as a tool to detect possible manipulation of the h-index. The results also show that the best strategy for an high h-index is publishing papers that are highly cited by others. The productivity has also a positive effect on the h-index."
              ],
              "metadata": null
            },
            {
              "id": "(Ioannidis et al., 2023)",
              "snippets": [
                "However, misuse and gaming of metrics are rampant (Ioannidis et al., 2020)(Fire et al., 2018). The urge to \"publish or perish\" (or even \"get cited or perish\") creates an environment where gaming of metrics is amply incentivized. A whole generation of old and new tricks try to make CVs and their impact look good and impactful-better and more impactful than they really are. Many of these gaming tricks can reach extravagant levels, as in the case of paper mills, massive self-citations, or citation cartels (Christopher, 2021)(Noorden et al., 2019)(Fister et al., 2016)",
                ".Self-citation practices may take also complex forms. Occasionally, the authors may collude to cite each other's works, even though they are not co-authors. Such citation cartels (citation farms) usually involve a small number of authors. Flow of citations may not necessarily be equally towards all members of the cartel. For instance, one or a few members may be cited, while the others may enjoy other repayments. The members of the citation farm may be in different institutions and countries. Again, quantitative metrics is the best way to diagnose a cartel. Usually, a large number of scientists cite one author's work and citations from each citing author account for a very small portion of the total citations. Conversely, in a citation farm, a handful of citing authors may account for > 50% or even > 80% of the citations received.\n\nSome of the inappropriate self-citing or citation farming behavior may even aim to inflate selectively some specific citation metric considered most important. For example, the Hirsch h-index has enjoyed inordinate popularity since its introduction in 2005 [35]. H-index can be more easily gamed than the number of total citations",
                ".many of the gaming practices used for single scientists have equivalents for gaming at the journal level, e.g., coercive journal self-citation (requests by the editor to cite other papers published by the journal) and citation cartels involving journals rather than single authors (\"citation stocking\") (Noorden, 2013)."
              ],
              "metadata": null
            },
            {
              "id": "(Jolly et al., 2020)",
              "snippets": [
                "citation indices which are more foolproof to collusion using inappropriate selfcitations. Bartneck and Kokkelmans (Bartneck et al., 2010) discussed how h-index can be inflated by authors by manipulating self-citation. W. Flatt et al. (Flatt et al., 2017), in their study on analyzing the effect of excessive self-citation, argued for a new metric based on more transparency to help curb excessive self-citations, which may have a negative effect on our ability to truly analyze the impact and contributions of a scientific research in its research domain. However, limited work has been done in the field of analyzing citation stack.\n\nIn 2011, Thomson Reuters [5] (currently known as Clarivate Analytics) suspended 33 journals from its Journal Citation Report due to a very high rate of self-citations, which contributed to as much as 90% of the JIF of those journals. Therefore, anomalous citation detection is an important task in bibliometrics."
              ],
              "metadata": null
            },
            {
              "id": "(Christopher, 2021)",
              "snippets": [
                "In 2018, the community first observed scientific papers in the biomedical literature that seemed to display systematically fabricated data, pointing to the existence of paper mills: unofficial, potentially illegal organizations selling fake scientific manuscripts. In the present article, we share relevant information specifically about the \u2018raw data\u2019 associated with paper mill manuscripts. If a submitted manuscript displays clear indicators of potential paper mill involvement, we found that the raw data at close inspection often raise doubts about their authenticity. In the absence of real data, paper mills may need to fabricate raw data images when responding to requests from journals. Given the necessity to streamline production of fake manuscripts, the alleged raw data might be created using templates. Some potential methods for generating fake Western blot images are discussed. Paying close attention to image data, including graphs, diagrams, plots and tables, ideally at pre\u2010publication stage, can clearly help prevent publication of incorrect and fabricated information."
              ],
              "metadata": null
            },
            {
              "id": "(Kacem et al., 2019)",
              "snippets": [
                "Without question, this creates a real career motivation to strategically use self-citation (Seeber et al. 2019), but this does not in any way diminish the value of self-cites that result from productive, sustained, leadingedge efforts (Cooke and Donaldson 2014). When used appropriately, self-cites are equally important as cites from the surrounding community, and without tracking them it is impossible to see how scholars build on their own work.\n\nDespite this, many favor a curated form of the h-index as a response to the gaming problem. Curation involves hacking away at the citation data to neatly remove all occurrences of self-citation. While such treatment effectively silences direct attempts to boost citation scores, it does not prevent indirect manipulation and also produces undesired side effects. For example, curation ignores when authors use self-citation to attract cites from others, which is alarming given that each self-citation appears to increase the number of citations from others by about one after a year, and by about three after 5 years (Fowler and Aksnes 2007)."
              ],
              "metadata": null
            },
            {
              "id": "(Flatt et al., 2017)",
              "snippets": [
                "Who among the many researchers is most likely to usher in a new era of scientific breakthroughs? This question is of critical importance to universities, funding agencies, as well as scientists who must compete under great pressure for limited amounts of research money. Citations are the current primary means of evaluating one\u2019s scientific productivity and impact, and while often helpful, there is growing concern over the use of excessive self-citations to help build sustainable careers in science. Incorporating superfluous self-citations in one\u2019s writings requires little effort, receives virtually no penalty, and can boost, albeit artificially, scholarly impact and visibility, which are both necessary for moving up the academic ladder. Such behavior is likely to increase, given the recent explosive rise in popularity of web-based citation analysis tools (Web of Science, Google Scholar, Scopus, and Altmetric) that rank research performance. Here, we argue for new metrics centered on transparency to help curb this form of self-promotion that, if left unchecked, can have a negative impact on the scientific workforce, the way that we publish new knowledge, and ultimately the course of scientific advance."
              ],
              "metadata": null
            },
            {
              "id": "(Cabanac et al., 2021)",
              "snippets": [
                "This work reveals metric gaming up to the point of absurdity: fraudsters publish nonsensical algorithmically generated papers featuring genuine references. It stresses the need to screen papers for nonsense before peer\u2010review and chase citation manipulation in published papers. Overall, this is yet another illustration of the harmful effects of the pressure to publish or perish."
              ],
              "metadata": null
            },
            {
              "id": "(Fister et al., 2016)",
              "snippets": [
                "In this perspective, our goal is to present and elucidate a thus far largely overlooked problem that is arising in scientific publishing, namely the identification and discovery of citation cartels in citation networks. Taking from the well-known definition of a community in the realm of network science, namely that people within a community share significantly more links with each other as they do outside of this community, we propose that citation cartels are defined as groups of authors that cite each other disproportionately more than they do other groups of authors that work on the same subject. Evidently, the identification of citation cartels is somewhat different, although similar to the identification of communities in networks. We systematically expose the problem, provide theoretical examples, and outline an algorithmic guide on how to approach the subject."
              ],
              "metadata": null
            },
            {
              "id": "(Soutar et al., 2015)",
              "snippets": [
                "Citation metrics have been criticised as measures of research quality because they can be inflated for the wrong reasons, such as when they are cited for their errors, because there are \"rubbish citations\" from low quality non-academic sources, or they are self-citations (Bornmann et al., 2008)(Strata, 1995). However, research into citation patterns indicates that these issues have little impact on summary citation metrics such as the h-index or the g-index (Harzing et al., 2008). A further issue is that of journals gaming the citation metrics by editors asking researchers to include more citations to papers published in their journal before a paper is published. This strategy can enhance the citation metrics of some journals and partially explain their rapid rise in perceived status (Seglen, 1997)."
              ],
              "metadata": null
            },
            {
              "id": "(Harzing et al., 2008)",
              "snippets": [
                "Traditionally, the most commonly used source of bibliometric data is Thomson ISI Web of Knowledge, in particular the Web of Science and the Journal Citation Reports (JCR), which provide the yearly Journal Impact Factors (JIF). This paper presents an alternative source of data (Google Scholar, GS) as well as 3 alternatives to the JIF to assess journal impact (h-index, g-index and the number of citations per paper). Because of its broader range of data sources, the use of GS gen- erally results in more comprehensive citation coverage in the area of management and international business. The use of GS particularly benefits academics publishing in sources that are not (well) cov- ered in ISI. Among these are books, conference papers, non-US journals, and in general journals in the field of strategy and international business. The 3 alternative GS-based metrics showed strong correlations with the traditional JIF. As such, they provide academics and universities committed to JIFs with a good alternative for journals that are not ISI-indexed. However, we argue that these metrics provide additional advantages over the JIF and that the free availability of GS allows for a democratization of citation analysis as it provides every academic access to citation data regardless of their institution's financial means."
              ],
              "metadata": null
            },
            {
              "id": "(Fire et al., 2018)",
              "snippets": [
                "Abstract Background The academic publishing world is changing significantly, with ever-growing numbers of publications each year and shifting publishing patterns. However, the metrics used to measure academic success, such as the number of publications, citation number, and impact factor, have not changed for decades. Moreover, recent studies indicate that these metrics have become targets and follow Goodhart\u2019s Law, according to which, \"when a measure becomes a target, it ceases to be a good measure.\" Results In this study, we analyzed >120 million papers to examine how the academic publishing world has evolved over the last century, with a deeper look into the specific field of biology. Our study shows that the validity of citation-based measures is being compromised and their usefulness is lessening. In particular, the number of publications has ceased to be a good metric as a result of longer author lists, shorter papers, and surging publication numbers. Citation-based metrics, such citation number and h-index, are likewise affected by the flood of papers, self-citations, and lengthy reference lists. Measures such as a journal\u2019s impact factor have also ceased to be good metrics due to the soaring numbers of papers that are published in top journals, particularly from the same pool of authors. Moreover, by analyzing properties of >2,600 research fields, we observed that citation-based metrics are not beneficial for comparing researchers in different fields, or even in the same department. Conclusions Academic publishing has changed considerably; now we need to reconsider how we measure success."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What is the known evidence for pervasiveness of gaming of citation metrics like citation count and h-index?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define citation metrics and their significance in academic evaluation",
            "weight": 0.0923076923076923,
            "evidence": [
              "This article provides a broad overview of widely available measures of academic productivity and impact using publication data and highlights uses of these metrics for various purposes. Metrics based on publication data include measures such as number of publications, number of citations, the journal impact factor score, and the h-index, as well as emerging metrics based on document-level metrics. Publication metrics can be used for a variety of purposes for tenure and promotion, grant applications and renewal reports, benchmarking, recruiting efforts, and administrative purposes for departmental or university performance reports. The authors also highlight practical applications of measuring and reporting academic productivity and impact to emphasize and promote individual investigators, grant applications, or department output."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention evidences of manipulation in citation counts and h-indexusing self-citation",
            "weight": 0.0923076923076923,
            "evidence": [
              "The phenomenon of self-citation can present in many different forms, including direct, co-author, collaborative, and coercive induced self-citation. It can also pertain to the citation of single scientists, groups of scientists, journals, and institutions. This article presents some case studies of extreme self-citation practices. It also discusses the implications of different types of self-citation.",
              "However, misuse and gaming of metrics are rampant [2, 3]. The urge to \"publish or perish\" (or even \"get cited or perish\") creates an environment where gaming of metrics is amply incentivized. A whole generation of old and new tricks try to make CVs and their impact look good and impactful--better and more impactful than they really are. Many of these gaming tricks can reach extravagant levels, as in the case of paper mills, massive self-citations, or citation cartels [4,5,6].",
              "It has recently been suggested there may be increasing misrepresentation of research performance by individuals who self-cite inordinately to achieve scores and win rewards. In this paper we consider self-referencing and self-citing, describe the typical shape of self-citation patterns for carefully curated publication sets authored by 3517 Highly Cited Researchers and quantify the variance in the distribution of self-citation rates within and between all 21 Essential Science Indicators' fields."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention some evidences of citation cartels",
            "weight": 0.0923076923076923,
            "evidence": [
              "Citation cartels are groups of researchers who excessively cite each other's work to artificially inflate their citation counts and enhance their reputation. The practice of the citation cartel involves journals agreeing to cite each other's publications to boost their own impact factors. The citation cartel has been criticised for distorting the impact factors of participating journals and undermining the integrity of the scientific process. Citation cartels can take many forms, including reciprocal citing, where researchers agree to cite each other's work in exchange for citations. Citation cartels often involve a small group of researchers who are closely connected and who may be deliberately hiding their activities.",
              "This paper is an attempt to study a well known (probably little studied) phenomenon in academia: citation cartels. This is the tacit or explicit agreement among authors to cite each other more often than they would do in a more \"sincere\" approach to science. It can be intended as collusion and it can distort scientific progress in affecting a scholar's attention. The phenomenon has been around for decades and it does not seem to spare any discipline. By starting from outlining the characteristics of a \"cartel,\" this study then builds an agent-based model in an attempt to define the extent to which colluding behavior affects progress in a given discipline by operating on citation counts."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention some evidences of encouraging citation inflation by pressuring authors to cite articles published in their journal",
            "weight": 0.0923076923076923,
            "evidence": [
              "Among studies that have analyzed INAP, some have focused their attention on specific disciplinary contexts, measuring, for instance, the growth in the average number of authors in a specific discipline or among certain journals (Kapoor et al., 2015; Papatheodorou et al., 2008; Tilak et al., 2015). Whilst these studies have quantified INAP, they have not adequately analyzed the ethical implications of this phenomenon beyond the claim that it represents authorship inflation and could have resulted from increased pressure for publication as well as honorary and guest authorship practices."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention some evidences of manipulation by citation stacking",
            "weight": 0.0923076923076923,
            "evidence": [
              "## Manipulation by Citation StackingCitation stacking happens when authors cite other works in a less direct but still manipulative manner to inflate metrics artificially:1. **Bartneck and Kokkelmans (2011)** showed that strategically \"stacking\" citations to certain works can inflate metrics even when the cited works do not significantly contribute to the citing papers, effectively gaming the system [7].2. **Pan et al. (2018)** performed analyses showing how particular \"stacked\" citation sources are disproportionately targeted, likely aiming to boost specific authors or journals [8]."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should mention some evidences of honorary authorship",
            "weight": 0.0923076923076923,
            "evidence": [
              "Honorary authorship: Adding authors who contributed little or nothing to a paper can inflate individual citation metrics  (5, Chakraborty et al., 2020)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should mention about debate and criticism on the use of citation metrics in research evaluation",
            "weight": 0.04615384615384615,
            "evidence": [
              "#### **Debate and Criticism**1. **Criticism of Metrics**: The use of citation metrics in research evaluation has been criticized for creating an environment where gaming is possible and for not accurately reflecting research quality.2. **Need for Responsible Metrics Use**: Scholars argue for a responsible use of metrics, with careful design to avoid unintended consequences, such as gaming."
            ]
          }
        ]
      }
    }
  },
  {
    "question": "What data preprocessing steps are most important for point cloud datasets before performing surface reconstruction?",
    "response": {
      "sections": [
        {
          "title": "Introduction/Background",
          "text": "\nPoint cloud data preprocessing is a fundamental and essential step in the surface reconstruction pipeline that directly impacts both the accuracy and efficiency of the final 3D model. This preprocessing phase serves as the foundation for successful reconstruction by addressing various data imperfections before the actual surface generation begins <Paper corpusId=\"272224979\" paperTitle=\"(Shi, 2024)\" isShortName></Paper>. As Yiliu et al. emphasize, \"Data preprocessing is an important step of reverse engineering as it determines whether the subsequent reconstruction process can be carried out in a convenient and accurate manner\" <Paper corpusId=\"214240484\" paperTitle=\"(Yiliu et al., 2020)\" isShortName></Paper>.\n\nThe raw point cloud data obtained from 3D scanning processes typically contains numerous imperfections that must be addressed. These point clouds are often unstructured, may not sample the surface uniformly, and are frequently contaminated with noise <Paper corpusId=\"61809270\" paperTitle=\"(Lalos et al., 2019)\" isShortName></Paper>. The process of addressing these artifacts and enhancing low-quality point cloud data is also known as consolidation, which generates a new point set that more accurately represents the underlying shape <Paper corpusId=\"61809270\" paperTitle=\"(Lalos et al., 2019)\" isShortName></Paper> <Paper corpusId=\"7929773\" paperTitle=\"(Alexa et al., 2003)\" isShortName></Paper>.\n\nWithout proper preprocessing, the subsequent reconstruction process can produce inaccurate or inefficient results. As noted by Yiliu et al., \"The existence of great noise directly leads to a large difference between the reconstructed model and the physical object\" <Paper corpusId=\"214240484\" paperTitle=\"(Yiliu et al., 2020)\" isShortName></Paper>. Additionally, excessive point cloud data can reduce processing efficiency and negatively impact surface smoothness <Paper corpusId=\"214240484\" paperTitle=\"(Yiliu et al., 2020)\" isShortName></Paper>.\n\nResearch has demonstrated that effective preprocessing can significantly improve reconstruction outcomes. For instance, Kulawiak et al. showed that their proposed preprocessing algorithm decreased the average distance between reconstructed model vertices and the ground truth by at least 20% in all test cases <Paper corpusId=\"219517028\" paperTitle=\"(Kulawiak et al., 2020)\" isShortName></Paper>. This highlights how targeted preprocessing steps can substantially enhance the quality of the final 3D reconstruction.",
          "citations": [
            {
              "id": "(Shi, 2024)",
              "snippets": [
                "The general technical route of point cloud 3 D reconstruction algorithms usually includes three steps. The first is the preprocessing phase, whose main purpose is to eliminate erroneous data or process the sampling points to reduce the time cost of subsequent calculations."
              ],
              "metadata": null
            },
            {
              "id": "(Yiliu et al., 2020)",
              "snippets": [
                "Data preprocessing is an important step of reverse engineering as it determines whether the subsequent reconstruction process can be carried out in a convenient and accurate manner. In order to better complete the model reconstruction, the raw measurement data must be preprocessed. \n\n1) Removal of noise The existence of great noise directly leads to a large difference between the reconstructed model and the physical object. Thus, it is necessary to identify and remove the noise in the measurement data. In addition, in order to improve the modeling speed of the subsequent reverse model, the redundant data should be removed as much as possible while ensuring the accuracy of the measured surface. Commonly-used noise filtering methods include human-computer interaction method, curve inspection method and string height difference method [2]. \n\nIn this paper, the human-computer interaction method and the curve inspection method are combined to remove the noise of the curved point cloud of the camera. Firstly, the human-computer interaction method is used to remove the obvious useless points in the point cloud image. Then, the curve check method is used to discriminate the point cloud by the fitted data curve, and the noise is identified and removed. \n\n2) Streamlining of data In the reverse operation process, too much point cloud data not only reduces the processing efficiency, but also has a serious impact on the smoothness of the surface. Therefore, it is necessary to streamline the measured massive data. Commonly-used methods for data streaming include: minimum distance method, angle deviation method, uniform network method and adaptive minimum distance method [1]."
              ],
              "metadata": null
            },
            {
              "id": "(Lalos et al., 2019)",
              "snippets": [
                "The output of a 3D scanning process is a sequence of unstructured point clouds, that are do not always sample uniformly the surface and they usually are contaminated with noise. The process of addressing the aforementioned artifacts and enhancing these low-quality point cloud data is also known as consolidation (Alexa et al., 2003). Consolidation techniques (Lipman et al., 2007)- (Wang et al., 0) process the captured data and they generate a new point set which represents more accurately the underlying shape. They are without a doubt, an essential pre-processing step before surface reconstruction, where a well-sampled point cloud is transformed into a surface mesh."
              ],
              "metadata": null
            },
            {
              "id": "(Alexa et al., 2003)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Kulawiak et al., 2020)",
              "snippets": [
                "The study presents a novel method of point cloud data processing for the purpose of 3D shape reconstruction. The performance and quality of the proposed algorithm was tested using real-world data and compared against high quality reference data. In its current state, the proposed algorithm is not meant to be used as a standalone tool. Instead, its purpose is to perform additional preprocessing before applying existing surface reconstruction methods, which allows for the generation of more detailed 3D meshes. The algorithm was shown to positively influence the quality of 3D models reconstructed from the processed datasets. In specific, application of the proposed algorithm allowed to decrease the average distance between the vertices of the reconstructed model and the ground truth model by no less than 20% in all presented cases."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Common Preprocessing Steps Overview",
          "text": "\nPoint cloud preprocessing encompasses a comprehensive set of operations designed to transform raw scan data into a form suitable for effective surface reconstruction. These steps are crucial for managing large data volumes, improving quality, and ensuring computational efficiency during the reconstruction phase. According to Stojanovic et al., the essential preprocessing steps include \"spatial alignment/registration, automated and/or manual removal of noise and clutter artifacts, computation of normal vectors, and automated and/or manual segmentation of homogeneous spatial regions\" <Paper corpusId=\"211169188\" paperTitle=\"(Stojanovic et al., 2019)\" isShortName></Paper>.\n\nLiang et al. identify several common preprocessing methods including \"removing abnormal data, filling in the missing points, data smoothing, filtering and de-noising, merging of multi-views clouds or redundant data and feature extraction\" <Paper corpusId=\"202487702\" paperTitle=\"(Liang et al., 2019)\" isShortName></Paper>. Similarly, Wang et al. outline that \"point cloud processing includes topological reconstruction, point cloud denoising, point cloud smoothing, point cloud sampling, point cloud sorting, point cloud merging, point cloud encryption, and triangular meshing\" <Paper corpusId=\"255944161\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>.\n\nThe need for these preprocessing steps stems from the challenges associated with raw point cloud data. As noted by Rodriguez-Cuenca et al., \"The 3D point clouds data are difficult to process due to the large amount of information corresponding to millions of points. The work with these raw MLS data could involve a significant amount of time and computational effort\" <Paper corpusId=\"2370489\" paperTitle=\"(Rodriguez-Cuenca et al., 2016)\" isShortName></Paper>. This sentiment is echoed by Long et al., who state that \"due to the complex structure of the surface of the measured object, the original point cloud data is huge, and the storage and transmission of these data will consume a large number of computer resources\" <Paper corpusId=\"234257605\" paperTitle=\"(Long et al., 2021)\" isShortName></Paper>.\n\nA structured approach to preprocessing typically follows a sequential workflow. Ismail et al. describe this process as \"converting the binary data into a more interpretable format compatible with standard 3D libraries and applications. Subsequently, downsampling the data is performed to make it efficiently better processed with shorter computational time. Next, data cleaning is implemented to mitigate imperfections in the real data\" <Paper corpusId=\"264944580\" paperTitle=\"(Ismail et al., 2023)\" isShortName></Paper>. Cheng et al. simplify this into two main components: \"data evaluation...to find skipped and repeated area when scanning and decide whether to measure again\" and \"data filtering...to reduce noise, smooth noise and resample data\" <Paper corpusId=\"250673901\" paperTitle=\"(Cheng et al., 2006)\" isShortName></Paper>.\n\nIn more technical implementations, Herrero et al. outline an extensive set of potential preprocessing requirements including \"decompression, conversion, noise removal, merging, reference system , registration, surface normals , classification, and segmentation\" <Paper corpusId=\"252961689\" paperTitle=\"(Herrero et al., 2022)\" isShortName></Paper>. Wang et al. condense these into four primary operations: \"denoising...eliminating random noise in point cloud data; filtering...smooth the data and reduce the errors in the scanning process; matching...to align the point cloud data from multiple scanning perspectives to the same coordinate system; simplification...to reduce the complexity of data by reducing the number of points\" <Paper corpusId=\"268525065\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nResearch demonstrates that proper preprocessing directly impacts reconstruction quality. Wei et al. found that \"the proposed data preprocessing methods improve the easy realization of point cloud data with higher accuracy and faster speed\" <Paper corpusId=\"65280787\" paperTitle=\"(Wei et al., 2017)\" isShortName></Paper>. This underscores that thorough preprocessing is not merely optional but fundamental to achieving accurate and efficient surface reconstruction.",
          "citations": [
            {
              "id": "(Stojanovic et al., 2019)",
              "snippets": [
                "Pre-processing of acquired point clouds is required in order to optimize the point cloud and introduce additional data required for semantic enrichment. Preprocessing of acquired point clouds requires the following steps: \n\n1. Spatial alignment/registration, 2. Automated and/ or manual removal of noise and clutter artifacts, 3. Computation of normal vectors, 4. Automated and/or manual segmentation of homogeneous spatial regions."
              ],
              "metadata": null
            },
            {
              "id": "(Liang et al., 2019)",
              "snippets": [
                "Common point clouds data preprocessing methods include: removing abnormal data, filling in the missing points, data smoothing, filtering and de-noising, merging of multi-views clouds or redundant data and feature extraction. Satisfactory point clouds can be obtained after data simplification, as shown in Figure 5."
              ],
              "metadata": null
            },
            {
              "id": "(Wang et al., 2023)",
              "snippets": [
                "To obtain complete and accurate point cloud data, a large amount of 3D data needs to be processed before later surface reconstruction modeling is carried out (Song et al., 2017). \n\nPoint cloud processing includes topological reconstruction, point cloud denoising, point cloud smoothing, point cloud sampling, point cloud sorting, point cloud merging, point cloud encryption, and triangular meshing (Litany et al., 2017)."
              ],
              "metadata": null
            },
            {
              "id": "(Rodriguez-Cuenca et al., 2016)",
              "snippets": [
                "The 3D point clouds data are difficult to process due to the large amount of information corresponding to millions of points. The work with these raw MLS data could involve a significant amount of time and computational effort. Thus, in this first stage, some methods to facilitate the handling of the point cloud are carried out. Preprocessing is divided into three main steps: a point cloud splitting, a coordinate system change, and a filtering phase."
              ],
              "metadata": null
            },
            {
              "id": "(Long et al., 2021)",
              "snippets": [
                "The main steps of laser ranging method include point cloud data acquisition, point cloud data preprocessing and 3D reconstruction. Figure 1 shows how the 3D laser scanner can scan the measured object to obtain the original point cloud data [3]. However, due to the complex structure of the surface of the measured object, the original point cloud data is huge, and the storage and transmission of these data will consume a large number of computer resources. so the point cloud data need to be compressed and preprocessed",
                "If 3D reconstruction is directly performed on the huge point cloud data, the efficiency will be very low. It is necessary to streamline the point cloud data, remove redundant data and optimize the distribution density of the point cloud. The simplified point cloud data forms a triangular mesh, which is mapped on the surface to complete the model construction [6]."
              ],
              "metadata": null
            },
            {
              "id": "(Ismail et al., 2023)",
              "snippets": [
                "The initial step involves converting the binary data into a more interpretable format compatible with standard 3D libraries and applications. Subsequently, downsampling the data is performed to make it efficiently better processed with shorter computational time. Next, data cleaning is implemented to mitigate imperfections in the real data, enhancing the efficiency of the surface reconstruction method for mesh generation."
              ],
              "metadata": null
            },
            {
              "id": "(Cheng et al., 2006)",
              "snippets": [
                "Data preprocessing This process involves data evaluation and filtering. Data evaluation is to find skipped and repeated area when scanning and decide whether to measure again. Data filtering is to reduce noise, smooth noise and resample data."
              ],
              "metadata": null
            },
            {
              "id": "(Herrero et al., 2022)",
              "snippets": [
                "The initial data for reconstruction testing will be the sample data provided by the developers which -it is assumed-will ensure the algorithm will perform as expected (see Section 3). It is possible that the data will require pre-processing in order to be completely ready for the algorithm, which could include: A. Decompressionwhere the software packages require '.LAS' files, the downloaded '.LAZ' files need to be decompressed first. B. Conversion -The downloaded data will be converted into another data type or file format the software requires. C. Noise removal -Unwanted objects or artifacts might need to be removed from the cloud for a better reconstruction. D. Merging -A given object or building might be segmented in different files which will need to be merged first in order to obtain the full coverage of the object. E. Reference system -While point cloud data in a relative coordinate system could be sufficient for some projects, in others a coordinate reference system must be designated. F. Registration -Somewhat linked to the previous two steps, different point clouds are unified by this process by geometric alignment from the same reference system (Bellekens et al., 2014). G. Surface normals -Where the reconstruction software does not provide this option, a pre-process of calculating those is required for the creation of polygonal meshes (264726683). H. Classification -The process where a class is assigned to every point. I. Segmentation -The practice where the data is split or divided, for instance when a file is too large, and a software cannot load it or process it further."
              ],
              "metadata": null
            },
            {
              "id": "(Wang et al., 2024)",
              "snippets": [
                "The main steps of data preprocessing include denoising, filtering, registration, and simplification. Among them, denoising aims at eliminating random noise in point cloud data; Filtering can smooth the data and reduce the errors in the scanning process; The matching criterion is to align the point cloud data from multiple scanning perspectives to the same coordinate system; Simplification is to reduce the complexity of data by reducing the number of points."
              ],
              "metadata": null
            },
            {
              "id": "(Wei et al., 2017)",
              "snippets": [
                "3D laser scanning point cloud data is the basis of free-form surface modeling technique. This paper focuses on the preprocessing of the original point cloud data. Techniques including denoising step using wavelet transform, data reduction and mending using Image ware are discussed in details. Surface reconstruction is finally achieved to demonstrate that the proposed data preprocessing methods improve the easy realization of point cloud data with higher accuracy and faster speed, and thus take a closer step towards the real applications in 3D laser scanning field."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Noise and Outlier Removal",
          "text": "\nNoise and outlier removal represents one of the most fundamental preprocessing steps for point cloud data, as the presence of these artifacts can severely compromise surface reconstruction quality. As Li et al. emphasize, \"Point cloud data often suffer from noise interference due to limitations in 3D scanning devices or imperfect image reconstruction techniques\" <Paper corpusId=\"265360270\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. Similarly, Li et al. note that \"Point cloud data collected from the real world are often noisy due to sensors and environments. Therefore, point cloud filtering is often an important preprocessing step before further processing of point clouds\" <Paper corpusId=\"271301620\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\nThe impact of noise on reconstruction outcomes is significant. Yiliu et al. state that \"The existence of great noise directly leads to a large difference between the reconstructed model and the physical object\" <Paper corpusId=\"214240484\" paperTitle=\"(Yiliu et al., 2020)\" isShortName></Paper>. Alrasheedi et al. further explain that \"Point clouds may exhibit various characteristics such as noise, outliers, irregularities, or missing data, which can affect the reconstruction process\" <Paper corpusId=\"267221118\" paperTitle=\"(Alrasheedi et al., 2024)\" isShortName></Paper>. This underscores why, according to Leal et al., \"Denoising the point cloud is fundamental for reconstructing high quality surfaces with details\" <Paper corpusId=\"219562030\" paperTitle=\"(Leal et al., 2020)\" isShortName></Paper>.\n\nSeveral effective techniques have been developed to address noise and outliers in point cloud data. Statistical filtering is one of the most common approaches. Huang et al. describe a method where \"for any p \u2208 P, we first compute the averaged distance dp between p and its k nearest neighbors in P; we then compute the mean m d and standard deviation \u03c3 d of such distances for all {p \u2208 P}; a point p is regarded as an outlier when its corresponding dp > 5 \u2022 \u03c3 d\" <Paper corpusId=\"248525170\" paperTitle=\"(Huang et al., 2022)\" isShortName></Paper>. Similarly, Zhang et al. employ an algorithm that \"calculate the average distance D from each point in the point cloud to its 10 nearest neighboring points. If the average distance D of a point exceeds the set threshold D max, the point is considered noise and removed\" <Paper corpusId=\"272369990\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nA comprehensive approach often involves multiple filtering steps. Shi et al. outline a three-step preprocessing pipeline: \"invalid point removal, statistical filtering, and voxel filtering\" <Paper corpusId=\"227173281\" paperTitle=\"(Shi et al., 2020)\" isShortName></Paper>. Sun et al. also advocate for \"a combined approach of statistical filtering and voxel filtering to effectively eliminate outliers and enhance computational efficiency\" <Paper corpusId=\"275774461\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>. Some approaches, like those described by Weyrich et al., include additional tools \"to remove outliers, to fill holes and to smooth the point cloud data\" <Paper corpusId=\"55697657\" paperTitle=\"(Ozendi et al., 2018)\" isShortName></Paper> <Paper corpusId=\"263861872\" paperTitle=\"(Weyrich et al., 2004)\" isShortName></Paper>.\n\nBeyond automated methods, human intervention can also play a role in noise removal. Yiliu et al. describe combining \"the human-computer interaction method and the curve inspection method...to remove the noise of the curved point cloud\" <Paper corpusId=\"214240484\" paperTitle=\"(Yiliu et al., 2020)\" isShortName></Paper>. Zhang et al. also highlight the importance of \"manual removal of non-cave points\" that automated algorithms cannot identify <Paper corpusId=\"272369990\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nA critical consideration in noise and outlier removal is preserving essential geometric features. Leal et al. note that \"The challenges for a denoising algorithm are noise reduction and sharp features preservation\" <Paper corpusId=\"219562030\" paperTitle=\"(Leal et al., 2020)\" isShortName></Paper>. To address this challenge, techniques like Jets smoothing can be employed, which \"smoothes out the point cloud without sacrificing its surface curvatures\" <Paper corpusId=\"248525170\" paperTitle=\"(Huang et al., 2022)\" isShortName></Paper>.\n\nThe importance of noise and outlier removal is underscored by Al-Bdairy et al., who developed \"a preprocessing algorithm...to determine and delete the unnecessary data as noised points and save the remaining data points for the surface reconstruction of 3D objects\" <Paper corpusId=\"225781015\" paperTitle=\"(Al-Bdairy et al., 2020)\" isShortName></Paper>. Similarly, Lee et al. emphasize that \"noise filtering should be performed to reduce the amount of points and remove unnecessary points\" because \"the point cloud produced from laser scanner may compose erroneous points that derived from object surface quality and laser dispersion\" <Paper corpusId=\"59125872\" paperTitle=\"(Lee et al., 2013)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Li et al., 2023)",
              "snippets": [
                "The denoising of point clouds is a critical preprocessing step aimed at eliminating noise and outliers from 3D point cloud data. Point cloud data often suffer from noise interference due to limitations in 3D scanning devices or imperfect image reconstruction techniques. The objective of denoising methods is to restore the true structure of the point cloud while preserving its essential features. This process involves analyzing each point in the point cloud to determine if it is noise and adjusting or removing it based on the attributes of its neighboring points. Efficient point cloud denoising not only enhances the accuracy of subsequent tasks, such as 3D reconstruction, classification, and recognition, but also enables the identification of outlier points based on the distribution of distances between each point and its neighbors."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al._1, 2023)",
              "snippets": [
                "Point cloud data collected from the real world are often noisy due to sensors and environments. Therefore, point cloud filtering is often an important preprocessing step before further processing of point clouds."
              ],
              "metadata": null
            },
            {
              "id": "(Yiliu et al., 2020)",
              "snippets": [
                "Data preprocessing is an important step of reverse engineering as it determines whether the subsequent reconstruction process can be carried out in a convenient and accurate manner. In order to better complete the model reconstruction, the raw measurement data must be preprocessed. \n\n1) Removal of noise The existence of great noise directly leads to a large difference between the reconstructed model and the physical object. Thus, it is necessary to identify and remove the noise in the measurement data. In addition, in order to improve the modeling speed of the subsequent reverse model, the redundant data should be removed as much as possible while ensuring the accuracy of the measured surface. Commonly-used noise filtering methods include human-computer interaction method, curve inspection method and string height difference method [2]. \n\nIn this paper, the human-computer interaction method and the curve inspection method are combined to remove the noise of the curved point cloud of the camera. Firstly, the human-computer interaction method is used to remove the obvious useless points in the point cloud image. Then, the curve check method is used to discriminate the point cloud by the fitted data curve, and the noise is identified and removed. \n\n2) Streamlining of data In the reverse operation process, too much point cloud data not only reduces the processing efficiency, but also has a serious impact on the smoothness of the surface. Therefore, it is necessary to streamline the measured massive data. Commonly-used methods for data streaming include: minimum distance method, angle deviation method, uniform network method and adaptive minimum distance method [1]."
              ],
              "metadata": null
            },
            {
              "id": "(Alrasheedi et al., 2024)",
              "snippets": [
                "Point clouds may exhibit various characteristics such as noise, outliers, irregularities, or missing data, which can affect the reconstruction process. These factors can affect the quality and the need for cleaning or smoothing operations. These operations help to improve the quality and reliability of the reconstructed surface by reducing noise, eliminating outliers, addressing irregularities, and filling in missing data. Cleaning and smoothing the point cloud enable the reconstruction algorithm to work with a more accurate and consistent dataset, resulting in a higher-quality reconstructed surface. These pretreatment operations are applied on the given 3D point cloud before executing the proposed approach."
              ],
              "metadata": null
            },
            {
              "id": "(Leal et al., 2020)",
              "snippets": [
                "Denoising the point cloud is fundamental for reconstructing high quality surfaces with details in order to eliminate noise and outliers in the 3D scanning process. The challenges for a denoising algorithm are noise reduction and sharp features preservation."
              ],
              "metadata": null
            },
            {
              "id": "(Huang et al., 2022)",
              "snippets": [
                "For synthetic data, the pipeline is in the order of outlier removal, denoising, and point re-sampling; details are given as follows. For real-scanned data, we use the inbuilt pre-processing of different scanners, and use a final step of FPS to re-sample 200, 000 points for each point cloud. \n\nOutlier removal -Performance of surface reconstruction degrades severely when extreme outliers exist in a point cloud; fortunately, these outliers are easy to be removed. We use a statistical method (Rusu et al., 2008) to remove extreme point outliers. For a point cloud P, it regards any p \u2208 P as an outlier and remove it when p is very far away from its local neighborhood. More precisely, for any p \u2208 P, we first compute the averaged distance dp between p and its k nearest neighbors in P; we then compute the mean m d and standard deviation \u03c3 d of such distances for all {p \u2208 P}; a point p is regarded as an outlier when its corresponding dp > 5 \u2022 \u03c3 d. We set k = 35 in this work for outlier removal. \n\nDe-noising -The inevitable existence of point-wise noise influences surface reconstruction as well. For an input point cloud P, we choose to suppress such noise using Jets smoothing (Cazals et al., 2003), which smoothes out the point cloud without sacrificing its surface curvatures. It works by first fitting a parametric surface patch to a local neighborhood N of k points in P, and then projecting {p \u2208 N } onto the fitted surface patch. We set k = 18 in this work for point-wise de-noising. \n\nPoint re-sampling -Empirical results show that surface reconstruction benefits from more uniform distribution of points, even when reducing the number of points contained in P [134]."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al., 2024)",
              "snippets": [
                "Data preprocessing is a critical prerequisite for high-precision 3D modeling based on point cloud data [29][30][31], as well as for the extraction of karst cave points. We conducted data preprocessing using two point cloud processing software, LiDAR 360 and Cyclone 3DR, which involved the following four steps (Fig. 6).\n\n\u2022 Resampling: the point cloud is resampled with a minimum point spacing of 1 cm. This approach reduces the redundancy in dense areas of the point cloud without compromising the quality in sparse regions, thereby enhancing data processing efficiency; \u2022 Noise removal: points that are noise due to sensors or other interference sources are removed. The basic principle of the algorithm is to calculate the average distance D from each point in the point cloud to its 10 nearest neighboring points. If the average distance D of a point exceeds the set threshold D max, the point is considered noise and removed. The threshold D max is calculated using the formula: D max = M(D) + S \u00d7 M(S), where M(D) represents the median of these average distances, S represents the standard deviation, and M(S) represents the standard deviation multiplier, which is a tool used to measure the variability of statistical data.\n\nIn this algorithm, the number of neighboring points and the standard deviation multiplier can be manually set. We have set these values to 10 and 5, respectively. \u2022 Outlier removal: points not belonging to the main structure of the karst cave and typically caused by people moving inside the cave are removed. This can be achieved using a distance-based segmentation algorithm to identify and remove points that are more than 0.1 m away from the main structure of the cave; \u2022 Manual removal of non-cave points: finally, some non-cave points that cannot be automatically identified and removed by algorithms remain."
              ],
              "metadata": null
            },
            {
              "id": "(Shi et al., 2020)",
              "snippets": [
                "The preprocessing comprises three steps: invalid point removal, statistical filtering, and voxel filtering. First, the invalid data type, \"NAN\", is removed in the raw point cloud and Map I is obtained. Subsequently, some error and noise points generated during scanning or stitching are eliminated via statistical filtering using Equation (1), and Map S is acquired. Finally, voxel filtering with voxel size (r * r * r), a downsampling method, is performed to leverage the data memory. In addition, the sampling point cloud data Map V is preserved."
              ],
              "metadata": null
            },
            {
              "id": "(Sun et al., 2025)",
              "snippets": [
                "The preprocessing of point clouds is the foundation of accurate surface reconstruction, and involves steps such as outlier removal, smoothing, and sampling [27]",
                "We employ a combined approach of statistical filtering and voxel filtering to effectively eliminate outliers and enhance computational efficiency",
                "Statistical filtering is primarily utilized to remove prominent outlier points."
              ],
              "metadata": null
            },
            {
              "id": "(Ozendi et al., 2018)",
              "snippets": [
                "Just after the acquisition step, the point cloud data may be preprocessed to eliminate noise and other errors. The preprocessing step is generally required in case of low density sampling, holes and surface reflectance induced problems. (Weyrich et al., 2004) presented a tool box implemented based on the MLS approach. It can be used to remove outliers, to fill holes and to smooth the point cloud data. Thereafter, the resulting point cloud can be used to generate high quality surface meshes by employing standard surface triangulation methods."
              ],
              "metadata": null
            },
            {
              "id": "(Weyrich et al., 2004)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Al-Bdairy et al., 2020)",
              "snippets": [
                "Pre-processing is essential for processing the row data point clouds which acquired using a 3D laser scanner as a modern technique to digitize and reconstruct the surface of the 3D objects in reverse engineering applications. Due to the accuracy limitation of some 3D scanners and the environmental noise factors such as illumination and reflection, there are some noised data points associated with the row point clouds, so, in the present paper, a preprocessing algorithm has been proposed to determine and delete the unnecessary data as noised points and save the remaining data points for the surface reconstruction of 3D objects from its point clouds which acquired using the 3D laser scanner (Matter and Form)."
              ],
              "metadata": null
            },
            {
              "id": "(Lee et al., 2013)",
              "snippets": [
                "Pre-processing is also known as a preliminary treatment. In preprocessing stage, the operations were directly carried out on the point clouds. This process includes two main actions which are noise filtering and registration point. The result of these procedures was a combined point cloud from all scans (registered) and \"noise-free\" point cloud",
                "After the registration process, noise filtering should be performed to reduce the amount of points and remove unnecessary points. This is because the point cloud produced from laser scanner may compose erroneous points that derived from object surface quality and laser dispersion."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Point Cloud Simplification/Downsampling",
          "text": "\nPoint cloud datasets often contain millions of points that can overwhelm computational resources and slow down the surface reconstruction process. Simplification or downsampling addresses this challenge by strategically reducing data volume while preserving essential geometric information. As Long et al. explain, \"due to the complex structure of the surface of the measured object, the original point cloud data is huge, and the storage and transmission of these data will consume a large number of computer resources\" <Paper corpusId=\"234257605\" paperTitle=\"(Long et al., 2021)\" isShortName></Paper>.\n\nThe necessity for downsampling is highlighted by several researchers. Yiliu et al. note that \"too much point cloud data not only reduces the processing efficiency, but also has a serious impact on the smoothness of the surface\" <Paper corpusId=\"214240484\" paperTitle=\"(Yiliu et al., 2020)\" isShortName></Paper>. Similarly, Ismail et al. describe downsampling as making data \"efficiently better processed with shorter computational time\" <Paper corpusId=\"264944580\" paperTitle=\"(Ismail et al., 2023)\" isShortName></Paper>. Hafiz et al. further emphasize that \"by simplifying the point set first, any subsequent surface reconstruction becomes significantly faster\" <Paper corpusId=\"30588518\" paperTitle=\"(Hafiz et al., 2011)\" isShortName></Paper>.\n\nVoxel grid filtering stands out as one of the most widely used downsampling techniques. This method divides the point cloud into uniform three-dimensional cubes (voxels) and replaces all points within each voxel with a single representative point. Chen et al. describe how \"scans were downsampled using a voxel grid filter\" which had \"the added effect of equalizing resolution throughout the entire point cloud since the resulting points were evenly spaced according to the voxel grid\" <Paper corpusId=\"221572059\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper>. Similarly, Shi et al. include \"voxel filtering with voxel size (r * r * r)\" as a key preprocessing step to \"leverage the data memory\" <Paper corpusId=\"227173281\" paperTitle=\"(Shi et al., 2020)\" isShortName></Paper>.\n\nLi et al. specifically highlight the advantages of voxel downsampling, noting that it \"offers advantages, such as high computational efficiency and uniform distribution of sample points\" <Paper corpusId=\"274293038\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Their implementation uses this technique as part of a comprehensive preprocessing pipeline that includes \"point cloud visualization, downsampling, target plane recognition, target plane reconstruction, and point cloud segmentation\" <Paper corpusId=\"274293038\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. The effectiveness of voxel downsampling is further emphasized by Lyu et al., who reported a \"processing time of 0.01289 s and a simplification rate of 91.89%\" <Paper corpusId=\"274293038\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269059725\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper>.\n\nBeyond basic voxel filtering, more sophisticated approaches aim to preserve critical geometric features during downsampling. Zhu et al. propose a multi-view point simplification method that first identifies key edge points of the object and then applies \"a grid-based resampling method...to both the key points and the remaining points with different grid sizes to acquire highly accurate and uniform points\" <Paper corpusId=\"208224676\" paperTitle=\"(Zhu et al., 2019)\" isShortName></Paper>. This approach ensures that important features are maintained at higher resolution while less significant areas are more aggressively simplified.\n\nKulawiak describes a specialized downsampling algorithm for building reconstruction that divides the point cloud \"along the vertical axis into a set of layers\" and then processes each layer to increase \"spatial regularity of the input data and reducing its complexity\" <Paper corpusId=\"247304223\" paperTitle=\"(Kulawiak, 2022)\" isShortName></Paper>. This approach maintains important structural information while significantly reducing data volume.\n\nSome researchers employ simpler but effective methods, such as the approach described by Hafiz et al. where \"points in point cloud are distributed into uniform cubes or cells\" and then \"each cell includes one point which is equivalent to the average value of the clustered points in that cell\" <Paper corpusId=\"30588518\" paperTitle=\"(Hafiz et al., 2011)\" isShortName></Paper>. They note that cells with only one or two points are \"discarded and labeled as noisy data,\" thereby combining simplification with noise removal.\n\nThe importance of downsampling as a preprocessing step is emphasized by Zhang et al., who state that operations like \"voxel downsampling...improve the robustness of the data, greatly improve the training accuracy of the model, minimize the overfitting phenomenon of the model, and improve the generalization ability of the model training\" <Paper corpusId=\"268819733\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. Similarly, Liu et al. note that \"downsampling can effectively improve the efficiency of the algorithm by reducing the number of point clouds without changing the geometry\" <Paper corpusId=\"249219058\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Long et al., 2021)",
              "snippets": [
                "The main steps of laser ranging method include point cloud data acquisition, point cloud data preprocessing and 3D reconstruction. Figure 1 shows how the 3D laser scanner can scan the measured object to obtain the original point cloud data [3]. However, due to the complex structure of the surface of the measured object, the original point cloud data is huge, and the storage and transmission of these data will consume a large number of computer resources. so the point cloud data need to be compressed and preprocessed",
                "If 3D reconstruction is directly performed on the huge point cloud data, the efficiency will be very low. It is necessary to streamline the point cloud data, remove redundant data and optimize the distribution density of the point cloud. The simplified point cloud data forms a triangular mesh, which is mapped on the surface to complete the model construction [6]."
              ],
              "metadata": null
            },
            {
              "id": "(Yiliu et al., 2020)",
              "snippets": [
                "Data preprocessing is an important step of reverse engineering as it determines whether the subsequent reconstruction process can be carried out in a convenient and accurate manner. In order to better complete the model reconstruction, the raw measurement data must be preprocessed. \n\n1) Removal of noise The existence of great noise directly leads to a large difference between the reconstructed model and the physical object. Thus, it is necessary to identify and remove the noise in the measurement data. In addition, in order to improve the modeling speed of the subsequent reverse model, the redundant data should be removed as much as possible while ensuring the accuracy of the measured surface. Commonly-used noise filtering methods include human-computer interaction method, curve inspection method and string height difference method [2]. \n\nIn this paper, the human-computer interaction method and the curve inspection method are combined to remove the noise of the curved point cloud of the camera. Firstly, the human-computer interaction method is used to remove the obvious useless points in the point cloud image. Then, the curve check method is used to discriminate the point cloud by the fitted data curve, and the noise is identified and removed. \n\n2) Streamlining of data In the reverse operation process, too much point cloud data not only reduces the processing efficiency, but also has a serious impact on the smoothness of the surface. Therefore, it is necessary to streamline the measured massive data. Commonly-used methods for data streaming include: minimum distance method, angle deviation method, uniform network method and adaptive minimum distance method [1]."
              ],
              "metadata": null
            },
            {
              "id": "(Ismail et al., 2023)",
              "snippets": [
                "The initial step involves converting the binary data into a more interpretable format compatible with standard 3D libraries and applications. Subsequently, downsampling the data is performed to make it efficiently better processed with shorter computational time. Next, data cleaning is implemented to mitigate imperfections in the real data, enhancing the efficiency of the surface reconstruction method for mesh generation."
              ],
              "metadata": null
            },
            {
              "id": "(Hafiz et al., 2011)",
              "snippets": [
                "Point cloud simplification: Simplification or data reduction of point cloud data is one of the key preprocessing technologies in reverse engineering. Where a typical laser scanned data set often contains millions of data points and this leads to significant computational challenges in processing the point cloud data for practical applications such as multiple scene registration, inspection of industrial parts.\n\nSimplification is done to reduce the number of the massive data points to facilitate geometric computation while preserving the geometric feature of point cloud (Peng et al., 2009). By simplifying the point set first, any subsequent surface reconstruction becomes significantly faster. Point cloud simplification tends to be computationally more efficient and less memory demanding than mesh simplification since no mesh data structures need to be maintained.\n\nIn this study we use a simple simplification method. First, points in point cloud are distributed into uniform cubes or cells as previously discussed. Next, each cell includes one point which is equivalent to the average value of the clustered points in that cell. The cells that have just one or two points will be discarded and labeled as noisy data. Dealing with the average value of each cube makes the segmentation process very fast rather than segmenting the whole point cloud. So after the segmentation process of the simplified point cloud, the results will be referred back to the original points. This simplification method depends mainly on the l cube of the grid and its resolution. Points in each cell are clustered based on properties of the point's distribution which lead to minimizing the deformation of the resulted simplified point cloud while keeping its geometric features."
              ],
              "metadata": null
            },
            {
              "id": "(Chen et al., 2020)",
              "snippets": [
                "For each building facade, the laser scans from different scan positions were manually registered so that they lay in the same coordinate system. The scans were also rotated so that the building point cloud was aligned with the X-Y axes. In addition, the scans were downsampled using a voxel grid filter. Voxel grid downsampling had the added effect of equalizing resolution throughout the entire point cloud since the resulting points were evenly spaced according to the voxel grid. Figure 2a-c shows the preprocessing steps including registration, downsampling, and rotation carried out on the raw point cloud data using CloudCompare [58]. Finally, a building facade was extracted from the scene by extracting a single side of the building and removing any surrounding clutter."
              ],
              "metadata": null
            },
            {
              "id": "(Shi et al., 2020)",
              "snippets": [
                "The preprocessing comprises three steps: invalid point removal, statistical filtering, and voxel filtering. First, the invalid data type, \"NAN\", is removed in the raw point cloud and Map I is obtained. Subsequently, some error and noise points generated during scanning or stitching are eliminated via statistical filtering using Equation (1), and Map S is acquired. Finally, voxel filtering with voxel size (r * r * r), a downsampling method, is performed to leverage the data memory. In addition, the sampling point cloud data Map V is preserved."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al., 2024)",
              "snippets": [
                "Point cloud processing in this paper mainly involves point cloud visualization, downsampling, target plane recognition, target plane reconstruction, and point cloud segmentation. The different point cloud processing procedures are as follows: \n\n1. Point cloud visualization. Point cloud data consist of a vast collection of threedimensional coordinate points. To enable computer reading and calculations, the data are converted into the \".ply\" standard format for storage. For point cloud visualization, the Open3D library in Python is utilized. \n\n2. Point cloud downsampling. During the preprocessing of point clouds, it is common to encounter noise and handle the massive amount of 3D point cloud data. Various preprocessing techniques, such as denoising, downsampling, and point cloud structuring, are employed to enhance processing efficiency and improve data quality. Among these techniques, the voxel downsampling algorithm (Lyu et al., 2024) is a commonly used technique and offers advantages, such as high computational efficiency and uniform distribution of sample points. Thus, we employ the voxel downsampling algorithm to simplify the point cloud. \n\n3. Target plane recognition. After preprocessing and downsampling the point cloud, we obtain the reduced point cloud. The plane recognition technique is employed to separate the end face of the column, where the rebars are embedded. This division provides convenience for subsequent calculations. In this work, the Random Sample Consensus algorithm (RANSAC) (Fischler et al., 1981) is utilized to identify the plane within the point cloud."
              ],
              "metadata": null
            },
            {
              "id": "(Lyu et al., 2024)",
              "snippets": [
                "In response to the challenge of handling large-scale 3D point cloud data, downsampling is a common approach, yet it often leads to the problem of feature loss. We present a dynamic downsampling algorithm for 3D point cloud maps based on an improved voxel filtering approach. The algorithm consists of two modules, namely, dynamic downsampling and point cloud edge extraction. The former adapts voxel downsampling according to the features of the point cloud, while the latter preserves edge information within the 3D point cloud map. Comparative experiments with voxel downsampling, grid downsampling, clustering-based downsampling, random downsampling, uniform downsampling, and farthest-point downsampling were conducted. The proposed algorithm exhibited favorable downsampling simplification results, with a processing time of 0.01289 s and a simplification rate of 91.89%. Additionally, it demonstrated faster downsampling speed and showcased improved overall performance. This enhancement not only benefits productivity but also highlights the system\u2019s efficiency and effectiveness."
              ],
              "metadata": null
            },
            {
              "id": "(Zhu et al., 2019)",
              "snippets": [
                "Our method is designed in two phases (see Figure 6). The first phase presents the multi-view point simplification method; the second one is the surface reconstruction with four well-known approaches. The first phase consists of three steps. First, the scanned object was rotated to the orthographic projection by the angle estimated from principal component analysis (PCA). Then, multi-view projection geometry (top-view, front-view and right-side-view) is applied to obtain the edges of the object from each view. These edge points are merged to produce the three dimensional (3D) edges of the object. Thus, the least important points are separated from the point cloud. Next, a grid-based resampling method is applied to both the key points and the remaining points with different grid sizes to acquire highly accurate and uniform points."
              ],
              "metadata": null
            },
            {
              "id": "(Kulawiak, 2022)",
              "snippets": [
                "The proposed algorithm has been created for the specific task of reconstructing simple 3D building models from sparse point clouds. It is based on several ideas first introduced by the 3D Grid conversion method [34], which performs special data preprocessing of the input point cloud, allowing for the generation of more detailed 3D meshes from sparse point clouds when using solutions such as Poisson surface reconstruction (Kazhdan et al., 2006). The algorithm proposed in this paper incorporates the ideas of preprocessing the input data in a way that increases the spatial regularity of the point cloud as well as dividing the data into a set of layers describing different height ranges",
                "The first step of the proposed approach consists of increasing the spatial regularity of the input data and reducing its complexity. For this reason, the entire point cloud is divided along the vertical axis into a set of layers, where each layer represents a certain height range. Then, a set of operations are applied to each layer in the following order: \n\n1. Every point in the layer is assigned to a single sector of a regular two-dimensional array projected onto the horizontal plane. 2. The number of points for each non-empty 2D sector is reduced in the following way: the height of every point lying within the sector borders is compared to the average height of the entire level. The one point that is closest to the average level height on the vertical axis is preserved, while all the other members of this sector are removed from the point cloud. 3. All points are moved along the vertical axis to match the average height of the entire layer. This is performed in order to achieve more regular surfaces in the final steps of the algorithm."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al._1, 2024)",
              "snippets": [
                "Preprocessing includes but is not limited to, overall rotation, translation, scaling, and flipping of the data, but the most important pre-processing steps are multiview point cloud alignment and voxel downsampling.These preprocessing operations are very important in the training of the model, especially for the special data of the point cloud, these operations improve the robustness of the data, greatly improve the training accuracy of the model, minimize the overfitting phenomenon of the model, and improve the generalization ability of the model training."
              ],
              "metadata": null
            },
            {
              "id": "(Liu et al., 2022)",
              "snippets": [
                "Point clouds preprocessing. Both laser point clouds and image-dense matching point clouds often cause data redundancy in the data acquisition process, so the point clouds data need to be pre-processed. For point clouds with large coverage, it is necessary to segment them to obtain the point clouds within the experimental range, and then it is necessary to align the point clouds data with the 3D model that needs texture mapping to the same coordinate system to judge the 3.2 spatial through-view relationships. For dense point clouds data, noise reduction and downsampling are required. Noise reduction can better represent the object geometry by eliminating discrete points, and downsampling can effectively improve the efficiency of the algorithm by reducing the number of point clouds without changing the geometry."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Registration and Alignment",
          "text": "\nPoint cloud registration and alignment are essential preprocessing steps when working with data collected from multiple scanning positions or angles. This process transforms separate scans into a unified coordinate system, creating a complete representation of the object or scene. As Wang et al. explain, registration aims \"to align the point cloud data from multiple scanning perspectives to the same coordinate system\" <Paper corpusId=\"268525065\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe necessity for registration stems from the inherent limitations of individual 3D scans. A single scan can only capture surfaces visible from one perspective, requiring multiple viewpoints to create a comprehensive model. Chen et al. describe this process where \"laser scans from different scan positions were manually registered so that they lay in the same coordinate system\" <Paper corpusId=\"221572059\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper>. In their approach, they also rotated the building point cloud to align with the X-Y axes, demonstrating how registration often includes both alignment and orientation adjustments.\n\nHerrero et al. position registration as a unification process, defining it as a procedure where \"different point clouds are unified by this process by geometric alignment from the same reference system\" <Paper corpusId=\"252961689\" paperTitle=\"(Herrero et al., 2022)\" isShortName></Paper>. This conceptualization highlights how registration brings coherence to what would otherwise be disparate, unconnected datasets.\n\nThe importance of registration is particularly evident in complex reconstruction projects. Zhao et al. explain that \"point cloud registration involves aligning multiple point clouds acquired from different viewpoints or time frames to create a unified and complete representation\" of the structure being modeled <Paper corpusId=\"266296830\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>. Without this step, subsequent surface reconstruction would be impossible or severely compromised.\n\nZhang et al. emphasize that \"multiview point cloud alignment\" is among \"the most important pre-processing steps\" <Paper corpusId=\"268819733\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. They note that proper registration \"greatly improve the training accuracy of the model, minimize the overfitting phenomenon of the model, and improve the generalization ability of the model training,\" highlighting how effective alignment impacts not just geometry but also subsequent computational processing.\n\nA comprehensive registration workflow was described by Aziz et al., who note that \"each set of point cloud has its own coordinate system\" and registration \"aligns all the point clouds to one common coordinate system\" <Paper corpusId=\"55139515\" paperTitle=\"(Aziz et al., 2016)\" isShortName></Paper>. Their approach includes both registration and noise filtering, culminating in \"all scans compiled into a single of large datasets of point cloud,\" which \"enables fast working and efficiently with the unified data format.\"\n\nRegistration is typically positioned as part of a sequential preprocessing pipeline. Wang et al. place it alongside denoising, filtering, and simplification as a core preprocessing step <Paper corpusId=\"268525065\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>, while Herrero et al. situate it within a comprehensive set of potential preprocessing requirements including decompression, conversion, and noise removal <Paper corpusId=\"252961689\" paperTitle=\"(Herrero et al., 2022)\" isShortName></Paper>. This positioning emphasizes that registration is not an isolated operation but part of an integrated approach to preparing point cloud data for reconstruction.",
          "citations": [
            {
              "id": "(Wang et al., 2024)",
              "snippets": [
                "The main steps of data preprocessing include denoising, filtering, registration, and simplification. Among them, denoising aims at eliminating random noise in point cloud data; Filtering can smooth the data and reduce the errors in the scanning process; The matching criterion is to align the point cloud data from multiple scanning perspectives to the same coordinate system; Simplification is to reduce the complexity of data by reducing the number of points."
              ],
              "metadata": null
            },
            {
              "id": "(Chen et al., 2020)",
              "snippets": [
                "For each building facade, the laser scans from different scan positions were manually registered so that they lay in the same coordinate system. The scans were also rotated so that the building point cloud was aligned with the X-Y axes. In addition, the scans were downsampled using a voxel grid filter. Voxel grid downsampling had the added effect of equalizing resolution throughout the entire point cloud since the resulting points were evenly spaced according to the voxel grid. Figure 2a-c shows the preprocessing steps including registration, downsampling, and rotation carried out on the raw point cloud data using CloudCompare [58]. Finally, a building facade was extracted from the scene by extracting a single side of the building and removing any surrounding clutter."
              ],
              "metadata": null
            },
            {
              "id": "(Herrero et al., 2022)",
              "snippets": [
                "The initial data for reconstruction testing will be the sample data provided by the developers which -it is assumed-will ensure the algorithm will perform as expected (see Section 3). It is possible that the data will require pre-processing in order to be completely ready for the algorithm, which could include: A. Decompressionwhere the software packages require '.LAS' files, the downloaded '.LAZ' files need to be decompressed first. B. Conversion -The downloaded data will be converted into another data type or file format the software requires. C. Noise removal -Unwanted objects or artifacts might need to be removed from the cloud for a better reconstruction. D. Merging -A given object or building might be segmented in different files which will need to be merged first in order to obtain the full coverage of the object. E. Reference system -While point cloud data in a relative coordinate system could be sufficient for some projects, in others a coordinate reference system must be designated. F. Registration -Somewhat linked to the previous two steps, different point clouds are unified by this process by geometric alignment from the same reference system (Bellekens et al., 2014). G. Surface normals -Where the reconstruction software does not provide this option, a pre-process of calculating those is required for the creation of polygonal meshes (264726683). H. Classification -The process where a class is assigned to every point. I. Segmentation -The practice where the data is split or divided, for instance when a file is too large, and a software cannot load it or process it further."
              ],
              "metadata": null
            },
            {
              "id": "(Zhao et al., 2023)",
              "snippets": [
                "To achieve accurate reconstruction of the pipeline network model, preprocessing operations are applied to the point cloud data. These preprocessing steps include point cloud filtering, registration, segmentation, and projection. Filtering techniques are employed to remove noise and outliers from the point cloud data, thereby enhancing the quality and reliability of the reconstructed model. Point cloud registration involves aligning multiple point clouds acquired from different viewpoints or time frames to create a unified and complete representation of the pipeline's internal structure. Point cloud segmentation and projection entail dividing the point cloud into meaningful regions based on geometric properties or clustering algorithms."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al._1, 2024)",
              "snippets": [
                "Preprocessing includes but is not limited to, overall rotation, translation, scaling, and flipping of the data, but the most important pre-processing steps are multiview point cloud alignment and voxel downsampling.These preprocessing operations are very important in the training of the model, especially for the special data of the point cloud, these operations improve the robustness of the data, greatly improve the training accuracy of the model, minimize the overfitting phenomenon of the model, and improve the generalization ability of the model training."
              ],
              "metadata": null
            },
            {
              "id": "(Aziz et al., 2016)",
              "snippets": [
                "Pre-processing is also known as a preliminary treatment. In preprocessing stage, the operations were directly carried out on the point clouds. This process includes two main actions which are noise filtering and registration point. The results of these procedures was a combined point cloud from all scans (registered) and \"noise-free\" point cloud",
                "Each set of point cloud has its own coordinate system. In this stage, registration process was performed to align all the point clouds to one common coordinate system. This action registers and merges several scans into one complete model",
                "After the registration process, noise filtering should be performed to reduce the amount of points and remove any unnecessary points that out from the coverage of the precast concrete. This is because the point cloud produced from laser scanner may compose erroneous points that derived from the object surface quality and laser dispersion",
                "The point cloud preliminary treatment was completed with the process of registration and noise filtering. Lastly, all scans were compiled into a single of large datasets of point cloud. This process enables fastworking and efficiently with the unified data format"
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Normal Vector Computation",
          "text": "\nNormal vector computation represents a fundamental preprocessing step for point cloud data, providing essential geometric information that guides the surface reconstruction process. As Stojanovic et al. highlight, the \"computation of normal vectors\" is one of the required preprocessing steps that follows spatial alignment and noise removal in the point cloud preparation pipeline <Paper corpusId=\"211169188\" paperTitle=\"(Stojanovic et al., 2019)\" isShortName></Paper>. This sequential positioning underscores how normal estimation builds upon earlier preprocessing steps to further enhance the point cloud for reconstruction.\n\nNormal vectors are particularly crucial because they provide orientation information about the underlying surface at each point. Herrero et al. explicitly note that \"surface normals\" calculation is a required pre-process \"for the creation of polygonal meshes\" when the reconstruction software doesn't automatically provide this functionality <Paper corpusId=\"252961689\" paperTitle=\"(Herrero et al., 2022)\" isShortName></Paper>. This emphasizes how normal vectors serve as a bridge between the raw point cloud and the structured mesh representation that follows.\n\nThe orientation aspect of normal computation is particularly significant. Yang et al. identify \"orientation\" as one of the key preprocessing elements for surface reconstruction, placing it alongside other critical steps like denoising and outlier removal <Paper corpusId=\"55879452\" paperTitle=\"(Yang et al., 2015)\" isShortName></Paper>. This orientation information helps reconstruction algorithms determine the inside/outside direction of the surface, which is essential for generating watertight models.\n\nNormal vector computation can also serve as a foundation for extracting higher-level geometric information. Karambakhsh et al. describe leveraging \"normal and curvature data by converting them to a series of one-dimensional histograms at the preprocessing step\" where \"surface normal, which contains nx, ny, and nz parameters, provide three histograms that represent the distribution of different point direction in our target point cloud\" <Paper corpusId=\"216507425\" paperTitle=\"(Karambakhsh et al., 2020)\" isShortName></Paper>. This approach demonstrates how normal vectors can be further processed to enhance the feature representation of the point cloud.\n\nThe necessity of normal vector computation is further emphasized by Bouchiba et al., who directly state that the point cloud must be preprocessed to \"compute surface normals\" alongside outlier removal and subsampling <Paper corpusId=\"58006668\" paperTitle=\"(Bouchiba et al., 2018)\" isShortName></Paper>. This positioning places normal estimation on equal footing with other core preprocessing operations, underlining its importance in preparing point clouds for effective surface reconstruction.\n\nBeyond just being a preprocessing step, normal estimation can also be viewed as part of the point cloud consolidation process. Yang et al. reference the work of Alexa et al., noting that consolidation includes \"denoising, outlier removal, thinning, orientation, and redistribution of the input points\" <Paper corpusId=\"55879452\" paperTitle=\"(Yang et al., 2015)\" isShortName></Paper> <Paper corpusId=\"7929773\" paperTitle=\"(Alexa et al., 2003)\" isShortName></Paper>. The high citation count of Alexa's work highlights the fundamental importance of these preprocessing steps, including normal orientation, in the broader field of point cloud processing.",
          "citations": [
            {
              "id": "(Stojanovic et al., 2019)",
              "snippets": [
                "Pre-processing of acquired point clouds is required in order to optimize the point cloud and introduce additional data required for semantic enrichment. Preprocessing of acquired point clouds requires the following steps: \n\n1. Spatial alignment/registration, 2. Automated and/ or manual removal of noise and clutter artifacts, 3. Computation of normal vectors, 4. Automated and/or manual segmentation of homogeneous spatial regions."
              ],
              "metadata": null
            },
            {
              "id": "(Herrero et al., 2022)",
              "snippets": [
                "The initial data for reconstruction testing will be the sample data provided by the developers which -it is assumed-will ensure the algorithm will perform as expected (see Section 3). It is possible that the data will require pre-processing in order to be completely ready for the algorithm, which could include: A. Decompressionwhere the software packages require '.LAS' files, the downloaded '.LAZ' files need to be decompressed first. B. Conversion -The downloaded data will be converted into another data type or file format the software requires. C. Noise removal -Unwanted objects or artifacts might need to be removed from the cloud for a better reconstruction. D. Merging -A given object or building might be segmented in different files which will need to be merged first in order to obtain the full coverage of the object. E. Reference system -While point cloud data in a relative coordinate system could be sufficient for some projects, in others a coordinate reference system must be designated. F. Registration -Somewhat linked to the previous two steps, different point clouds are unified by this process by geometric alignment from the same reference system (Bellekens et al., 2014). G. Surface normals -Where the reconstruction software does not provide this option, a pre-process of calculating those is required for the creation of polygonal meshes (264726683). H. Classification -The process where a class is assigned to every point. I. Segmentation -The practice where the data is split or divided, for instance when a file is too large, and a software cannot load it or process it further."
              ],
              "metadata": null
            },
            {
              "id": "(Yang et al., 2015)",
              "snippets": [
                "The preprocessing for surface reconstruction includes denoising, outlier removal, thinning, orientation, and redistribution of the input points (see, e.g., (\u00d6ztireli et al., 2010)(Alexa et al., 2003)(Miao et al., 2009)(Lipman et al., 2007)(Huang et al., 2009)), which has resorted to resampling for point cloud consolidation."
              ],
              "metadata": null
            },
            {
              "id": "(Karambakhsh et al., 2020)",
              "snippets": [
                "The first step of the proposed method is to convert the data from a list of the points to a series of information that can quickly be learned by our light and deep neural network. This information-processing includes: converting a point cloud to volumetric data, and converting normal and curvature information to a series of one-dimensional histograms. Firstly, the volumetric data for our primary target dataset, Shapenetcore-part (Yi et al., 2016), has already been provided (see FIGURE 2). However, in the case that volumetric data is necessary to be acquired, the 3D occupancy grid [34] is an accurate method. This method allows us efficiently estimate occupied space between two 3D points. Also, it can be stored and be operated with efficient and straight forward data structures. Secondly, as the Algorithm 1 is illustrated, we take advantage of normal and curvature data by converting them to a series of one-dimensional histograms at the preprocessing step. In this step, surface normal, which contains nx, ny, and nz parameters, provide three histograms that represent the distribution of different point direction in our target point cloud. Thus, at the end of the preprocessing step, provided data, containing 3D volumes, histograms of the surface normal in three directions, and histogram of surface curvature are ready for the learning process."
              ],
              "metadata": null
            },
            {
              "id": "(Bouchiba et al., 2018)",
              "snippets": [
                "To do so, the point cloud must be preprocessed to remove the outliers, be subsampled, and compute surface normals."
              ],
              "metadata": null
            },
            {
              "id": "(Alexa et al., 2003)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Feature Preservation and Enhancement",
          "text": "\nWhen preparing point clouds for surface reconstruction, preserving and enhancing important geometric features is critical for maintaining model fidelity. Feature preservation techniques focus on identifying and protecting sharp edges, corners, and other distinctive geometric elements that define an object's character while still performing necessary preprocessing operations.\n\nA sophisticated approach to feature preservation was developed by Zhu et al., who designed a two-phase method specifically addressing this challenge. Their technique first employs multi-view point simplification that identifies key features before simplification occurs. The process begins by rotating the scanned object using principal component analysis, then applies \"multi-view projection geometry (top-view, front-view and right-side-view) to obtain the edges of the object from each view\" <Paper corpusId=\"208224676\" paperTitle=\"(Zhu et al., 2019)\" isShortName></Paper>. By merging these edge points to produce three-dimensional edges, their method effectively separates the most important geometric features from less significant points. This division enables a selective approach to simplification, where \"a grid-based resampling method is applied to both the key points and the remaining points with different grid sizes\" <Paper corpusId=\"208224676\" paperTitle=\"(Zhu et al., 2019)\" isShortName></Paper>. This adaptive approach ensures that critical features maintain higher resolution while allowing more aggressive simplification of less important regions.\n\nThe importance of feature preservation has been recognized in other research as well. Marzoog et al. describe a technique by Wang et al. that follows a three-step process including \"preprocessing to improve the quality of point cloud dissemination and upgrade sharp features\" <Paper corpusId=\"225429994\" paperTitle=\"(Marzoog et al., 2020)\" isShortName></Paper>. This method demonstrated \"efficiency in producing state-of-art results in terms of preserving sharp features and handling missing data,\" highlighting how targeted preprocessing can specifically enhance important geometric elements while addressing data deficiencies <Paper corpusId=\"225429994\" paperTitle=\"(Marzoog et al., 2020)\" isShortName></Paper>.\n\nBeyond preserving existing features, some preprocessing techniques focus on enhancing surface quality through interpolation. Dong et al. describe using \"Radial Basis Function (RBF) interpolation to create a smooth surface of the original point cloud, effectively mitigating irregularities and filling gaps\" <Paper corpusId=\"276683130\" paperTitle=\"(Dong et al., 2025)\" isShortName></Paper> <Paper corpusId=\"8290519\" paperTitle=\"(Franke, 1982)\" isShortName></Paper>. This approach demonstrates how preprocessing can both preserve and enhance features by creating a smooth, coherent surface while maintaining the underlying geometric structure. The comprehensive preprocessing approach they describe also includes \"resampling to a suitable grid and rotational adjustments for optimal alignment within the coordinate system\" which \"improve the uniformity of the point cloud and transform it from a disordered state into a regularly distributed structure\" <Paper corpusId=\"276683130\" paperTitle=\"(Dong et al., 2025)\" isShortName></Paper>.\n\nThe concept of point cloud consolidation encompasses many of these feature preservation and enhancement techniques. Yang et al. note that preprocessing for surface reconstruction includes \"denoising, outlier removal, thinning, orientation, and redistribution of the input points\" <Paper corpusId=\"55879452\" paperTitle=\"(Yang et al., 2015)\" isShortName></Paper> <Paper corpusId=\"7929773\" paperTitle=\"(Alexa et al., 2003)\" isShortName></Paper>. This comprehensive approach to preprocessing ensures that all aspects of the point cloud\u2014from noise levels to point distribution\u2014are optimized while preserving the essential character of the original object.",
          "citations": [
            {
              "id": "(Zhu et al., 2019)",
              "snippets": [
                "Our method is designed in two phases (see Figure 6). The first phase presents the multi-view point simplification method; the second one is the surface reconstruction with four well-known approaches. The first phase consists of three steps. First, the scanned object was rotated to the orthographic projection by the angle estimated from principal component analysis (PCA). Then, multi-view projection geometry (top-view, front-view and right-side-view) is applied to obtain the edges of the object from each view. These edge points are merged to produce the three dimensional (3D) edges of the object. Thus, the least important points are separated from the point cloud. Next, a grid-based resampling method is applied to both the key points and the remaining points with different grid sizes to acquire highly accurate and uniform points."
              ],
              "metadata": null
            },
            {
              "id": "(Marzoog et al., 2020)",
              "snippets": [
                "Wang et.al. (2019) proposed a technique for building a mesh surface from a point cloud by following three steps. First, preprocessing to improve the quality of point cloud dissemination and upgrade sharp features. Second, dividing the point cloud into segments. Finally, build a curve skeleton for each part and guide the surface reconstruction with insignificant user interaction. The method approved its efficiency in producing state-of-art results in terms of preserving sharp features and handling missing data [7]."
              ],
              "metadata": null
            },
            {
              "id": "(Dong et al., 2025)",
              "snippets": [
                "1) 3D scanning: The first step involves conducting a highprecision 3D scan of the e-skin (Shining 3D EINSTAR). This process captures point cloud data that accurately represents the surface of the e-skin (Dong et al., 2024). Its resolution is 0.1 mm, ensuring detailed and reliable surface representation. It is noteworthy that the point cloud data is obtained before touch interaction occurs and does not include information on the touch areas, which only provides the geometric information of the deformed sensor surface. 2) Point cloud pre-processing: Radial Basis Function (RBF) interpolation (Franke, 1982) is employed to create a smooth surface of the original point cloud, effectively mitigating irregularities and filling gaps. Then, the coordinates are reorganized and outliers are removed to enhance data consistency. Additional processing includes resampling to a suitable grid and rotational adjustments for optimal alignment within the coordinate system. The preprocessing can improve the uniformity of the point cloud and transform it from a disordered state into a regularly distributed structure."
              ],
              "metadata": null
            },
            {
              "id": "(Franke, 1982)",
              "snippets": [
                "Absract. This paper is concerned with the evaluation of methods for scattered data interpolation and some of the results of the tests when applied to a number of methods. The process involves evaluation of the methods in terms of timing, storage, accuracy, visual pleasantness of the surface, and ease of implementation. To indicate the flavor of the type of results obtained, we give a summary table and representative perspective plots of several surfaces."
              ],
              "metadata": null
            },
            {
              "id": "(Yang et al., 2015)",
              "snippets": [
                "The preprocessing for surface reconstruction includes denoising, outlier removal, thinning, orientation, and redistribution of the input points (see, e.g., (\u00d6ztireli et al., 2010)(Alexa et al., 2003)(Miao et al., 2009)(Lipman et al., 2007)(Huang et al., 2009)), which has resorted to resampling for point cloud consolidation."
              ],
              "metadata": null
            },
            {
              "id": "(Alexa et al., 2003)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Data Structuring and Organization",
          "text": "\nEffective data structuring and organization represents a crucial preprocessing step that transforms raw, unstructured point clouds into optimized formats suitable for efficient surface reconstruction. As Che et al. emphasize, \"Data structuring is a common but critical procedure in point cloud data processing. The data structure can help organize the point clouds to enable a variety of basic processes (e.g., nearest-neighbor queries) to be achieved efficiently\" <Paper corpusId=\"73457772\" paperTitle=\"(Che et al., 2019)\" isShortName></Paper>.\n\nOne widely adopted structuring approach is voxelization, which discretizes the point cloud into a uniformly spaced 3D grid. Xu et al. describe this process as creating \"a voxel grid by means of kdtree with cubic cells of certain size\" which is \"applied to the point cloud of the whole construction site area\" to \"distribute the density of points uniformly to avoid the overly dense and sparsely sensed points in specific areas\" <Paper corpusId=\"54527123\" paperTitle=\"(Xu et al., 2015)\" isShortName></Paper>. This uniform distribution is particularly valuable for subsequent processing steps that benefit from consistent point density.\n\nBeyond basic voxelization, octree structures have proven effective for organizing point clouds prior to surface reconstruction. Mi et al. detail a preprocessing workflow where \"an octree is built from the point cloud\" using a method adapted from Poisson Surface Reconstruction that \"ensures that the finest level of the result octree contains not only cubes with points in them, but also their 3 \u00d7 3 \u00d7 3 neighbor cubes\" <Paper corpusId=\"215754836\" paperTitle=\"(Mi et al., 2019)\" isShortName></Paper>. This comprehensive octree structure is \"dense enough to completely reconstruct the surface\" and provides an organized framework for the subsequent reconstruction process <Paper corpusId=\"215754836\" paperTitle=\"(Mi et al., 2019)\" isShortName></Paper>.\n\nFor certain applications, converting point cloud data into alternative representations can facilitate more effective processing. Karambakhsh et al. describe a preprocessing approach that transforms point clouds \"from a list of points to a series of information that can quickly be learned by our light and deep neural network\" <Paper corpusId=\"216507425\" paperTitle=\"(Karambakhsh et al., 2020)\" isShortName></Paper>. Their method includes \"converting a point cloud to volumetric data\" using \"3D occupancy grid\" which \"allows us efficiently estimate occupied space between two 3D points\" and \"can be stored and be operated with efficient and straight forward data structures\" <Paper corpusId=\"216507425\" paperTitle=\"(Karambakhsh et al., 2020)\" isShortName></Paper>.\n\nThe organization of point cloud data can also be tailored to specific processing requirements. Caros et al. employ \"heights normalization and tile partitioning\" in their preprocessing pipeline, with tile partitioning implemented to \"handle the abundance of points in raw ALS tiles\" <Paper corpusId=\"270519515\" paperTitle=\"(Caros et al., 2024)\" isShortName></Paper>. Their approach carefully considers tile dimensions, ensuring they are \"large enough to ensure that the objects are adequately represented, but not so large that the number of points becomes unmanageable\" <Paper corpusId=\"270519515\" paperTitle=\"(Caros et al., 2024)\" isShortName></Paper>.\n\nFor handling very large datasets, spatial databases provide an effective organizational framework. Che et al. note that \"for organizing the dataset in a large area, a spatial database is usually generated to improve the efficiency of data query\" <Paper corpusId=\"73457772\" paperTitle=\"(Che et al., 2019)\" isShortName></Paper>. This approach is particularly valuable when working with extensive point cloud collections that exceed the processing capacity of standard tools.\n\nSome preprocessing pipelines incorporate data structuring as part of a more comprehensive workflow. Li et al. outline a process involving \"point cloud visualization, downsampling, target plane recognition, target plane reconstruction, and point cloud segmentation\" <Paper corpusId=\"274293038\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Within this pipeline, they employ voxel downsampling which \"offers advantages, such as high computational efficiency and uniform distribution of sample points\" <Paper corpusId=\"274293038\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269059725\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper>.\n\nAdvanced data structuring approaches can address specific challenges in point cloud processing. Haitman et al. introduce a \"Sampling Equalizer Module\" to \"mitigate the mismatch in the sampling patterns of a pair of point cloud observations\" <Paper corpusId=\"271924340\" paperTitle=\"(Haitman et al., 2024)\" isShortName></Paper>. Their two-stage approach employs \"an off-the-shelf surface reconstruction technique\" followed by sampling \"the reconstructed surface generated from the point cloud observation...into a uniform grid with voxel size of \u03c1\" <Paper corpusId=\"271924340\" paperTitle=\"(Haitman et al., 2024)\" isShortName></Paper>. For their implementation, they chose Neural Kernel Surface Reconstruction (NKSR) due to its \"robust and real-time performance compared to other methods\" <Paper corpusId=\"271924340\" paperTitle=\"(Haitman et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258987633\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\nData structuring can also facilitate more targeted processing approaches. Che et al. highlight how \"voxelization is a common structuring technique used in many pole-like object detection methods by aligning the voxels along z-axis\" which \"inherently embeds the assumption that a pole-like object is placed vertical\" <Paper corpusId=\"73457772\" paperTitle=\"(Che et al., 2019)\" isShortName></Paper>. This example demonstrates how appropriate data organization can simplify subsequent analysis by incorporating relevant domain knowledge into the structure itself.\n\nThe importance of data organization extends to processing workflows that segment point clouds into meaningful components. Mansor et al. describe four general data processes: \"feature extraction...segmentation...object recognition... classification\" <Paper corpusId=\"235409865\" paperTitle=\"(Mansor et al., 2021)\" isShortName></Paper>. They note that \"since the point cloud data contained huge data volume to be processed, we need high computer processing resources\" and emphasize that appropriate data structuring helps address the challenges posed by point clouds being \"a set of discrete data records and do not have semantic information in it\" <Paper corpusId=\"235409865\" paperTitle=\"(Mansor et al., 2021)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Che et al., 2019)",
              "snippets": [
                "Preprocessing may include various processes such as data structuring, ground filtering, and clustering/segmentation. Data structuring is a common but critical procedure in point cloud data processing. The data structure can help organize the point clouds to enable a variety of basic processes (e.g., nearest-neighbor queries) to be achieved efficiently. For organizing the dataset in a large area, a spatial database is usually generated to improve the efficiency of data query. Additionally, some data structures can simplify the analysis as well as the implementation. For example, voxelization is a common structuring technique used in many pole-like object detection methods by aligning the voxels along z-axis. This approach inherently embeds the assumption that a pole-like object is placed vertical",
                ".By separating the point cloud into ground and non-ground points, a ground filtering process can reduce the data size significantly when searching for objects that are not part of the ground. Because on-ground objects are usually connected by the ground surface, once the ground points are removed, the objects on the ground can be simply separated via a clustering approach."
              ],
              "metadata": null
            },
            {
              "id": "(Xu et al., 2015)",
              "snippets": [
                "The preprocessing of the point clouds aims at refining and structuring the primitive point clouds data. In detail, the objectives of the preprocessing of point cloud is twofold. On one hand, since the photogrammetric point clouds may introduce more errors compared with that from TLS, the outliers in the raw dataset should be removed. On the other hand, due to the large amount and the uneven distribution of points, a voxelization process is required to organize point cloud (Moravec, 1996), which will reduce the amount and discretize the point clouds in a uniformly spaced 3D grid data structure. \n\nTo remove the sparse outliers, a statistical analysis on the neighborhood of each point is conducted (Rusu et al., 2011). \n\nFor each point, the mean distances from it to all its neighbors are computed. Assume that the distribution of points is Gaussian with a standard deviation, those points whose mean distances are beyond the interval defined by the global distance and standard deviation can be considered as outliers and filtered. \n\nFor the voxelization of datasets, a voxel grid by means of kdtree with cubic cells of certain size is created. This grid is applied to the point cloud of the whole construction site area. This voxelization can distribute the density of points uniformly to avoid the overly dense and sparsely sensed points in specific areas."
              ],
              "metadata": null
            },
            {
              "id": "(Mi et al., 2019)",
              "snippets": [
                "To prepare training data, we first normalize the coordinates of the input point cloud and surfaces. Then an octree is built from the point cloud. The octree building method is adapted from the open-source code of PSR (Kazhdan et al., 2013). It ensures that the finest level of the result octree contains not only cubes with points in them, but also their 3 \u00d7 3 \u00d7 3 neighbor cubes. Therefore, the octree is dense enough to completely reconstruct the surface. We then use the ground truth surface with normals to label these vertices. For datasets without ground truth surfaces, we generate ground truth surfaces through PSR."
              ],
              "metadata": null
            },
            {
              "id": "(Karambakhsh et al., 2020)",
              "snippets": [
                "The first step of the proposed method is to convert the data from a list of the points to a series of information that can quickly be learned by our light and deep neural network. This information-processing includes: converting a point cloud to volumetric data, and converting normal and curvature information to a series of one-dimensional histograms. Firstly, the volumetric data for our primary target dataset, Shapenetcore-part (Yi et al., 2016), has already been provided (see FIGURE 2). However, in the case that volumetric data is necessary to be acquired, the 3D occupancy grid [34] is an accurate method. This method allows us efficiently estimate occupied space between two 3D points. Also, it can be stored and be operated with efficient and straight forward data structures. Secondly, as the Algorithm 1 is illustrated, we take advantage of normal and curvature data by converting them to a series of one-dimensional histograms at the preprocessing step. In this step, surface normal, which contains nx, ny, and nz parameters, provide three histograms that represent the distribution of different point direction in our target point cloud. Thus, at the end of the preprocessing step, provided data, containing 3D volumes, histograms of the surface normal in three directions, and histogram of surface curvature are ready for the learning process."
              ],
              "metadata": null
            },
            {
              "id": "(Caros et al., 2024)",
              "snippets": [
                "In the pre-processing stage of our point cloud segmentation pipeline, we employ heights normalization and tile partitioning to facilitate the network training.These preprocessing steps are recommended when possible for effective management of the highdensity ALS point cloud data.\n\n1. Heights Normalization.Given the variability of surface altitude across our dataset, heights are normalized by subtracting the ground's elevation (the topographic ground map is provided by ICGC) to all points, obtaining height above ground.This transformation ensures that heights are represented as distances above ground level with the intention of enhancing the model's comprehension of features related to height.\n\n2. Tile Partitioning.In order to handle the abundance of points in raw ALS tiles, we implement a tile partitioning process.The dimensions of the subtiles are selected based on the point cloud density and the size of objects within the dataset.The subtile size must be large enough to ensure that the objects are adequately represented, but not so large that the number of points becomes unmanageable.We have established subtiles of 100 \u00d7 100 m with a 50 m overlap, considering that our target object sizes range from 30 to 120 m and our mean point density is between 13 and 27 ppm 2 .This configuration ensures that subtiles are manageable in terms of the number of points while effectively capturing the necessary detail."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al., 2024)",
              "snippets": [
                "Point cloud processing in this paper mainly involves point cloud visualization, downsampling, target plane recognition, target plane reconstruction, and point cloud segmentation. The different point cloud processing procedures are as follows: \n\n1. Point cloud visualization. Point cloud data consist of a vast collection of threedimensional coordinate points. To enable computer reading and calculations, the data are converted into the \".ply\" standard format for storage. For point cloud visualization, the Open3D library in Python is utilized. \n\n2. Point cloud downsampling. During the preprocessing of point clouds, it is common to encounter noise and handle the massive amount of 3D point cloud data. Various preprocessing techniques, such as denoising, downsampling, and point cloud structuring, are employed to enhance processing efficiency and improve data quality. Among these techniques, the voxel downsampling algorithm (Lyu et al., 2024) is a commonly used technique and offers advantages, such as high computational efficiency and uniform distribution of sample points. Thus, we employ the voxel downsampling algorithm to simplify the point cloud. \n\n3. Target plane recognition. After preprocessing and downsampling the point cloud, we obtain the reduced point cloud. The plane recognition technique is employed to separate the end face of the column, where the rebars are embedded. This division provides convenience for subsequent calculations. In this work, the Random Sample Consensus algorithm (RANSAC) (Fischler et al., 1981) is utilized to identify the plane within the point cloud."
              ],
              "metadata": null
            },
            {
              "id": "(Lyu et al., 2024)",
              "snippets": [
                "In response to the challenge of handling large-scale 3D point cloud data, downsampling is a common approach, yet it often leads to the problem of feature loss. We present a dynamic downsampling algorithm for 3D point cloud maps based on an improved voxel filtering approach. The algorithm consists of two modules, namely, dynamic downsampling and point cloud edge extraction. The former adapts voxel downsampling according to the features of the point cloud, while the latter preserves edge information within the 3D point cloud map. Comparative experiments with voxel downsampling, grid downsampling, clustering-based downsampling, random downsampling, uniform downsampling, and farthest-point downsampling were conducted. The proposed algorithm exhibited favorable downsampling simplification results, with a processing time of 0.01289 s and a simplification rate of 91.89%. Additionally, it demonstrated faster downsampling speed and showcased improved overall performance. This enhancement not only benefits productivity but also highlights the system\u2019s efficiency and effectiveness."
              ],
              "metadata": null
            },
            {
              "id": "(Haitman et al., 2024)",
              "snippets": [
                "Sampling Equalizer Module. To mitigate the mismatch in the sampling patterns of a pair of point cloud observations we implemented a two-stage Sampling Equalizer Module (SEM). Initially, we employ an off-the-shelf surface reconstruction technique for point cloud observations. Numerous options exist in the literature, ranging from classical methods such as Poisson reconstruction (Kazhdan et al., 2006) to DNN-based approaches. In this study, we adopted the Neural Kernel Surface Reconstruction (NKSR) (Huang et al., 2023), a single-shot surface reconstruction technique, chosen for its robust and real-time performance compared to other methods. Next, the reconstructed surface generated from the point cloud observation is sampled into a uniform grid with voxel size of \u03c1, resulting in a new, uniformly sampled point cloud denoted in Fig. 2 by P, Q."
              ],
              "metadata": null
            },
            {
              "id": "(Huang et al., 2023)",
              "snippets": [
                "We present a novel method for reconstructing a 3D implicit surface from a large-scale, sparse, and noisy point cloud. Our approach builds upon the recently introduced Neural Kernel Fields (NKF) [58] representation. It enjoys similar generalization capabilities to NKF, while simultaneously addressing its main limitations: (a) We can scale to large scenes through compactly supported kernel functions, which enable the use of memory-efficient sparse linear solvers. (b) We are robust to noise, through a gradient fitting solve. (c) We minimize training requirements, enabling us to learn from any dataset of dense oriented points, and even mix training data consisting of objects and scenes at different scales. Our method is capable of reconstructing millions of points in a few seconds, and handling very large scenes in an out-of-core fashion. We achieve state-of-the-art results on reconstruction benchmarks consisting of single objects (ShapeNet [5], ABC [33]), indoor scenes (ScanNet [11], Matterport3D [4]), and outdoor scenes (CARLA [16], Waymo [49])."
              ],
              "metadata": null
            },
            {
              "id": "(Mansor et al., 2021)",
              "snippets": [
                "Generally, there are four general data processes involved. The first one is feature extraction. Feature extraction consists of a single point or a group of point that helps detect certain type of point based on low-level attributes. Example of feature extraction based on low-level attributes includes planar surface and edge detection. The second process is segmentation. Segmentation is a process of grouping the point based on low-level attributes into segment or object. Next, is an object recognition. It is used to recognize one or multiple types of object in point cloud data. Lastly, is the classification process. The classification involves assigning a class or identification to each point, segment, or object to represent certain type of object (Che et al., 2019). \n\nSince the point cloud data contained huge data volume to be processed, we need high computer processing resources to process the data. Furthermore, point cloud data is a set of discrete data records and do not have semantic information in it. The presence of noise at scene also affect the results of data taken. These problems are reduced by using object recognition and classification techniques (Che et al., 2019)."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Impact on Surface Reconstruction Quality",
          "text": "\nThe quality of surface reconstruction is directly dependent on the effectiveness of point cloud preprocessing, with research quantifying significant improvements in reconstruction accuracy following proper data preparation. A comprehensive study by Kulawiak et al. demonstrated the substantial impact of preprocessing on final model quality, showing that their proposed preprocessing algorithm \"allowed to decrease the average distance between the vertices of the reconstructed model and the ground truth model by no less than 20% in all presented cases\" <Paper corpusId=\"219517028\" paperTitle=\"(Kulawiak et al., 2020)\" isShortName></Paper>. This measurable improvement highlights how appropriate preprocessing serves not as a standalone process but as a critical foundation that enhances the effectiveness of existing reconstruction methods.\n\nThe relationship between preprocessing and reconstruction quality is particularly evident when dealing with imperfect point cloud data. Alrasheedi et al. explain that \"point clouds may exhibit various characteristics such as noise, outliers, irregularities, or missing data, which can affect the reconstruction process\" and emphasize that \"cleaning and smoothing the point cloud enable the reconstruction algorithm to work with a more accurate and consistent dataset, resulting in a higher-quality reconstructed surface\" <Paper corpusId=\"267221118\" paperTitle=\"(Alrasheedi et al., 2024)\" isShortName></Paper>. This highlights how preprocessing creates the conditions necessary for reconstruction algorithms to achieve their full potential.\n\nFeature preservation during preprocessing has been shown to directly enhance the quality of reconstructed surfaces. Marzoog et al. describe a technique by Wang et al. that incorporates \"preprocessing to improve the quality of point cloud dissemination and upgrade sharp features\" which \"approved its efficiency in producing state-of-art results in terms of preserving sharp features and handling missing data\" <Paper corpusId=\"225429994\" paperTitle=\"(Marzoog et al., 2020)\" isShortName></Paper>. This approach demonstrates how targeted preprocessing steps can address specific reconstruction challenges like maintaining geometric detail and filling data gaps.\n\nThe consolidation of point cloud data through preprocessing represents a transformative step that significantly impacts reconstruction outcomes. Lalos et al. explain that \"consolidation techniques process the captured data and they generate a new point set which represents more accurately the underlying shape\" and are \"without a doubt, an essential pre-processing step before surface reconstruction, where a well-sampled point cloud is transformed into a surface mesh\" <Paper corpusId=\"61809270\" paperTitle=\"(Lalos et al., 2019)\" isShortName></Paper>. This process builds on fundamental work by Alexa et al., who introduced methods to define \"a smooth manifold surface from a set of points close to the original surface\" <Paper corpusId=\"7929773\" paperTitle=\"(Alexa et al., 2003)\" isShortName></Paper>. The high citation count of Alexa's work underscores the foundational importance of proper data preparation for quality surface generation.\n\nFocused preprocessing approaches that target specific quality issues can yield particularly effective reconstruction results. Sun et al. note that \"preprocessing of point clouds is the foundation of accurate surface reconstruction\" and describe employing \"a combined approach of statistical filtering and voxel filtering to effectively eliminate outliers and enhance computational efficiency\" <Paper corpusId=\"275774461\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>. This combination of filtering methods demonstrates how preprocessing steps can be strategically selected and combined to address both quality and efficiency concerns in the reconstruction pipeline.\n\nThe evidence across multiple studies consistently shows that thorough preprocessing is not merely beneficial but essential for achieving high-quality surface reconstruction. By systematically addressing issues like noise, outliers, and uneven sampling, preprocessing creates an optimized foundation that allows reconstruction algorithms to generate more accurate, detailed, and faithful representations of the original objects.",
          "citations": [
            {
              "id": "(Kulawiak et al., 2020)",
              "snippets": [
                "The study presents a novel method of point cloud data processing for the purpose of 3D shape reconstruction. The performance and quality of the proposed algorithm was tested using real-world data and compared against high quality reference data. In its current state, the proposed algorithm is not meant to be used as a standalone tool. Instead, its purpose is to perform additional preprocessing before applying existing surface reconstruction methods, which allows for the generation of more detailed 3D meshes. The algorithm was shown to positively influence the quality of 3D models reconstructed from the processed datasets. In specific, application of the proposed algorithm allowed to decrease the average distance between the vertices of the reconstructed model and the ground truth model by no less than 20% in all presented cases."
              ],
              "metadata": null
            },
            {
              "id": "(Alrasheedi et al., 2024)",
              "snippets": [
                "Point clouds may exhibit various characteristics such as noise, outliers, irregularities, or missing data, which can affect the reconstruction process. These factors can affect the quality and the need for cleaning or smoothing operations. These operations help to improve the quality and reliability of the reconstructed surface by reducing noise, eliminating outliers, addressing irregularities, and filling in missing data. Cleaning and smoothing the point cloud enable the reconstruction algorithm to work with a more accurate and consistent dataset, resulting in a higher-quality reconstructed surface. These pretreatment operations are applied on the given 3D point cloud before executing the proposed approach."
              ],
              "metadata": null
            },
            {
              "id": "(Marzoog et al., 2020)",
              "snippets": [
                "Wang et.al. (2019) proposed a technique for building a mesh surface from a point cloud by following three steps. First, preprocessing to improve the quality of point cloud dissemination and upgrade sharp features. Second, dividing the point cloud into segments. Finally, build a curve skeleton for each part and guide the surface reconstruction with insignificant user interaction. The method approved its efficiency in producing state-of-art results in terms of preserving sharp features and handling missing data [7]."
              ],
              "metadata": null
            },
            {
              "id": "(Lalos et al., 2019)",
              "snippets": [
                "The output of a 3D scanning process is a sequence of unstructured point clouds, that are do not always sample uniformly the surface and they usually are contaminated with noise. The process of addressing the aforementioned artifacts and enhancing these low-quality point cloud data is also known as consolidation (Alexa et al., 2003). Consolidation techniques (Lipman et al., 2007)- (Wang et al., 0) process the captured data and they generate a new point set which represents more accurately the underlying shape. They are without a doubt, an essential pre-processing step before surface reconstruction, where a well-sampled point cloud is transformed into a surface mesh."
              ],
              "metadata": null
            },
            {
              "id": "(Alexa et al., 2003)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Sun et al., 2025)",
              "snippets": [
                "The preprocessing of point clouds is the foundation of accurate surface reconstruction, and involves steps such as outlier removal, smoothing, and sampling [27]",
                "We employ a combined approach of statistical filtering and voxel filtering to effectively eliminate outliers and enhance computational efficiency",
                "Statistical filtering is primarily utilized to remove prominent outlier points."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What data preprocessing steps are most important for point cloud datasets before performing surface reconstruction?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define point cloud datasets.",
            "weight": 0.15,
            "evidence": [
              "With the invention of the iPhone 12 Pro's new Light Detection and Ranging (LiDAR) sensor, and the increased capability of the iPhone's camera array system, the generation of 3D point clouds using this sensor led to several studies on their capability and resulting accuracy for some applications in Architecture, Engineering and Construction (AEC) industries, especially in planning and decision-making purposes towards 3D reconstructed model. Generally, this 3D model is incorporated from different surface materials which have different results when scanning via laser scanning platform"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the importance of data preprocessing for point cloud datasets before performing surface reconstruction",
            "weight": 0.15,
            "evidence": [
              "If these raw data are directly used for surface reconstruction, the reconstructed surface will be not smooth or vulnerable, and the desired surface model will not be obtained. Moreover, the huge amount of data will put higher demands on the computer in terms of storage and display, resulting in inefficient data processing in the later stage. Therefore, under the premise of retaining the details of point cloud data and not affecting the accuracy of model reconstruction, point cloud reduction is required for massive data"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mentioned key data processing steps for point cloud datasets",
            "weight": 0.15,
            "evidence": [
              "Moreover, the main steps of point cloud processing are: 1. registration that combines multiple point clouds into one coordinate system; 2. segmentation that is the process of grouping similar points into homogeneous regions; 3. classification that classifies points into certain categories; and 4. object detection that recognizes objects in point clouds and localizes them [10].",
              "4.1 Outlier removalSome acquisition techniques generate points which are far away from the surface. These points, commonly referred to as \"outliers\", have no relevance for reconstruction.",
              "4.2 SimplificationSome laser scanners generate points with widely variable sampling. Typically, lines of scan are very densely sampled but the gap between two lines of scan is much larger, leading to an overly massive point cloud with large variations of sampling density. This type of input point cloud might generate imperfect output using algorithms which, in general, only handle small variations of sampling density.",
              "#### 2. **Noise Reduction**Noise in point cloud data can be caused by various factors such as sensor errors or environmental conditions. Noise reduction techniques, such as filtering, are essential to smooth out the point cloud and reduce the impact of noise on surface reconstruction. This can be achieved using techniques like least-square fitting of ellipses.",
              "#### 3. **Data Normalization**Normalizing the point cloud data is crucial to ensure that all points are on the same scale. This step helps in preventing features with large ranges from dominating the reconstruction process. Normalization can be done by scaling the point cloud to a common range, such as between 0 and 1.",
              "## 9. Hole FillingIncomplete data often results in holes in the point cloud, which need to be addressed for a seamless reconstruction.**Inpainting Techniques**: Predict and fill missing areas based on surrounding geometry.**Surface Interpolation**: Generates surfaces to bridge gaps between existing points.",
              "#### 4. **Feature Extraction**Feature extraction is a critical step in point cloud preprocessing. It involves identifying and extracting relevant features such as corners, edges, and creases from the point cloud. These features are essential for surface reconstruction algorithms to accurately reconstruct the surface. Techniques like Delaunay filtering can be used for feature extraction.",
              "#### 7. **Data Loading and Preparation**Finally, the preprocessed point cloud data needs to be loaded and prepared for surface reconstruction. This involves loading the data into the desired format and preparing it for the specific surface reconstruction algorithm being used. Tools like `dataloader.py` can be used to load visibility-augmented point clouds from `scan.npz` files."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention point cloud sampling can help in reducing the input size in the preprocessing step",
            "weight": 0.15,
            "evidence": [
              "In this paper, we proposed a point cloud sampling method to improve the scalability of surface reconstruction with neural fields. We introduced a seed point that serves as the origin to encourage the sampling network to sample point clouds from partial regions of the scene. Our split-and-merge approach allowed the construction of sampled point clouds while suppressing the input size to the sampling network."
            ]
          }
        ]
      }
    }
  },
  {
    "question": "In recommendation systems, how are new methods that optimize diversity typically evaluated?",
    "response": {
      "sections": [
        {
          "title": "Background on Diversity in Recommendation Systems",
          "text": "\nTraditionally, recommender systems have been evaluated primarily on accuracy metrics, but this narrow focus has led to problems such as over-fitting, where models become too specialized to detect users' broader interests <Paper corpusId=\"231627840\" paperTitle=\"(Mulder et al., 2021)\" isShortName></Paper>. Research has shown that accuracy alone is insufficient to fully capture how users perceive recommendations <Paper corpusId=\"236772777\" paperTitle=\"(Kim, 2021)\" isShortName></Paper>. Consequently, diversity has emerged as one of the most prominent \"beyond-accuracy\" metrics in recommendation systems <Paper corpusId=\"231627840\" paperTitle=\"(Mulder et al., 2021)\" isShortName></Paper>.\n\nDiversity in recommendations has been proven to improve user interactions with recommender systems <Paper corpusId=\"15783771\" paperTitle=\"(Castagnos et al., 2015)\" isShortName></Paper> and is positively correlated with user satisfaction <Paper corpusId=\"15783771\" paperTitle=\"(Castagnos et al., 2015)\" isShortName></Paper> <Paper corpusId=\"271744774\" paperTitle=\"(Coppolillo et al., 2024)\" isShortName></Paper>. Multiple studies have demonstrated that users perceive and appreciate diversity in recommendation lists <Paper corpusId=\"15783771\" paperTitle=\"(Castagnos et al., 2015)\" isShortName></Paper>. Furthermore, diversity helps improve users' appreciation of recommendation quality <Paper corpusId=\"209862313\" paperTitle=\"(Morales et al., 2020)\" isShortName></Paper> <Paper corpusId=\"58334595\" paperTitle=\"(Silveira et al., 2017)\" isShortName></Paper>.\n\nHowever, there exists a fundamental challenge in recommendation systems: accuracy and diversity do not always complement each other. Developing systems that consider both criteria typically requires addressing an accuracy-diversity trade-off <Paper corpusId=\"230578101\" paperTitle=\"(Isufi et al., 2021)\" isShortName></Paper>. This challenge arises because the most reliably accurate recommendations are often based on user or object similarity, while the most useful individual recommendations tend to be found among diverse niche objects <Paper corpusId=\"8071414\" paperTitle=\"(Zhou et al., 2008)\" isShortName></Paper>.\n\nWhen diversity is not properly incorporated into recommendation systems, there can be negative consequences. Studies have shown that recommendation systems without diversity considerations can lead to homogenization of user behavior without increasing utility <Paper corpusId=\"271744774\" paperTitle=\"(Coppolillo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"39558129\" paperTitle=\"(Chaney et al., 2017)\" isShortName></Paper>. This homogenization can influence users' perceptions of the world and potentially affect important decisions <Paper corpusId=\"39558129\" paperTitle=\"(Chaney et al., 2017)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Mulder et al., 2021)",
              "snippets": [
                "Traditionally, research on recommender systems focused on evaluating their performance in terms of accuracy metrics (Ziegler et al., 2005). Such focus, however, induced a problem which is known as over-fitting, e.g., a model is fitted so strongly to a user that it is unable to detect any other interests (Kunaver et al., 2017). Additionally, there is a need for a more user-centric evaluation of recommender systems. Thus, diversity has become one of the most prominent beyond-accuracy metrics for recommender systems (Ziegler et al., 2005). In this context, diversity is generally defined as the opposite of similarity (Kunaver et al., 2017), and it is often based on topic diversity (e.g., corona versus farmers strike). For example, Ziegler et al. (Ziegler et al., 2005) proposed a topic diversification method based in the intra-list diversity metric."
              ],
              "metadata": null
            },
            {
              "id": "(Kim, 2021)",
              "snippets": [
                "Evaluating recommender systems with accuracy (Adomavicius et al., 2005)(Herlocker et al., 2004) alone is not sufficient to fully describe how users perceive recommendations (Ge et al., 2010)38,(McNee et al., 2006)(Pu et al., 2011). Thus, recent research starts investigating other objectives beyond accuracy that can capture qualitative aspects of recommender systems. The newly proposed objectives include novelty, diversity, transparency, and satisfaction."
              ],
              "metadata": null
            },
            {
              "id": "(Castagnos et al., 2015)",
              "snippets": [
                "Diversity has long been proven to improve the interactions between users and recommender systems (McGinty et al., 2003)",
                "Many works focus on controlling the diversity level brought by recommender systems. Diversity was initially dedicated to content-based algorithms, especially in the case we have attribute values for each item. We distinguish 3 practices: we can compute the diversity between two items (Smyth et al., 2001), the diversity within a set of items (Ziegler et al., 2005), or the relative diversity brought by a single item relatively to a set of items (Smyth et al., 2001) (see Equation 1). These metrics have then been used in content-based filtering to reorder the recommendation list, according to a diversity criterion (Bradley et al., 2001), (Zhang et al., 2008). In addition to these content-based algorithms, some works have focused on a way to integrate diversity in collaborative filtering (Ziegler et al., 2005), [17]",
                "Many works showed that diversity is perceived by users (Zhang et al., 2008), (Lathia et al., 2010), [12], and positively correlated to user satisfaction [5], (Ekstrand et al., 2014)."
              ],
              "metadata": null
            },
            {
              "id": "(Coppolillo et al., 2024)",
              "snippets": [
                "Several online and targeted user studies assessed the increase in user satisfaction when diversity is incorporated into the list of suggested items [10](Hurley et al., 2011). For example, Allison et al. (Chaney et al., 2017) show that, if diversity (besides other objectives) is not taken into account, the interactions between users and recommender systems are prone to homogenization and, consequently, low utility."
              ],
              "metadata": null
            },
            {
              "id": "(Morales et al., 2020)",
              "snippets": [
                "In recommender systems, diversity can help improve users' appreciation of the quality of recommendations (Silveira et al., 2017)(Bobadilla et al., 2013). It also has other applications, such as detecting changes in consumption behavior for context-aware recommenders [111]. As a property of recommendations, diversity has been traditionally captured by a set of related indicators proposed on intuitive bases, called serendipity, discovery, novelty, dissimilarity (see Section 8.3 of [112], or (Silveira et al., 2017)(Bobadilla et al., 2013) for a discussion of terminology and definitions). These indicators are often computed using past collective choices of items made by users (Zhou et al., 2008), or classifications of items into types (Ziegler et al., 2005)."
              ],
              "metadata": null
            },
            {
              "id": "(Silveira et al., 2017)",
              "snippets": [
                "Recommender Systems have become a very useful tool for a large variety of domains. Researchers have been attempting to improve their algorithms in order to issue better predictions to the users. However, one of the current challenges in the area refers to how to properly evaluate the predictions generated by a recommender system. In the extent of offline evaluations, some traditional concepts of evaluation have been explored, such as accuracy, Root Mean Square Error and P@N for top-k recommendations. In recent years, more research have proposed some new concepts such as novelty, diversity and serendipity. These concepts have been addressed with the goal to satisfy the users\u2019 requirements. Numerous definitions and metrics have been proposed in previous work. On the absence of a specific summarization on evaluations of recommendation combining traditional metrics and recent progresses, this paper surveys and organizes the main research that present definitions about concepts and propose metrics or strategies to evaluate recommendations. In addition, this survey also settles the relationship between the concepts, categorizes them according to their objectives and suggests potential future topics on user satisfaction."
              ],
              "metadata": null
            },
            {
              "id": "(Isufi et al., 2021)",
              "snippets": [
                "Despite accuracy is still the most dominant criterion guiding the design and evaluation of recommender systems (RecSys), numerous studies have shown that recommendation diversity -decreasing the similarity of the items in the recommended item listsignificantly improves user satisfaction (Aggarwal et al., 2016;(Bradley et al., 2001)Kaminskas & Bridge, 2016). However, accuracy and diversity do not always go hand in hand and the development of a recommender system to consider both criteria typically requires dealing with an accuracy-diversity trade-off, a.k.a. balance or dilemma Kunaver and Po\u017erl (2017), Wu et al. (2019)."
              ],
              "metadata": null
            },
            {
              "id": "(Zhou et al., 2008)",
              "snippets": [
                "Recommender systems use data on past user preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse niche objects, the most reliably accurate results are obtained by methods that recommend objects based on user or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybrid with an accuracy-focused algorithm. By tuning the hybrid appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and diversity of recommendations."
              ],
              "metadata": null
            },
            {
              "id": "(Chaney et al., 2017)",
              "snippets": [
                "Recommendation systems are ubiquitous and impact many domains; they have the potential to influence product consumption, individuals' perceptions of the world, and life-altering decisions. These systems are often evaluated or trained with data from users already exposed to algorithmic recommendations; this creates a pernicious feedback loop. Using simulations, we demonstrate how using data confounded in this way homogenizes user behavior without increasing utility."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Diversity Metrics and Measurement Approaches",
          "text": "\nDiversity in recommendation systems is evaluated through a variety of metrics that can be broadly categorized into two main approaches: individual diversity and aggregate diversity <Paper corpusId=\"232233008\" paperTitle=\"(Raza et al., 2021)\" isShortName></Paper>. \n\nIndividual diversity (also called personalized diversity or intra-list diversity) measures the dissimilarity between items in a single user's recommendation list <Paper corpusId=\"213771645\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"232233008\" paperTitle=\"(Raza et al., 2021)\" isShortName></Paper>. This metric quantifies how different recommended items are from one another, with the assumption that greater dissimilarity increases the probability of users finding items of interest <Paper corpusId=\"213771645\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. Mathematically, this is often calculated as the average pairwise distance between items in a ranked list <Paper corpusId=\"231839834\" paperTitle=\"(Gharahighehi et al., 2021)\" isShortName></Paper>. For more sophisticated analysis, researchers have developed rank and relevance sensitive intra-list diversity measures (RR-ILD) that assess both diversification and the preservation of relevant items in higher ranks <Paper corpusId=\"231839834\" paperTitle=\"(Gharahighehi et al., 2021)\" isShortName></Paper>.\n\nAggregate diversity (also called system diversity) evaluates diversity from a system-wide perspective by measuring the total number of unique items recommended across all users <Paper corpusId=\"213771645\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"232233008\" paperTitle=\"(Raza et al., 2021)\" isShortName></Paper>. This approach emphasizes the system's ability to recommend a broad range of items from the entire catalog <Paper corpusId=\"231839834\" paperTitle=\"(Gharahighehi et al., 2021)\" isShortName></Paper>. Related to this concept is item coverage, which calculates the percentage of available items that appear at least once in recommendation lists <Paper corpusId=\"218486752\" paperTitle=\"(Mansoury et al., 2020)\" isShortName></Paper>.\n\nInter-user diversity (or personalization) is another important metric that assesses how different the recommendation lists are between users <Paper corpusId=\"4876217\" paperTitle=\"(Lu et al., 2012)\" isShortName></Paper>. The Hamming distance is commonly used for this purpose, measuring the dissimilarity between the recommendation lists of two users <Paper corpusId=\"234939662\" paperTitle=\"(Liu, 2020)\" isShortName></Paper> <Paper corpusId=\"4876217\" paperTitle=\"(Lu et al., 2012)\" isShortName></Paper>. A higher average Hamming distance across all user pairs indicates more personalized recommendations <Paper corpusId=\"4876217\" paperTitle=\"(Lu et al., 2012)\" isShortName></Paper>.\n\nSeveral distribution-based metrics evaluate the fairness and uniformity of item recommendations:\n\n- The Gini index measures how uniformly items appear in recommendation lists, with a lower value indicating fairer distribution <Paper corpusId=\"218486752\" paperTitle=\"(Mansoury et al., 2020)\" isShortName></Paper>.\n- Entropy assesses the uniformity of the distribution of recommended items, with higher entropy indicating greater diversity <Paper corpusId=\"218486752\" paperTitle=\"(Mansoury et al., 2020)\" isShortName></Paper>.\n- Category coverage (CC) measures the coverage of user interests by counting the relevant categories in the recommendation set <Paper corpusId=\"235792479\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper>.\n\nOther related metrics include calibration, which measures how well recommendation lists reflect the proportions of a user's different interests <Paper corpusId=\"231839834\" paperTitle=\"(Gharahighehi et al., 2021)\" isShortName></Paper> <Paper corpusId=\"52902267\" paperTitle=\"(Steck, 2018)\" isShortName></Paper>, and serendipity, which evaluates how unexpected yet satisfactory the recommended items are <Paper corpusId=\"42183268\" paperTitle=\"(Willemsen et al., 2016)\" isShortName></Paper> <Paper corpusId=\"2863027\" paperTitle=\"(Kumar et al., 2017)\" isShortName></Paper>.\n\nIt's important to note that there can be a gap between computational diversity metrics and human perceptions of diversity. Research indicates that while metrics like intra-list similarity can be good proxies for perceived diversity, the implementation details matter and require validation in specific domains <Paper corpusId=\"269588075\" paperTitle=\"(Vrijenhoek et al., 2024)\" isShortName></Paper>. Furthermore, diversity metrics that don't consider user identity and embedding can lead to homophily and bias in recommendations <Paper corpusId=\"258762637\" paperTitle=\"(Boldi et al., 2023)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Raza et al., 2021)",
              "snippets": [
                "The diversity is these methods is usually evaluated in two ways: (i) the individual diversity (Ziegler et al., 2005) or (ii) the aggregate diversity (Niemann et al., 2013). Individual Diversity is quantified as the pairwise dissimilarity between items in a (given) user's recommendation list. This technique is used to assess the diversity from the user's perspective. The aggregate diversity, on the other hand, is measured as the number of unique items recommended across users. This scheme captures the system-centric notion of diversity."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al., 2020)",
              "snippets": [
                "Previous works have generally used two criteria to evaluate recommendation diversity (Gogna et al., 2017)(Gogna et al., 2017): First, personalized diversity (PD) is used to describe the dissimilarity between any two items in the recommendation list. As the diversity of the recommended items increases, the probability of users finding items of interest increases. The PD is defined as \n\nwhere |U| is the number of users and where sim(i, j) is the similarity between item i and item j; L(u) is the recommendation list of the users and |L(u)| is the number of items in the L(u). \n\nSecondly, aggregate diversity (AD) is measured by the total number of unique items recommended for all users. A high AD value ensures that more items can be seen by users of an RS. The AD is defined as \n\nwhere |I| is the number of recommended items for all users."
              ],
              "metadata": null
            },
            {
              "id": "(Gharahighehi et al., 2021)",
              "snippets": [
                "To measure the diversity of a recommendation list, a common evaluation measure is the average pair-wise distance between items in the ranked list (Smyth et al., 2001). This measure is called intra-list diversity (ILD) and a high value in ILD means the recommended list contains items with a broad range of content. In recommender systems accuracy and ranking play important roles. Vargas and Castells (Vargas et al., 2011) introduced a rank and relevance sensitive intra-list diversity measure (RR-ILD) that shows to what extent the recommender can diversify the list and preserve the relevant items in the high ranks. It is worth mentioning that there are some other measures related to diversity such as aggregate diversity, calibration and fairness. Aggregate diversity or item coverage measures the portion of available items in the catalog that are included in the recommendation lists (Adomavicius et al., 2011). A calibrated recommendation list reflects the different areas of interest of a user with their proportions in the user history (Steck, 2018). Fairness measures the degree of parity of recommendations for individuals or groups of users based on a sensitive factor such as gender, ethnicity or age (Dwork et al., 2011)."
              ],
              "metadata": null
            },
            {
              "id": "(Mansoury et al., 2020)",
              "snippets": [
                "For evaluation, we use the following metrics to measure the effectiveness of each method: \n\n(1) Precision (P@n): The fraction of the recommended items shown to the users that are part of the users' profile in the test set. (2) Coverage (C@n): The percentage of items which appear at least once in the recommendation lists. (3) Gini index (G@n): The measure of fair distribution of recommended items. It takes into account how uniformly items appear in recommendation lists. Uniform distribution will have Gini index equal to zero which is the ideal case (lower Gini index is better). Given all the recommendation lists for users, L, and p(i k |L) as the probability of the k-th least recommended item being drawn from L calculated as (Vargas et al., 2014): \n\nwhere L u is the recommendation list for user u. Now, Gini index of L can be computed as: \n\n(4) Entropy (E@n): Given the distribution of recommended items, entropy measures the uniformity of that distribution. Uniform distribution has the highest entropy or information gain, thus higher entropy is more desired when the goal is increasing diversity. \n\nwhere p(i |L) is the observed probability value of item i in recommendation lists L."
              ],
              "metadata": null
            },
            {
              "id": "(Lu et al., 2012)",
              "snippets": [
                "Diversity in recommender systems refers to how different the recommended objects are with respect to each other. There are two levels to interpret diversity: one refers to the ability of an algorithm to return different results to different users-we call it Inter-user diversity (i.e., the diversity between recommendation lists). The other one measures the extent to which an algorithm can provide diverse objects to each individual user-we call it Intra-user diversity (i.e., the diversity within a recommendation list). Inter-user diversity (Zhou et al., 2007) is defined by considering the variety of users' recommendation lists. Given users i and j, the difference between the top L places of their recommendation lists can be measured by the Hamming distance\n\nwhere Q ij (L) is the number of common objects in the top-L places of the lists of users i and j. If the lists are identical, H ij (L) = 0, while if their lists are completely different, H ij (L) = 1. Averaging H ij (L) over all user pairs, we obtain the mean Hamming distance H(L). The greater its value, the more diverse (more personalized) recommendation is given to the users. Denoting the recommended objects for user i as {o 1 , o 2 , \u00b7 \u00b7 \u00b7 , o L }, similarity of these objects s(o \u03b1 , o \u03b2 ) can be used to measure the intra-user diversity (this similarity can be obtained either directly from the input ratings or from object metadata) (Zhou et al., 2008). The average similarity of objects recommended to user i,\n\ncan be further averaged over all users to obtain the mean intra-similarity of the recommendation lists, I(L). The lower is this quantity, the more diverse objects are recommended to the users. Notably, intra-list diversity can be used to enhance improve recommendation lists by avoiding recommendation of excessively similar objects [35]. The rank-sensetive version can be obtained by introducing a discount function of the object's rank in recommendation list [109]."
              ],
              "metadata": null
            },
            {
              "id": "(Liu, 2020)",
              "snippets": [
                "The evaluation index to be used in the project is Hamming distance. Hamming distance is used to measure the difference between the two user recommendation lists, such as represents the number of the same products in the recommended list of users u and t.If the two recommended lists are identical, then H ut (L) = 0; otherwise, if the two recommended lists have no overlapping products, then H ut (L) = 1.The result of the above formula is the recommended dissimilarity between any two users, and then the average Hamming distance between all users is the overall diversity evaluation index of the whole system. The higher the value, the higher the diversity of recommendations."
              ],
              "metadata": null
            },
            {
              "id": "(Liang et al., 2021)",
              "snippets": [
                "Diverisity. We evaluate the recommendation diversity by the intra-list distance (ILD) [26,(Zhang et al., 2008) and category coverage (CC) [26](Wu et al., 2019). ILD measures the diversity of the recommended item set by the mean distance between all pairs of items, while CC measures the coverage of the user interests by counting the relevant categories in the set."
              ],
              "metadata": null
            },
            {
              "id": "(Steck, 2018)",
              "snippets": [
                "When a user has watched, say, 70 romance movies and 30 action movies, then it is reasonable to expect the personalized list of recommended movies to be comprised of about 70% romance and 30% action movies as well. This important property is known as calibration, and recently received renewed attention in the context of fairness in machine learning. In the recommended list of items, calibration ensures that the various (past) areas of interest of a user are reflected with their corresponding proportions. Calibration is especially important in light of the fact that recommender systems optimized toward accuracy (e.g., ranking metrics) in the usual offline-setting can easily lead to recommendations where the lesser interests of a user get crowded out by the user's main interests-which we show empirically as well as in thought-experiments. This can be prevented by calibrated recommendations. To this end, we outline metrics for quantifying the degree of calibration, as well as a simple yet effective re-ranking algorithm for post-processing the output of recommender systems."
              ],
              "metadata": null
            },
            {
              "id": "(Willemsen et al., 2016)",
              "snippets": [
                "Existing research such as discussed above typically evaluates diversity by performing simulations to show that enhancing diversity improves the accuracy or efficiency with which simulated users interact with a recommender system. (Ge et al., 2010) propose a number of metrics that are more suitable for the evaluation of diversity: Coverage, describing the number of items a recommender system can and does recommend; and Serendipity, describing to what extent the recommended items are unexpected and satisfactory. However, to better understand how real users perceive and evaluate diversity, and to be able to answer our question of whether diversification can reduce choice difficulty, we need to go beyond these simulations and study actual users' diversity perceptions, choice difficulty, and satisfaction with diversified item sets."
              ],
              "metadata": null
            },
            {
              "id": "(Kumar et al., 2017)",
              "snippets": [
                "There are a few papers which describe the approaches of diversity used in recommender systems. One approach of solving this problem was by introduction of intra-list similarity metric and topic diversification for recommendation lists (McNee et al., 2006). Intuitively, diversity and accuracy are thought as a trade-off between each other. This means diversity can be achieved at the expense of accuracy and vice-versa. In another approach variance based approach was introduced in order to solve the accuracy-diversity tradeoff",
                "Diversity metric in this paper was taken as total number of distinct items recommended across all users [40].\n\nThere is another view of aggregate diversity metric or average diversity metric in the literature. It can be defined as dissimilarity between recommended items based on distance between feature vectors of user ratings",
                "Serendipity and coverage as performance metrics have also been introduced in literature apart from novelty and diversity in order to capture the performance of RS (Ge et al., 2010), (Shani et al., 2011)."
              ],
              "metadata": null
            },
            {
              "id": "(Vrijenhoek et al., 2024)",
              "snippets": [
                "In recommender systems research, diversity is often viewed as the opposite of similarity; a list of recommended items is diverse if the items are sufficiently different between them along a set of axes [53]. However, Jesse et al. [24] point to a gap between human perceptions on diversity and intra-list similarity (ILS) commonly used to assess diversity in offline recommender systems experiments. Through a user study, they find that while ILS can be a good proxy, the details of the implementation matter and require validation in a given domain and application."
              ],
              "metadata": null
            },
            {
              "id": "(Boldi et al., 2023)",
              "snippets": [
                "Metrics measuring diversity and novelty are very important for evaluating recommendation systems, but can be counterproductive if users are shown items that are not interesting to them. While these metrics often use dissimilarity measures to previous items viewed (novelty) or between items within a recommendation list (diversity), they do not consider user identity and embedding, which can lead to homophily and bias in recommendations [Castells et al., 2011, Adomavicius andKwon, 2012]."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Accuracy-Diversity Trade-off Evaluation",
          "text": "\nThe challenge of evaluating recommendation systems that optimize for diversity stems from the fundamental trade-off between accuracy and diversity. Since these objectives often conflict with each other, researchers have developed various approaches to evaluate and balance this trade-off <Paper corpusId=\"235363444\" paperTitle=\"(Dacrema, 2021)\" isShortName></Paper> <Paper corpusId=\"267938705\" paperTitle=\"(Luo et al., 2024)\" isShortName></Paper>.\n\nA common evaluation approach uses composite metrics that combine both accuracy and diversity measures. This typically takes the form of a weighted sum where the weights control the recommender system's preference for each objective <Paper corpusId=\"248986330\" paperTitle=\"(Yin et al., 2022)\" isShortName></Paper>. For example, one implementation defines a composite metric as \"the weighted sum of the F1-score and the combined diversity scores (mean of diversity and novelty)\" <Paper corpusId=\"232320407\" paperTitle=\"(Raza, 2021)\" isShortName></Paper>.\n\nFor accuracy measurement, researchers utilize traditional metrics such as RMSE (Root Mean Square Error), precision@k, recall@k, and F1-score@k <Paper corpusId=\"232320407\" paperTitle=\"(Raza, 2021)\" isShortName></Paper>. Other accuracy indicators include Mean Absolute Error (MAE) <Paper corpusId=\"267938705\" paperTitle=\"(Luo et al., 2024)\" isShortName></Paper> and ranking metrics like MRR (Mean Reciprocal Rank) <Paper corpusId=\"270440683\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nOn the diversity side, evaluation typically includes measuring intra-list diversity (average pairwise dissimilarity between recommended items), novelty (ratio of unknown items recommended to a user), and metrics that assess the coverage of user interests or system-level diversity <Paper corpusId=\"232320407\" paperTitle=\"(Raza, 2021)\" isShortName></Paper> <Paper corpusId=\"270440683\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe challenge of comparing models with multiple metrics remains an open research question. One approach is to aggregate diverse metrics into a single metric for comparison purposes, though this is complicated by differences in scales and variations across metrics <Paper corpusId=\"259501626\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>. Rank-based evaluations that use the average rank across multiple metrics can avoid scale issues but have limitations for model selection <Paper corpusId=\"259501626\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\nUser studies play a crucial role in validating diversity metrics and determining appropriate trade-off levels. Research has shown that users notice different diversity levels in recommendations and generally prefer some degree of diversification, but not at the extreme <Paper corpusId=\"256616145\" paperTitle=\"(Castells et al., 2023)\" isShortName></Paper>. These studies help ensure that computational metrics actually reflect users' perceptions of diversity and its impact on satisfaction <Paper corpusId=\"256616145\" paperTitle=\"(Castells et al., 2023)\" isShortName></Paper>.\n\nImportantly, user preferences for diversity can vary widely both between users and within the same user depending on context. Research indicates that preference for diversification is often linked to user mood, and users distinguish between \"inner diversity\" (within existing preferences) and \"outer diversity\" (outside existing preferences) <Paper corpusId=\"267938705\" paperTitle=\"(Luo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"236096973\" paperTitle=\"(Robinson et al., 2020)\" isShortName></Paper>. This complexity suggests that fixed diversity strategies may not be optimal, as user preferences for the accuracy-diversity trade-off can be highly contextual and personalized <Paper corpusId=\"267938705\" paperTitle=\"(Luo et al., 2024)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Dacrema, 2021)",
              "snippets": [
                "The evaluation of recommender systems is frequently focused on accuracy metrics, but this is only part of the picture. The diversity of recommendations is another important dimension that has received renewed interest in recent years. It is known that accuracy and diversity can be conflicting goals and finding appropriate ways to combine them is still an open research question. Several ways have been proposed to measure the diversity of recommendations and to include its optimization in the loss function used to train the model."
              ],
              "metadata": null
            },
            {
              "id": "(Luo et al., 2024)",
              "snippets": [
                "Over the years, numerous recommendation algorithms have been developed to construct recommender systems that process massive amounts of data, aiming to identify potential user preferences and provide optimal recommendations (Resnick, 1994)(Balabanovic et al., 1997)(Deshpande et al., 2004)(Zhao et al., 2016)",
                "However, most personalized recommender systems compromise the accuracy of recommended items in their pursuit of increased diversity in recommendations (Fleder et al., 2007)[16]",
                "Many current diversity-oriented recommender systems adopt a fixed strategy to adjust the diversity degree for all users.Typically, they define a score function that balances diversity and accuracy with a hyper-parameter.Subsequently, the generated recommendation list is re-ranked based on the calculated scores (Lu et al., 2018)(Robinson et al., 2020)",
                "Two indicators, intra-user diversity and mean absolute error were selected to evaluate recommended diversity and accuracy, respectively [27]."
              ],
              "metadata": null
            },
            {
              "id": "(Yin et al., 2022)",
              "snippets": [
                "Most re-ranking-based diversification methods balance the trade-off between recommendation accuracy and diversity because improving one usually sacrifices the other (Kaminskas and Bridge, 2016). Accordingly, the objective function of these methods is formulated as a weighted sum of recommendation accuracy and diversity, where the weights control the recommender system's preferences for accuracy and diversity. Recommendation diversity is normally measured as the average pair-wise dissimilarity of recommended items (Ziegler et al., 2005;Zhang and Hurley, 2008)."
              ],
              "metadata": null
            },
            {
              "id": "(Raza, 2021)",
              "snippets": [
                "Normally, the machine learning models are evaluated using RMSE to check for the prediction accuracy. However, in a recommender system, we also need to see if the user is getting the relevant (good) recommendations. So, we need to evaluate our models using other measures such as precision @k (the proportion of the top k results that are relevant) and recall @k (the proportion of all relevant results included in the top k). We also take the F1-score @k (harmonic mean of both) as one metric. We take RMSE, precision, recall and F1-score as our accuracy metrics. \n\nSince diversity is a key principle in our design, we also need to evaluate our model on diversity and novelty. In our work, we take both (diversity and novelty) as our diversity metrics. We compute the diversity as the average dissimilarity of all pairs of items in a user's recommended list. We compute the novelty as the ratio of how many recommended items are unknown to a user. \n\nRecommendations high on accuracy metrics are more biased towards popular items or items that are rated extensively by users. The diverse recommendations, on the other hand, are less biased towards popular items and less aligned with users' preferences. Both diversity and novelty trade off recommendation accuracy but the difference between these two is that the former recommends less biased items from the seen list of recommended items, whereas the latter makes recommendations from the unseen items. Since our primary goal is to include both diversity and accuracy in a unified optimization framework, we define a composite metric to measure the trade-off between these measures, which is the weighted sum of the F1-score and the combined diversity scores (mean of diversity and novelty)."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al., 2024)",
              "snippets": [
                "In this section, we first evaluate all algorithms based on the accuracy and diversity as two key metrics. Table 2 presents the results of our offline experiments on two industrial dataset. Based on these results, we have the following findings: (1) Compared to the base DCN model, all algorithms can enhance both user-level and system-level diversity",
                "RECALL and MRR focus on the accuracy of the recommended results, ILAD measures the diversity of the user-level recommendation list, and CC assesses the diversity at the system level. For each dataset, bold scores indicate the best in each column, underlined scores indicate the best baseline, and +/\u2212 represents whether the value is higher/lower compared to the DCN model. For all metrics, the higher the result, the better."
              ],
              "metadata": null
            },
            {
              "id": "(Zhao et al., 2023)",
              "snippets": [
                "(3) Model evaluations: How to compare and evaluate the model performances when multiple metrics are provided. It is worth further investigating how to aggregate diverse metrics into one single metric for comparison purposes where the scales and variations of metrics might be different. Rank-based evaluations via the average of the ranks in multiple metrics could avoid the scale issue but cannot be applied in model selection."
              ],
              "metadata": null
            },
            {
              "id": "(Castells et al., 2023)",
              "snippets": [
                "Over the last decade, a multitude of algorithmic approaches were proposed to create more novel and diverse recommendations by optimizing the list of suggestions according to corresponding computational metrics. As with any computational metric, it is however important to ensure that these metrics are actually valid proxies of the users' perceptions, e.g., of the novelty and diversity of the recommendations. Moreover, it is crucial to understand how these perceptions then impact the users' satisfaction with the system. Intra-list dissimilarity, as mentioned above, is widely used in the literature to express the diversity of a recommendation list. In (Ziegler et al., 2005), the impact of topic diversification on user satisfaction was explored with the help of a user study 13 . Different levels of diversifications were tried and the obtained peak value in satisfaction indicated that the participants preferred a certain degree of diversification in their book recommendations. Similar observations were made in [Castagnos et al. 2013] in the context of movie recommendations. In their study, the authors also presented study participants with recommendation lists that had different degrees of diversity. The authors found that (i) participants notice the different diversity levels and (ii) that diversity may positively influence user satisfaction."
              ],
              "metadata": null
            },
            {
              "id": "(Robinson et al., 2020)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "User-centric Evaluation Methods",
          "text": "\nWhile computational metrics provide quantitative measures of diversity, user-centric evaluation methods are essential for validating these metrics and understanding how diversity impacts user experience. Researchers employ various user study approaches to ensure that computational diversity metrics actually reflect users' perceptions and satisfaction <Paper corpusId=\"256616145\" paperTitle=\"(Castells et al., 2023)\" isShortName></Paper>.\n\nUser studies have consistently shown that participants can detect different levels of diversity in recommendation lists. In experiments where users are presented with recommendation lists having varying degrees of diversification, researchers have observed that users not only notice these differences but also express preferences regarding diversity levels <Paper corpusId=\"256616145\" paperTitle=\"(Castells et al., 2023)\" isShortName></Paper>. Studies in both book and movie recommendation domains have confirmed these findings, indicating that diversity perception is consistent across different content types <Paper corpusId=\"256616145\" paperTitle=\"(Castells et al., 2023)\" isShortName></Paper>.\n\nImportantly, user studies have revealed a preference pattern regarding diversity. Users typically prefer recommendation lists with a moderate degree of diversification rather than lists that are either completely homogeneous or extremely diverse <Paper corpusId=\"256616145\" paperTitle=\"(Castells et al., 2023)\" isShortName></Paper>. This suggests an optimal \"sweet spot\" for diversity that balances novelty with relevance to user interests.\n\nThe impact of diversity on user satisfaction has been assessed through both online and targeted user studies. These evaluations have demonstrated that incorporating diversity into recommendation lists generally increases user satisfaction <Paper corpusId=\"271744774\" paperTitle=\"(Coppolillo et al., 2024)\" isShortName></Paper>. Conversely, when diversity is not considered, research shows that user interactions with recommendation systems tend toward homogenization and decreased utility <Paper corpusId=\"271744774\" paperTitle=\"(Coppolillo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"39558129\" paperTitle=\"(Chaney et al., 2017)\" isShortName></Paper>.\n\nUser-centric evaluations go beyond traditional accuracy-focused simulations to capture real-world user perceptions and behaviors. While simulations can demonstrate how diversity might improve system-level metrics, they cannot adequately represent how actual users perceive and evaluate diversity in recommendations <Paper corpusId=\"42183268\" paperTitle=\"(Willemsen et al., 2016)\" isShortName></Paper>. To address this limitation, researchers have developed metrics more suitable for diversity evaluation, such as coverage (the range of items a system can and does recommend) and serendipity (the extent to which recommended items are both unexpected and satisfactory) <Paper corpusId=\"42183268\" paperTitle=\"(Willemsen et al., 2016)\" isShortName></Paper>.\n\nThese user-centric approaches also help researchers understand how diversity affects cognitive aspects of the recommendation experience, such as choice difficulty. By studying actual users interacting with diversified item sets, researchers can assess whether diversity reduces choice overload and leads to higher satisfaction with the decision-making process <Paper corpusId=\"42183268\" paperTitle=\"(Willemsen et al., 2016)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Castells et al., 2023)",
              "snippets": [
                "Over the last decade, a multitude of algorithmic approaches were proposed to create more novel and diverse recommendations by optimizing the list of suggestions according to corresponding computational metrics. As with any computational metric, it is however important to ensure that these metrics are actually valid proxies of the users' perceptions, e.g., of the novelty and diversity of the recommendations. Moreover, it is crucial to understand how these perceptions then impact the users' satisfaction with the system. Intra-list dissimilarity, as mentioned above, is widely used in the literature to express the diversity of a recommendation list. In (Ziegler et al., 2005), the impact of topic diversification on user satisfaction was explored with the help of a user study 13 . Different levels of diversifications were tried and the obtained peak value in satisfaction indicated that the participants preferred a certain degree of diversification in their book recommendations. Similar observations were made in [Castagnos et al. 2013] in the context of movie recommendations. In their study, the authors also presented study participants with recommendation lists that had different degrees of diversity. The authors found that (i) participants notice the different diversity levels and (ii) that diversity may positively influence user satisfaction."
              ],
              "metadata": null
            },
            {
              "id": "(Coppolillo et al., 2024)",
              "snippets": [
                "Several online and targeted user studies assessed the increase in user satisfaction when diversity is incorporated into the list of suggested items [10](Hurley et al., 2011). For example, Allison et al. (Chaney et al., 2017) show that, if diversity (besides other objectives) is not taken into account, the interactions between users and recommender systems are prone to homogenization and, consequently, low utility."
              ],
              "metadata": null
            },
            {
              "id": "(Chaney et al., 2017)",
              "snippets": [
                "Recommendation systems are ubiquitous and impact many domains; they have the potential to influence product consumption, individuals' perceptions of the world, and life-altering decisions. These systems are often evaluated or trained with data from users already exposed to algorithmic recommendations; this creates a pernicious feedback loop. Using simulations, we demonstrate how using data confounded in this way homogenizes user behavior without increasing utility."
              ],
              "metadata": null
            },
            {
              "id": "(Willemsen et al., 2016)",
              "snippets": [
                "Existing research such as discussed above typically evaluates diversity by performing simulations to show that enhancing diversity improves the accuracy or efficiency with which simulated users interact with a recommender system. (Ge et al., 2010) propose a number of metrics that are more suitable for the evaluation of diversity: Coverage, describing the number of items a recommender system can and does recommend; and Serendipity, describing to what extent the recommended items are unexpected and satisfactory. However, to better understand how real users perceive and evaluate diversity, and to be able to answer our question of whether diversification can reduce choice difficulty, we need to go beyond these simulations and study actual users' diversity perceptions, choice difficulty, and satisfaction with diversified item sets."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Experimental Evaluation Methodologies",
          "text": "\nThe experimental evaluation of recommendation systems that optimize for diversity involves using a combination of both accuracy-focused and diversity-focused metrics. On the accuracy side, researchers commonly employ metrics such as precision@k (the proportion of top k results that are relevant), recall@k (the proportion of all relevant results in the top k), F1-score@k (the harmonic mean of precision and recall), RMSE (Root Mean Square Error), and MRR (Mean Reciprocal Rank) <Paper corpusId=\"232320407\" paperTitle=\"(Raza, 2021)\" isShortName></Paper> <Paper corpusId=\"270440683\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nFor diversity measurement, several specialized metrics are used:\n- Intra-list distance (ILD) measures the diversity within a recommendation list by calculating the mean distance between all pairs of items <Paper corpusId=\"235792479\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper>\n- Coverage (C@n) evaluates the percentage of items that appear at least once in recommendation lists <Paper corpusId=\"218486752\" paperTitle=\"(Mansoury et al., 2020)\" isShortName></Paper>\n- Gini index measures how uniformly items appear in recommendation lists, with lower values indicating fairer distribution <Paper corpusId=\"218486752\" paperTitle=\"(Mansoury et al., 2020)\" isShortName></Paper>\n- Entropy assesses the uniformity of the distribution of recommended items, with higher values indicating greater diversity <Paper corpusId=\"218486752\" paperTitle=\"(Mansoury et al., 2020)\" isShortName></Paper>\n- Category coverage (CC) measures how well the recommendations cover a user's interests across different categories <Paper corpusId=\"235792479\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper>\n- Novelty is calculated as the ratio of recommended items that are unknown to a user <Paper corpusId=\"232320407\" paperTitle=\"(Raza, 2021)\" isShortName></Paper>\n\nResearchers often use composite metrics to evaluate the trade-off between accuracy and diversity. For example, one approach defines a composite metric as \"the weighted sum of the F1-score and the combined diversity scores (mean of diversity and novelty)\" <Paper corpusId=\"232320407\" paperTitle=\"(Raza, 2021)\" isShortName></Paper>.\n\nThe experimental evaluation methodologies also vary based on the diversity optimization approach being tested. These approaches can be broadly categorized into two main groups:\n\n1. **Diversity modeling**: These are recommendation models directly optimized to produce diverse recommendations. Most of these systems are derivations or extensions of Matrix Factorization (MF) models <Paper corpusId=\"267069408\" paperTitle=\"(Carraro et al., 2024)\" isShortName></Paper>. End-to-end approaches have been developed to jointly optimize accuracy and diversity during training by incorporating item category information <Paper corpusId=\"274130930\" paperTitle=\"(Han et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235324643\" paperTitle=\"(Zheng et al., 2021)\" isShortName></Paper> <Paper corpusId=\"252668512\" paperTitle=\"(Bao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"255825546\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n2. **Diversity post-processing**: These approaches apply a post-processing step (typically using a greedy strategy) to refine an initial list of recommendations from a conventional system to balance relevance and diversity <Paper corpusId=\"267069408\" paperTitle=\"(Carraro et al., 2024)\" isShortName></Paper>. The greedy algorithm typically selects candidate items that maximize a re-ranking objective function combining both relevance and diversity.\n\nWhen evaluating these approaches, researchers have noted that post-processing modules operate independently of the candidate generation process, meaning diversity signals are not incorporated during training, which can lead to suboptimal solutions <Paper corpusId=\"274130930\" paperTitle=\"(Han et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235324643\" paperTitle=\"(Zheng et al., 2021)\" isShortName></Paper> <Paper corpusId=\"8071414\" paperTitle=\"(Zhou et al., 2008)\" isShortName></Paper>. In contrast, end-to-end approaches address this issue by incorporating diversity metrics during the training phase.\n\nExperimental results often compare a baseline model (without diversity optimization) against various diversity-enhanced algorithms to demonstrate improvements in both user-level and system-level diversity metrics while monitoring the impact on accuracy metrics <Paper corpusId=\"270440683\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Raza, 2021)",
              "snippets": [
                "Normally, the machine learning models are evaluated using RMSE to check for the prediction accuracy. However, in a recommender system, we also need to see if the user is getting the relevant (good) recommendations. So, we need to evaluate our models using other measures such as precision @k (the proportion of the top k results that are relevant) and recall @k (the proportion of all relevant results included in the top k). We also take the F1-score @k (harmonic mean of both) as one metric. We take RMSE, precision, recall and F1-score as our accuracy metrics. \n\nSince diversity is a key principle in our design, we also need to evaluate our model on diversity and novelty. In our work, we take both (diversity and novelty) as our diversity metrics. We compute the diversity as the average dissimilarity of all pairs of items in a user's recommended list. We compute the novelty as the ratio of how many recommended items are unknown to a user. \n\nRecommendations high on accuracy metrics are more biased towards popular items or items that are rated extensively by users. The diverse recommendations, on the other hand, are less biased towards popular items and less aligned with users' preferences. Both diversity and novelty trade off recommendation accuracy but the difference between these two is that the former recommends less biased items from the seen list of recommended items, whereas the latter makes recommendations from the unseen items. Since our primary goal is to include both diversity and accuracy in a unified optimization framework, we define a composite metric to measure the trade-off between these measures, which is the weighted sum of the F1-score and the combined diversity scores (mean of diversity and novelty)."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al., 2024)",
              "snippets": [
                "In this section, we first evaluate all algorithms based on the accuracy and diversity as two key metrics. Table 2 presents the results of our offline experiments on two industrial dataset. Based on these results, we have the following findings: (1) Compared to the base DCN model, all algorithms can enhance both user-level and system-level diversity",
                "RECALL and MRR focus on the accuracy of the recommended results, ILAD measures the diversity of the user-level recommendation list, and CC assesses the diversity at the system level. For each dataset, bold scores indicate the best in each column, underlined scores indicate the best baseline, and +/\u2212 represents whether the value is higher/lower compared to the DCN model. For all metrics, the higher the result, the better."
              ],
              "metadata": null
            },
            {
              "id": "(Liang et al., 2021)",
              "snippets": [
                "Diverisity. We evaluate the recommendation diversity by the intra-list distance (ILD) [26,(Zhang et al., 2008) and category coverage (CC) [26](Wu et al., 2019). ILD measures the diversity of the recommended item set by the mean distance between all pairs of items, while CC measures the coverage of the user interests by counting the relevant categories in the set."
              ],
              "metadata": null
            },
            {
              "id": "(Mansoury et al., 2020)",
              "snippets": [
                "For evaluation, we use the following metrics to measure the effectiveness of each method: \n\n(1) Precision (P@n): The fraction of the recommended items shown to the users that are part of the users' profile in the test set. (2) Coverage (C@n): The percentage of items which appear at least once in the recommendation lists. (3) Gini index (G@n): The measure of fair distribution of recommended items. It takes into account how uniformly items appear in recommendation lists. Uniform distribution will have Gini index equal to zero which is the ideal case (lower Gini index is better). Given all the recommendation lists for users, L, and p(i k |L) as the probability of the k-th least recommended item being drawn from L calculated as (Vargas et al., 2014): \n\nwhere L u is the recommendation list for user u. Now, Gini index of L can be computed as: \n\n(4) Entropy (E@n): Given the distribution of recommended items, entropy measures the uniformity of that distribution. Uniform distribution has the highest entropy or information gain, thus higher entropy is more desired when the goal is increasing diversity. \n\nwhere p(i |L) is the observed probability value of item i in recommendation lists L."
              ],
              "metadata": null
            },
            {
              "id": "(Carraro et al., 2024)",
              "snippets": [
                "Diversification methods can be categorised into two main groups: diversity modelling and diversity post-processing.\n\nThe first group of works includes recommendation models that are directly optimized to produce diverse recommendations. Most of these RSs are derivations or extensions of the popular and effective Matrix Factorization (MF) model.\n\nThe second group of works includes approaches that apply a post-processing step (typically, using a greedy strategy) that refines an initial list of recommendations from a conventional RS to obtain a final set of recommendations that balances relevance and diversity. More formally, these techniques typically produce a final list of recommended items of size from a larger set of candidate recommendations of size (with > ), where is generated by a baseline recommendation algorithm that ranks items based on their relevance. The vast majority of the RS literature on this uses greedy re-ranking, which involves an iterative process where items are selected from one by one and added to . At each step, the greedy algorithm selects the candidate item that maximizes a re-ranking objective function , like the one defined in Eq. 1. Here, is a linear combination of two scores, i.e. a candidate item's relevance and the diversity this item brings when added to the current version of ."
              ],
              "metadata": null
            },
            {
              "id": "(Han et al., 2024)",
              "snippets": [
                "Existing methods for controlling recommendation diversity can be broadly categorized into two types: post-processing and end-toend approaches. In the early stage, the post-processing modules (Chen et al., 2017)(Sha et al., 2016)(Ziegler et al., 2005) are introduced to control diversity after generating candidate items. However, since the post-processing modules operate independently of the candidate generation process, diversity signals are not incorporated during training, leading to suboptimal solutions (Zheng et al., 2021)(Zhou et al., 2008). To address this issue, end-to-end approaches (Bao et al., 2022)(Zhang et al., 2023)(Zheng et al., 2021) have been developed to jointly optimize accuracy and diversity during training by incorporating item category information."
              ],
              "metadata": null
            },
            {
              "id": "(Zheng et al., 2021)",
              "snippets": [
                "These years much effort has been devoted to improving the accuracy or relevance of the recommendation system. Diversity, a crucial factor which measures the dissimilarity among the recommended items, received rather little scrutiny. Directly related to user satisfaction, diversification is usually taken into consideration after generating the candidate items. However, this decoupled design of diversification and candidate generation makes the whole system suboptimal. In this paper, we aim at pushing the diversification to the upstream candidate generation stage, with the help of Graph Convolutional Networks (GCN). Although GCN based recommendation algorithms have shown great power in modeling complex collaborative filtering effect to improve the accuracy of recommendation, how diversity changes is ignored in those advanced works. We propose to perform rebalanced neighbor discovering, category-boosted negative sampling and adversarial learning on top of GCN. We conduct extensive experiments on real-world datasets. Experimental results verify the effectiveness of our proposed method on diversification. Further ablation studies validate that our proposed method significantly alleviates the accuracy-diversity dilemma."
              ],
              "metadata": null
            },
            {
              "id": "(Bao et al., 2022)",
              "snippets": [
                "Collaborative Metric Learning (CML) has recently emerged as a popular method in recommendation systems (RS), closing the gap between metric learning and Collaborative Filtering. Following the convention of RS, existing methods exploit unique user representation in their model design. This paper focuses on a challenging scenario where a user has multiple categories of interests. Under this setting, we argue that the unique user representation might induce preference bias, especially when the item category distribution is imbalanced. To address this issue, we propose a novel method called \\textit{Diversity-Promoting Collaborative Metric Learning} (DPCML), with the hope of considering the commonly ignored minority interest of the user. The key idea behind DPCML is to include a multiple set of representations for each user in the system. Based on this embedding paradigm, user preference toward an item is aggregated from different embeddings by taking the minimum item-user distance among the user embedding set. Furthermore, we observe that the diversity of the embeddings for the same user also plays an essential role in the model. To this end, we propose a \\textit{diversity control regularization} term to accommodate the multi-vector representation strategy better. Theoretically, we show that DPCML could generalize well to unseen test data by tackling the challenge of the annoying operation that comes from the minimum value. Experiments over a range of benchmark datasets speak to the efficacy of DPCML."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al., 2023)",
              "snippets": [
                "Accuracy and diversity have long been considered to be two conflicting goals for recommendations. We point out, however, that as the diversity is typically measured by certain pre-selected item attributes, e.g., category as the most popularly employed one, improved diversity can be achieved without sacrificing recommendation accuracy, as long as the diversification respects the user's preference about the pre-selected attributes. This calls for a fine-grained understanding of a user's preferences over items, where one needs to recognize the user's choice is driven by the quality of the item itself, or the pre-selected attributes of the item. In this work, we focus on diversity defined on item categories. We propose a general diversification framework agnostic to the choice of recommendation algorithm. Our solution disentangles the learnt user representation in the recommendation module into category- independent and category-dependent components to differentiate a user's preference over items from two orthogonal perspectives. Experimental results on three benchmark datasets and online A/B test demonstrate the effectiveness of our solution in improving both recommendation accuracy and diversity. In-depth analysis suggests that the improvement is due to our improved modeling of users' categorical preferences and refined ranking within item categories."
              ],
              "metadata": null
            },
            {
              "id": "(Zhou et al., 2008)",
              "snippets": [
                "Recommender systems use data on past user preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse niche objects, the most reliably accurate results are obtained by methods that recommend objects based on user or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybrid with an accuracy-focused algorithm. By tuning the hybrid appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and diversity of recommendations."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Types of Diversity Optimization Methods",
          "text": "\nResearch in diversity optimization for recommendation systems has developed several distinct methodological approaches:\n\n## Re-ranking Techniques\nRe-ranking methods modify an initial list of candidate recommendations produced by a base algorithm to enhance diversity. These methods are flexible as they treat the base algorithm as a \"black box\" that can be replaced freely <Paper corpusId=\"218832125\" paperTitle=\"(Mo et al., 2020)\" isShortName></Paper>. Typically, a greedy selection algorithm scans through a large candidate list and selects items that optimize both relevance and diversity <Paper corpusId=\"218832125\" paperTitle=\"(Mo et al., 2020)\" isShortName></Paper> <Paper corpusId=\"237941156\" paperTitle=\"(Beregovskaya et al., 2021)\" isShortName></Paper>. The Maximal Marginal Relevance (MMR) approach was one of the earliest works in this area, originally used for text summarization <Paper corpusId=\"240070324\" paperTitle=\"(Baghi et al., 2021)\" isShortName></Paper>.\n\n## Diversity Modeling/End-to-End Approaches\nThese methods directly optimize recommendation models to produce diverse recommendations. Most are derivations or extensions of Matrix Factorization (MF) models <Paper corpusId=\"267069408\" paperTitle=\"(Carraro et al., 2024)\" isShortName></Paper>. End-to-end approaches jointly optimize accuracy and diversity during the training phase by incorporating item category information <Paper corpusId=\"274130930\" paperTitle=\"(Han et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235324643\" paperTitle=\"(Zheng et al., 2021)\" isShortName></Paper>. This addresses a key limitation of post-processing methods, which operate independently of candidate generation and thus don't incorporate diversity signals during training <Paper corpusId=\"274130930\" paperTitle=\"(Han et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235324643\" paperTitle=\"(Zheng et al., 2021)\" isShortName></Paper> <Paper corpusId=\"8071414\" paperTitle=\"(Zhou et al., 2008)\" isShortName></Paper>.\n\n## Probabilistic Methods\nDeterminantal Point Process (DPP) has gained popularity as an elegant probabilistic model for set diversity <Paper corpusId=\"220793830\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"29160711\" paperTitle=\"(Gartrell et al., 2016)\" isShortName></Paper>. DPP provides diversified recommendations based on a positive semi-definite kernel matrix, with recent developments in low-rank factorization enabling faster learning and prediction algorithms <Paper corpusId=\"29160711\" paperTitle=\"(Gartrell et al., 2016)\" isShortName></Paper>.\n\n## Result Diversification and Clustering\nThese methods typically incorporate metrics of both similarity (relevance) and diversity. Initial ranked lists based on relevance are subsequently re-ranked to enhance diversity at the top of the list <Paper corpusId=\"220841858\" paperTitle=\"(Huppenkothen et al., 2020)\" isShortName></Paper>. Many approaches use objective functions that jointly optimize relevance and diversity, with parameters to express the trade-off between metrics <Paper corpusId=\"220841858\" paperTitle=\"(Huppenkothen et al., 2020)\" isShortName></Paper> <Paper corpusId=\"144219695\" paperTitle=\"(McAlister et al., 2018)\" isShortName></Paper>.\n\n## Evolutionary Algorithms\nEvolutionary algorithms (EAs) optimize recommender systems by considering multiple performance indicators simultaneously, including accuracy, novelty, and diversity <Paper corpusId=\"228929584\" paperTitle=\"(Zhang et al._1, 2020)\" isShortName></Paper>. Multi-objective optimization techniques help find neighbors in memory-based collaborative filtering to achieve accurate and diverse recommendations <Paper corpusId=\"228929584\" paperTitle=\"(Zhang et al._1, 2020)\" isShortName></Paper>. One notable approach is the probabilistic multi-objective evolutionary algorithm that balances accuracy and diversity using a multiparent probability genetic operator <Paper corpusId=\"228929584\" paperTitle=\"(Zhang et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"1025328\" paperTitle=\"(Zhang et al., 2010)\" isShortName></Paper>.\n\n## Temporal and Categorical Methods\nSome specialized approaches focus on temporal diversity, measuring how recommendation diversity changes over time <Paper corpusId=\"240070324\" paperTitle=\"(Baghi et al., 2021)\" isShortName></Paper>. Other methods increase diversity through category-based techniques like avoiding duplicate categories <Paper corpusId=\"240070324\" paperTitle=\"(Baghi et al., 2021)\" isShortName></Paper> or using entropy concepts to balance novelty and relevancy <Paper corpusId=\"240070324\" paperTitle=\"(Baghi et al., 2021)\" isShortName></Paper>. Recent techniques like Locality-Sensitive Hashing (LSH) have also been proposed to produce diversified recommendations <Paper corpusId=\"240070324\" paperTitle=\"(Baghi et al., 2021)\" isShortName></Paper>.\n\n## Fairness-Oriented Approaches\nSome methods aim to improve both diversity and fairness of recommendation systems, either by extending the rating prediction model with additional terms to reduce bias, or by applying re-ranking techniques to build more diversified recommendation lists <Paper corpusId=\"242039436\" paperTitle=\"(Elahi et al., 2021)\" isShortName></Paper> <Paper corpusId=\"232185223\" paperTitle=\"(Abdollahpouri et al., 2021)\" isShortName></Paper> <Paper corpusId=\"59158829\" paperTitle=\"(Abdollahpouri et al., 2019)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Mo et al., 2020)",
              "snippets": [
                "Two main techniques, i.e., reranking and diversityoriented techniques, are used to improve diversity of POI recommendation. The reranking-based technique re-arranges the order of items in a candidate list generated from an existing recommendation algorithm. The advantage of reranking is flexibility, because it considers the base algorithm as a black box that can be replaced freely. The diversity-oriented recommendation designs new algorithms to optimize diversity directly, usually increasing diversity. \n\nThe reranking-based techniques (Valcarce et al., 2017)[23] select recommended items from a large candidate list generated from a base algorithm that is considered a black box function. Usually, greedy selection is adopted to avoid combination explosion and improve the efficiency of the reranking technique. An n-size candidate list is scanned k(k<n) iterations, and in each iteration, the item with the highest objective score is selected as part of the user's final recommendation list."
              ],
              "metadata": null
            },
            {
              "id": "(Beregovskaya et al., 2021)",
              "snippets": [
                "Several strategies have been proposed to address the issue of diversity. In earlier studies, authors have proposed a greedy selection algorithm (Bradley et al., 2001). In this method, the items are first sorted according to their similarity to the target query, and then the algorithm begins to gradually build the search set (or recommendation list) so that both similarity and diversity are optimized."
              ],
              "metadata": null
            },
            {
              "id": "(Baghi et al., 2021)",
              "snippets": [
                "The maximal marginal relevance (MRR) (Carbonell et al., 1998) was one of the earliest works to maximize relevance and diversity in reranking retrieved items used in text summarization. Ziegler et al. (Ziegler et al., 2005) proposed a topic diversification method to increase the diversity of recommendation list by decreasing the intra-list similarity (ILS). Bridge et al. (Bridge et al., 2006) measured the distance between items using Hamming Distance, which enhanced the diversity of the recommendations by considering collaborative data only",
                "Zhang et al. (Zhang et al., 2008) modeled these goals as a bi-criterion optimization problem. Lathia et al. (Lathia et al., 2010) showed the importance of temporal diversity in recommender systems by proposing an approach to measuring how the diversity of collaborative filtering data changes over the time. Abbassi et al. (Abbassi et al., 2013) proposed a (partition) matroid constraint algorithm to increase the diversity of representative documents set by avoiding selecting duplicate categories. Premchaiswadi et al. (Premchaiswadi et al., 2013) proposed a ranking method named Total Diversity Effect Ranking (TDE) that increases the diversity of recommendation list by considering the overall diversity effect of each item",
                "utilizing the concept of entropy, find recommendations by considering novelty and relevancy. Cheng et al. (Cheng et al., 2017) modeled the diversification problem in the recommendation system as a supervised learning task and formulated this diversified recommendation task as two optimization problems. Then proposed a diversified collaborative filtering method to solve the optimization problems. Wang et al. (Wang et al., 2020) proposed a novel method named DivRec LSH to produce diversified recommendations, which was achieved by using Locality-Sensitive Hashing (LSH) technique. Liu et al. [32] proposed the determinantal point process (DPP) method to promoting diversity in recommendations."
              ],
              "metadata": null
            },
            {
              "id": "(Carraro et al., 2024)",
              "snippets": [
                "Diversification methods can be categorised into two main groups: diversity modelling and diversity post-processing.\n\nThe first group of works includes recommendation models that are directly optimized to produce diverse recommendations. Most of these RSs are derivations or extensions of the popular and effective Matrix Factorization (MF) model.\n\nThe second group of works includes approaches that apply a post-processing step (typically, using a greedy strategy) that refines an initial list of recommendations from a conventional RS to obtain a final set of recommendations that balances relevance and diversity. More formally, these techniques typically produce a final list of recommended items of size from a larger set of candidate recommendations of size (with > ), where is generated by a baseline recommendation algorithm that ranks items based on their relevance. The vast majority of the RS literature on this uses greedy re-ranking, which involves an iterative process where items are selected from one by one and added to . At each step, the greedy algorithm selects the candidate item that maximizes a re-ranking objective function , like the one defined in Eq. 1. Here, is a linear combination of two scores, i.e. a candidate item's relevance and the diversity this item brings when added to the current version of ."
              ],
              "metadata": null
            },
            {
              "id": "(Han et al., 2024)",
              "snippets": [
                "Existing methods for controlling recommendation diversity can be broadly categorized into two types: post-processing and end-toend approaches. In the early stage, the post-processing modules (Chen et al., 2017)(Sha et al., 2016)(Ziegler et al., 2005) are introduced to control diversity after generating candidate items. However, since the post-processing modules operate independently of the candidate generation process, diversity signals are not incorporated during training, leading to suboptimal solutions (Zheng et al., 2021)(Zhou et al., 2008). To address this issue, end-to-end approaches (Bao et al., 2022)(Zhang et al., 2023)(Zheng et al., 2021) have been developed to jointly optimize accuracy and diversity during training by incorporating item category information."
              ],
              "metadata": null
            },
            {
              "id": "(Zheng et al., 2021)",
              "snippets": [
                "These years much effort has been devoted to improving the accuracy or relevance of the recommendation system. Diversity, a crucial factor which measures the dissimilarity among the recommended items, received rather little scrutiny. Directly related to user satisfaction, diversification is usually taken into consideration after generating the candidate items. However, this decoupled design of diversification and candidate generation makes the whole system suboptimal. In this paper, we aim at pushing the diversification to the upstream candidate generation stage, with the help of Graph Convolutional Networks (GCN). Although GCN based recommendation algorithms have shown great power in modeling complex collaborative filtering effect to improve the accuracy of recommendation, how diversity changes is ignored in those advanced works. We propose to perform rebalanced neighbor discovering, category-boosted negative sampling and adversarial learning on top of GCN. We conduct extensive experiments on real-world datasets. Experimental results verify the effectiveness of our proposed method on diversification. Further ablation studies validate that our proposed method significantly alleviates the accuracy-diversity dilemma."
              ],
              "metadata": null
            },
            {
              "id": "(Zhou et al., 2008)",
              "snippets": [
                "Recommender systems use data on past user preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse niche objects, the most reliably accurate results are obtained by methods that recommend objects based on user or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybrid with an accuracy-focused algorithm. By tuning the hybrid appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and diversity of recommendations."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al., 2020)",
              "snippets": [
                "Diversity measures the variety of items in a recommendation list, which is commonly modeled as the aggregate pairwise similarity of recommended items (Ziegler et al., 2005). Typically, models to improve diversity of recommendations include Determinantal Point Process (DPP) (Chen et al., 2017)(Gartrell et al., 2016), which proposes a novel algorithm to greatly accelerate the greedy MAP inference and provides diversified recommendations accordingly; Greedy Re-ranking methods (Barraza-Urbina, 2017)(Kelly et al., 2006)(Smyth et al., 2001)(Vargas et al., 2014)(Ziegler et al., 2005), which provide diversified recommendations based on the combination of the item's relevance and its average distance to items already in the recommended list; and Latent Factor models to optimize diversity measures [22,57]61]"
              ],
              "metadata": null
            },
            {
              "id": "(Gartrell et al., 2016)",
              "snippets": [
                "Determinantal point processes (DPPs) have garnered attention as an elegant probabilistic model of set diversity. They are useful for a number of subset selection tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. In this work we present a new method for learning the DPP kernel from observed data using a low-rank factorization of this kernel. We show that this low-rank factorization enables a learning algorithm that is nearly an order of magnitude faster than previous approaches, while also providing for a method for computing product recommendation predictions that is far faster (up to 20x faster or more for large item catalogs) than previous techniques that involve a full-rank DPP kernel. Furthermore, we show that our method provides equivalent or sometimes better test log-likelihood than prior full-rank DPP approaches."
              ],
              "metadata": null
            },
            {
              "id": "(Huppenkothen et al., 2020)",
              "snippets": [
                "Research on recommender systems and information retrieval systems generally distinguishes between diversity as the differences between items in a set, and novelty as the difference between present and past experience. While much research concerns introducing noveltyand, as a related concept, serendipity-into recommender systems, there is no analogue for novelty in the context of cohort selection, and we do not discuss it any further except where needed.\n\nThere are a number of different proposed frameworks to introduce diversity into recommender systems, including result-diversification and re-ranking, and clustering methods. Result-diversification methods generally incorporate metrics of both similarity (or relevance) and diversity. Ranked lists of candidate items are produced according to relevance metric, and subsequently re-ranked to enhance the diversity or novelty of the items at the top of the list (e.g. [73,74]). Many approaches use an objective that jointly optimizes relevance and diversity, using a parameter to express the trade-off between both metrics for a candidate set. This objective can be used, for example, in greedy algorithms that re-rank an initial set selected using e.g. a nearest-neighbour collaborative filtering method [75]."
              ],
              "metadata": null
            },
            {
              "id": "(McAlister et al., 2018)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al._1, 2020)",
              "snippets": [
                "Evolutionary algorithms (EAs) are used to optimize these recommender systems by considering multiple performance indicators, e.g., accuracy, novelty and diversity (Zhang et al., 2017)(Ribeiro et al., 2012)(Rodr\u00edguez et al., 2012). To achieve accurate and diverse recommendations, Karabadji et al. (Karabadji et al., 2018) improved a memory-based CF method by using multi-objective optimization to find neighbors. A new probabilistic multi-objective evolutionary algorithm was proposed in (Zhang et al., 2010) that strikes a good balance between accuracy and diversity, in which a new crossover operator called multiparent probability genetic operator and a new topic diversity indicator were introduced."
              ],
              "metadata": null
            },
            {
              "id": "(Zhang et al., 2010)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Elahi et al., 2021)",
              "snippets": [
                "Some existing works in this direction enhance the diversity of the recommendation output by modifying the core recommendation algorithms. Others rely on re-ranking the output of an existing recommendation algorithm (Abdollahpouri et al., 2019)(Adomavicius et al., 2012)(Jugovac et al., 2017)(Trattner et al., 2017). In the former case, the rating prediction model is extended with additional terms aiming to improve the fairness of the system, e.g., by reducing the bias. In the latter case, re-ranking techniques are applied on top of the existing recommendation algorithm, e.g., to post-process the recommendation output and to build a more diversified list (Abdollahpouri et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Abdollahpouri et al., 2021)",
              "snippets": [
                "Recommendation and ranking systems are known to suffer from popularity bias; the tendency of the algorithm to favor a few popular items while under-representing the majority of other items. Prior research has examined various approaches for mitigating popularity bias and enhancing the recommendation of long-tail, less popular, items. The effectiveness of these approaches is often assessed using different metrics to evaluate the extent to which over-concentration on popular items is reduced. However, not much attention has been given to the user-centered evaluation of this bias; how different users with different levels of interest towards popular items are affected by such algorithms. In this paper, we show the limitations of the existing metrics to evaluate popularity bias mitigation when we want to assess these algorithms from the users\u2019 perspective and we propose a new metric that can address these limitations. In addition, we present an effective approach that mitigates popularity bias from the user-centered point of view. Finally, we investigate several state-of-the-art approaches proposed in recent years to mitigate popularity bias and evaluate their performances using the existing metrics and also from the users\u2019 perspective. Our experimental results using two publicly-available datasets show that existing popularity bias mitigation techniques ignore the users\u2019 tolerance towards popular items. Our proposed user-centered method can tackle popularity bias effectively for different users while also improving the existing metrics."
              ],
              "metadata": null
            },
            {
              "id": "(Abdollahpouri et al., 2019)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "In recommendation systems, how are new methods that optimize diversity typically evaluated?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly point to the importance of considering diversity in recommendation systems and how its impact on research in this field.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Diversity in recommendation systems refers to the variety of items suggested to users, aiming to avoid monotony and provide a broader range of options.",
              "Diversification has become one of the leading topics of recommender system research not only as a way to solve the over-fitting problem but also an approach to increasing the quality of the user's experience with the recommender system."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide different diversity measures that have been considered in the literature such as similarity, coverage, novelty, etc.",
            "weight": 0.09999999999999999,
            "evidence": [
              "2.1 Intra-List Diversity (ILD): Intra-List Diversity measures the dissimilarity between items within a single recommendation list. It is commonly calculated using the average pairwise distance between items [1].",
              "2.2 Coverage: Coverage measures the proportion of items from the entire catalog that appear in recommendations across all users [2].",
              "2.3 Novelty and Serendipity: Novelty measures how unfamiliar recommended items are to users, while serendipity captures the pleasant surprise factor of recommendations [3].",
              "2.4 Gini Coefficient: The Gini coefficient measures the inequality in item popularity distribution within recommendations [4]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss that there is no single unified metric for measuring diversity as each paper tends to define its own measure.",
            "weight": 0.09999999999999999,
            "evidence": [
              "We found that while all research groups agree that diversity is important and should be measured, few groups agree on the metric that should be used. While some metrics (such as the intra-list diversity) appear more often, the community still has not accepted one (or several) of them as the preferred diversity measurement."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answers should mention that conventional performance metrics should be used in conjunction with diversity measures for an overall comparison of different recommendation systems.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Several of research groups have focused on the effect of diversification on the quality of the recommendation procedure. In order to evaluate this they used a combination of proposed diversity measure and existing RS performance measures such as F-measure, MAE and NMAE [28] to determine how diversification impacts the overall performance of the RS."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should explain that the evaluation can be done offline, online through user study or in a hybrid way.",
            "weight": 0.09999999999999999,
            "evidence": [
              "1. **Offline Evaluation**: Uses historical data to evaluate the performance of the recommendation algorithm. Metrics such as precision, recall, and F1-score are used to assess accuracy, while diversity metrics are used to assess diversity.",
              "2. **Online Evaluation**: Involves A/B testing or sample testing to evaluate the performance of the recommendation algorithm in real-world scenarios. This method provides more accurate results but is resource-intensive.",
              "3. **Hybrid Evaluation**: Combines offline and online evaluation methods to leverage the strengths of both approaches."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could discuss some challenges of evaluating diversity in RS systems such as the tradeoff between diversity measure and accuracy.",
            "weight": 0.049999999999999996,
            "evidence": [
              "1. **Trade-off between Diversity and Accuracy**: Optimizing diversity often comes at the cost of accuracy. Finding the right balance between these two metrics is crucial.",
              "2. **Cold Start Problem**: Evaluating diversity for new users or items with limited interaction data is challenging.",
              "3. **Scalability**: Evaluating diversity for large-scale recommendation systems can be computationally expensive."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer could quote the first paper introducing diversity in the recommendation systems.",
            "weight": 0.049999999999999996,
            "evidence": [
              "Bradley and Smith [2] were one the first to mention diversity by proposing the introduction of diversification in the recommendation procedure and also evaluating a new algorithm designed to diversify recommendations."
            ]
          }
        ]
      }
    }
  },
  {
    "question": "How have open-source publishing platforms impacted the global distribution of academic publishing?",
    "response": {
      "sections": [
        {
          "title": "Introduction: The Emergence of Open Access Publishing",
          "text": "\nThe internet's emergence in the early 1990s dramatically transformed academic publishing by digitizing scholarly communication processes. While this digital revolution streamlined peer review, publishing procedures, and content retrieval, most academic articles remained behind paywalls, limiting their accessibility to wider audiences. <Paper corpusId=\"219019938\" paperTitle=\"(Shopovski et al., 2020)\" isShortName></Paper> This restricted access prompted the birth of the open access movement in the early 1990s, with advocates emphasizing openness as a fundamental scientific principle and launching open publishing venues to provide free access to scholarly content. <Paper corpusId=\"219019938\" paperTitle=\"(Shopovski et al., 2020)\" isShortName></Paper>\n\nThe web's advent catalyzed significant changes in academic publishing, though this transformation occurred more rapidly in the hard sciences than in humanities and social sciences. Early adoption in scientific fields led to the creation of disciplinary-specific repositories like arXiv.org, which now houses over 1 million pre-prints - drafts of articles before peer review. These repositories established de facto open access to scholarship even in draft form, making it freely available to anyone with internet access. <Paper corpusId=\"247947726\" paperTitle=\"(Ball, 2017)\" isShortName></Paper>\n\nBeyond enabling the shift from print to online publishing, the internet has facilitated the development of new tools, formats, and business models for open access journal publishing. Open access literature is characterized as \"digital, online, free of charge, and free of most copyright and licensing restrictions.\" <Paper corpusId=\"57620679\" paperTitle=\"(Dawson, 2014)\" isShortName></Paper> Researchers can make their work openly accessible through two primary routes: publishing in open access journals (\"gold\") or self-archiving manuscripts in open repositories (\"green\"). The Budapest Open Access Initiative of 2002 is widely considered the defining moment in the movement's formalization, after which it has grown substantially. Some analyses suggest that gold open access could become the dominant publishing mode within a decade, potentially accounting for 50 percent of scholarly journal articles between 2017 and 2021, and reaching 90 percent as soon as 2020 or more conservatively by 2025. <Paper corpusId=\"57620679\" paperTitle=\"(Dawson, 2014)\" isShortName></Paper> <Paper corpusId=\"41597409\" paperTitle=\"(Lewis, 2012)\" isShortName></Paper>",
          "citations": [
            {
              "id": "(Shopovski et al., 2020)",
              "snippets": [
                "The emergence of the Internet has changed the landscape of academic publishing. Digitalization facilitated peer review, publishing procedures, and content retrieval (Suber 2012). However, the majority of academic articles were brought behind pay walls, thus remaining inaccessible to a wider audience. This initiated another approach towards academic publishing in the early 1990s when the open access movement was conceived. Its protagonists, underlining the openness as a fundamental scientific ethos, launched open access publishing venues to provide free usage of scholarly content (Bjork, 2018)."
              ],
              "metadata": null
            },
            {
              "id": "(Ball, 2017)",
              "snippets": [
                "The medium of academic publishing has changed radically in the last 20 years, since the advent of the Web in the early 1990s, although that change has come more slowly to the humanities and social sciences, including design studies, than in the hard sciences. The creation of disciplinary-specific repositories in the early 1990s flourished in the sciences with examples such as arXiv.org now containing over 1 million pre-prints. Pre-prints \u200bare articles written for submission to peer-reviewed journals but which have not yet undergone the peerreview process. A \u200bpost-print \u200bis the manuscript \u200bafter \u200bit has been peer-reviewed but before it has been copyedited. The concept of pre-prints and post-prints have been widely accepted and used in the sciences for almost three decades and are de facto \u200bopen access \u200b, meaning that this scholarship even as it is only in draft form is freely available on the Web for anyone with an Internet connection."
              ],
              "metadata": null
            },
            {
              "id": "(Dawson, 2014)",
              "snippets": [
                "The rise of the Internet has not only enabled the rapid shift from print to online, but has also enabled the development of new tools, new formats, and even new business models for open access journal publishing.\n\n\"Open access literature is digital, online, free of charge, and free of most copyright and licensing restrictions\" (Suber, 2004). Researchers can make their articles open access by publishing in an open access journal (\"gold\") or by self-archiving a copy of their manuscript in an open repository (\"green\"). The Budapest Open Access Initiative of 2002 (Chan et al., 2002) is widely viewed as the defining event when this movement was born, and since then it has grown rapidly. In fact, Lewis (2012) argues that gold open access will be the dominant mode of publishing within the next decade."
              ],
              "metadata": null
            },
            {
              "id": "(Lewis, 2012)",
              "snippets": [
                "Open access (OA) is an alternative business model for the publication of scholarly journals. It makes articles freely available to readers on the Internet and covers the costs associated with publication through means other than subscriptions. This article argues that Gold OA, where all of the articles of a journal are available at the time of publication, is a disruptive innovation as defined by business theorist Clayton Christensen. Using methods described by Christensen, we can predict the growth of Gold OA. This analysis suggests that Gold OA could account for 50 percent of the scholarly journal articles sometime between 2017 and 2021, and 90 percent of articles as soon as 2020 and more conservatively by 2025."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Types of Open-Source Publishing Platforms",
          "text": "\nOpen-source publishing platforms represent a diverse ecosystem of digital infrastructure that supports open access scholarly communication. Among these, Open Journal Systems (OJS) stands as one of the most established and widely-used platforms. OJS is an open-source software solution designed to manage and publish scholarly journals online, reducing publishing costs compared to traditional methods. With over 34,000 OA journals worldwide using this platform, OJS has become a cornerstone of the \"scholar-led publishing\" movement that positions itself as an alternative to commercial journals charging Article Processing Charges (APCs) <Paper corpusId=\"262190921\" paperTitle=\"(Pampel, 2023)\" isShortName></Paper> <Paper corpusId=\"261302471\" paperTitle=\"(Willinsky, 2005)\" isShortName></Paper>. OJS helps reduce time and energy spent on clerical and managerial tasks associated with editing while improving record keeping and editorial process efficiency <Paper corpusId=\"35088764\" paperTitle=\"(Mangiafico et al., 2014)\" isShortName></Paper>.\n\nFor book publishing, the Open Monograph Press (OMP) serves as a companion platform to OJS. It provides similar workflow management capabilities but is specifically designed for monographs, edited volumes, and other scholarly editions. The system supports internal and external reviewing and editing processes while reducing clerical costs and overhead <Paper corpusId=\"39136493\" paperTitle=\"(Raju et al., 2015)\" isShortName></Paper>.\n\nSeveral newer platforms have emerged with support from private funders like the Andrew W. Mellon Foundation. PubPub, developed by MIT Knowledge Futures Group, and Manifold, created by the University of Minnesota Press and the City University of New York, have gained particular attention for making publications freely available <Paper corpusId=\"231628709\" paperTitle=\"(Watkinson, 2021)\" isShortName></Paper>. Library publishers also utilize Janeway and Digital Commons alongside OJS to publish open access books, textbooks, and journals <Paper corpusId=\"258594943\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>.\n\nDespite their benefits, no single platform can effectively support the entire spectrum of scholarly publishing activities. For example, while platforms like PubPub offer better future-oriented features and contemporary web functions, OJS remains valued for its stability and well-documented standard journal production functions. However, OJS's conventional approach to scholarly journals means it is less equipped for supporting new content formats (like HTML alongside PDF) and leveraging experimental possibilities in digital-only journals <Paper corpusId=\"241180134\" paperTitle=\"(Collective et al., 2021)\" isShortName></Paper>.\n\nTo address these limitations, initiatives such as the Next Generation Library Publishing project are working to link different tools together and develop \"community governed, open solutions that rival best-of-breed commercial tools\" <Paper corpusId=\"231628709\" paperTitle=\"(Watkinson, 2021)\" isShortName></Paper> <Paper corpusId=\"258594943\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Pampel, 2023)",
              "snippets": [
                "In the field of OA, the term publication platform covers a wide range of digital information infrastructures that RPOs use to make publications openly accessible. These can be infrastructures for operating journals, publishing books, or making digital collections openly accessible. Within the realm of textual publications, publishing platforms frequently facilitate quality assurance mechanisms through standardized review processes that exceed the capabilities of institutional OA repositories. The Open Journal Systems (OJS) software (Willinsky, 2005) for operating OA journals is widely used. Worldwide, over 34,000 OA journals use the open-source software Open Journal Systems (OJS). Frequently, they position themselves under the label of scholar-led publishing as an alternative to commercial journals that finance their activities through APCs."
              ],
              "metadata": null
            },
            {
              "id": "(Willinsky, 2005)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Mangiafico et al., 2014)",
              "snippets": [
                "Another wrinkle in the author-pays model of open-access publishing is the so-called hybrid journal, which publishes most articles in a traditional subscription format but offers authors an option to \"bail out\" a specific article by paying an author-side APC",
                "The most common form of gold open access supported by libraries (and that is part of the open-access landscape in general) is what we call \"pure gold\"-where there is neither a subscription fee nor an APC, and all articles are available to anyone with an Internet connection. The organization that developed the most commonly used software platform for publishing such journals-the Open Journal Systems (OJS) from the Public Knowledge Project-reports that more than twenty thousand journals are being published using that platform."
              ],
              "metadata": null
            },
            {
              "id": "(Raju et al., 2015)",
              "snippets": [
                "The pilot is guided by the announcement by Willinsky [29] that the OMP software platform is available for use to manage the editorial workflow required for the publication of monographs, edited volumes, and other scholarly editions. The workflow allows for internal and external reviewing and editing. In using open source software, the system is freely available to the academic community, and designed to reduce clerical costs and supplies, as well as overhead, with libraries becoming involved in hosting the system and scholars being able to play a more active role as series editors in the review process."
              ],
              "metadata": null
            },
            {
              "id": "(Watkinson, 2021)",
              "snippets": [
                "In the USA and UK over the last few years, the Andrew W. Mellon Foundation and other private funders have supported the development of open-source publishing platforms. Several of these have been particularly well used by publishers interested in making their publications freely available in general, or with a particular focus on pandemic-related knowledge. These include PubPub from the MIT Knowledge Futures Group and Manifold from the University of Minnesota Press and the City University of New York (Berkery, Windhorn, & Lawlor, 2019;Maxwell, Bordini, & Shamash, 2017). The Next Generation Library Publishing project is one of several initiatives aimed at linking such tools together (https://educopia.org/next-generation-library-publishing/)."
              ],
              "metadata": null
            },
            {
              "id": "(Ma et al., 2023)",
              "snippets": [
                "Library publishers use a range of open source and proprietary publishing platforms such as open journal systems (OJS), Janeway, Manifold and Digital Commons to publish books, textbooks and journals on an open access basis. Educopia's Next Generation Library Publishing Project (NGLP) is currently researching 'community governed, open solutions that rival best-of-breed commercial tools and advance scholarly communication in important ways'."
              ],
              "metadata": null
            },
            {
              "id": "(Collective et al., 2021)",
              "snippets": [
                "We note this here not to propose an idealized workflow for journal management, but rather to highlight the insufficiency of any one tool or platform to effectively support the entire spectrum of ESTS activities. This is especially true of working in an open source environment: despite the emergence of new technologies, business models, and infrastructural players resulting from vigorous debates on the social, technical, and economic conditions of academic research publication practice over the past twenty years, 1 OA today still comes with significant hurdles and requires investment of resources often beyond what academic organizations can commit (see Eve and Gray (2020) for more on the latest developments in OA scholarly publishing)",
                "we explored alternative open source platforms for hosting ESTS, but in the end concluded that while other platforms (PubPub especially) offered better future-oriented features and contemporary web-functions, OJS was still a more stable and well-documented platform for providing standard journal production functions. OJS's more conventional imagination of scholarly journals, however, has meant that it is less infrastructured for supporting the production of content in new formats (HTML in addition to PDF, for example, as we have implemented starting this issue) and leveraging the experimental possibilities of virtual space for a digital only journal like ESTS."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Global Reach and Distribution Patterns",
          "text": "\nOpen access publishing has significantly altered the global distribution of academic publishing, challenging the traditional dominance of a few countries in the Global North. While commercial publishers from the United States, Great Britain, the Netherlands, and Germany have historically dominated academic publishing, open access platforms have enabled a more geographically diverse landscape <Paper corpusId=\"19104181\" paperTitle=\"(Solomon, 2013)\" isShortName></Paper>. This shift is particularly critical for developing countries, as open access helps \"level the playing field and provide a voice for developing country publications\" by removing price barriers and most permission barriers to scholarly literature <Paper corpusId=\"153413164\" paperTitle=\"(Gray, 2013)\" isShortName></Paper>.\n\nRegional patterns in open access adoption reveal important geographic variations. Latin American and African universities have embraced publisher-mediated (gold) open access models, while growth in Europe and North America has been primarily driven by repositories <Paper corpusId=\"221719726\" paperTitle=\"(Huang et al., 2020)\" isShortName></Paper>. This difference reflects varied approaches to implementing open access across regions, with different institutional and economic contexts shaping adoption patterns.\n\nThe impact of open-source publishing platforms on global distribution is particularly evident through the widespread adoption of Open Journal Systems (OJS). A comprehensive analysis of 25,671 journals using OJS\u2014journals largely absent from common citation indexes like Web of Science and Scopus\u2014revealed that scholarly communication is significantly more global than commonly recognized. These journals have published 5.8 million items across 136 countries, with 79.9% based in the Global South. Notably, 84.2% follow the \"diamond\" open access model, charging neither readers nor authors <Paper corpusId=\"254447831\" paperTitle=\"(Khanna et al., 2022)\" isShortName></Paper>.\n\nOpen access platforms have also fostered linguistic diversity in academic publishing. Among OJS journals, 48.3% operate in more than one language, with research published in 60 different languages including English, Indonesian, Spanish, and Portuguese <Paper corpusId=\"254447831\" paperTitle=\"(Khanna et al., 2022)\" isShortName></Paper>. This multilingual approach challenges the English-language dominance of traditional academic publishing and makes research more accessible to diverse global audiences.\n\nBeyond geographic and linguistic diversity, open scholarship has aimed to create a more equitable \"knowledge commons\" that promotes sustainable, inclusive, and participatory scholarly communication <Paper corpusId=\"260715586\" paperTitle=\"(Arthur et al., 2023)\" isShortName></Paper> <Paper corpusId=\"58845188\" paperTitle=\"(Albagli et al., 2018)\" isShortName></Paper>. Initiatives like the 2005 Salvador Declaration on Open Access have highlighted the importance of open access in promoting equity, particularly for developing countries, with goals aligned with the United Nations' Millennium Development Goals <Paper corpusId=\"260715586\" paperTitle=\"(Arthur et al., 2023)\" isShortName></Paper>.\n\nHowever, significant barriers to equitable global participation persist. Despite optimism that the internet would support sustainable \"knowledge societies,\" opportunities remain unequally distributed. Restricted internet access, limited digital literacy, the primacy of English-language publishing, and the costs of making research openly available continue to reinforce global North-South imbalances <Paper corpusId=\"260715586\" paperTitle=\"(Arthur et al., 2023)\" isShortName></Paper> <Paper corpusId=\"240532032\" paperTitle=\"(Arthur et al., 2021)\" isShortName></Paper>.\n\nSome critical perspectives question whether pursuing global scale in open access publishing risks \"repeating and maintaining the kind of centre/periphery relationality of power\" that characterizes traditional scholarly publishing. This critique has led to alternative approaches like \"scaling small,\" which focuses on serving local communities rather than pursuing global reach <Paper corpusId=\"233452312\" paperTitle=\"(Adema et al., 2021)\" isShortName></Paper> <Paper corpusId=\"225831883\" paperTitle=\"(Kiesewetter, 2020)\" isShortName></Paper>.\n\nRecent developments in open infrastructure are addressing some access inequities. For instance, open bibliographic databases like OpenAlex offer enhanced coverage of humanities, non-English languages, and Global South research compared to proprietary databases like Scopus and Web of Science. The unrestricted accessibility of such resources is particularly significant for researchers in lower-income countries, where accessing proprietary data sources may entail prohibitive costs <Paper corpusId=\"272368021\" paperTitle=\"(Alonso-Alvarez et al., 2024)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Solomon, 2013)",
              "snippets": [
                "Universities and societies published half the OA journals and about 40% of the OA articles indexed in Scopus during 2010. Unlike professional publishers that accounted for just over a third of the journals and 42% of the articles, university and society publishers tend to be located outside the four largest publishing countries, the USA, Great Britain, the Netherlands and Germany. Many of these journals, particularly those located in Latin America, Japan and Eastern Europe are being hosted on national or internationally funded software platforms that appear to provide a variety of services and in at least some cases are selective in the journals they will host. Since these journals tend not charge APCs, it appears universities and professional societies collaborating with national or international consortia that provide web hosting and variety of technical services are able to create and maintain high quality scholarly journals without the need or the problems associated with charging APCs."
              ],
              "metadata": null
            },
            {
              "id": "(Gray, 2013)",
              "snippets": [
                "It is in this context that Open Access becomes critically important as a way of levelling the playing field and providing a voice for developing country publications. Open Access has emerged as an alternative paradigm to address these concerns. OA refers to online access to scholarly literature that are free from price and most permission barriers."
              ],
              "metadata": null
            },
            {
              "id": "(Huang et al., 2020)",
              "snippets": [
                "The proportion of research outputs published in open access journals or made available on other freely-accessible platforms has increased over the past two decades, driven largely by funder mandates, institutional policies, grass-roots advocacy, and changing attitudes in the research community",
                "the analysis also showed that publisher-mediated (gold) open access was popular in Latin American and African universities, whereas the growth of open access in Europe and North America has mostly been driven by repositories."
              ],
              "metadata": null
            },
            {
              "id": "(Khanna et al., 2022)",
              "snippets": [
                "By analyzing 25,671 journals largely absent from common journal counts, as well as Web of Science and Scopus, this study demonstrates that scholarly communication is more of a global endeavor than is commonly credited. These journals, employing the open-source publishing platform Open Journal Systems (OJS), have published 5.8 million items; they are in 136 countries, with 79.9% in the Global South and 84.2% following the OA diamond model (charging neither reader nor author). A substantial proportion of journals operate in more than one language (48.3%), with research published in 60 languages (led by English, Indonesian, Spanish, and Portuguese)."
              ],
              "metadata": null
            },
            {
              "id": "(Arthur et al., 2023)",
              "snippets": [
                "The open scholarship movement has sought to make scholarly communication and research results freely available through the support of a nonprofit ecosystem whose cooperative work promotes a more sustainable, inclusive, and participatory \"knowledge commons\" (Albagli et al., 2018)[14](Hynek, 2008)(Rhoten et al., 2007)[17]. Accordingly, since its emergence at the start of the 21st century, many statements have advocated for greater equity in access to publications, open data, copyright revision, open educational resources, open government data, alternative metrics and assessment, and responsive research and development, with some calling for the implementation of Findable, Accessible, Interoperable and Reusable (FAIR) practices to facilitate greater public access, collaboration, and multi-stakeholder and citizen engagement (Hampson, 2020)(Crick, 2019)[20][21](Wilsdon et al., 2015). For example, the 2005 Salvador Declaration on Open Access highlighted the importance of open access in promoting equity, envisaging outcomes commensurate with the United Nations' Millennium Development Goals, including large-scale collaborative partnerships for more equitable access to information, especially among developing countries. \n\nYet, two decades after the foundational Budapest Open Access Initiative (2002) was first drafted and optimism that the Internet would assist in developing and supporting sustainable \"knowledge societies,\" the opportunities are unequally distributed [23]. Despite drives for greater equity of access to and engagement with education through networked platforms and repositories to bridge social divides (Beaulieu et al., 2018), restricted Internet access, limited digital literacy, the primacy of English language publishing, and crucially, the cost of making research openly available, have become barriers reinforcing the global North-South imbalance and inequities within and across borders (Arthur et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Albagli et al., 2018)",
              "snippets": [
                "The paper presents a critical analysis of the possibilities and limits of the Institutional Analysis and Development (IAD) framework, proposed by Elinor Ostrom and team, specially addressing the mutual relations between natural and knowledge commons. It results from an action-research project on the role of open science (OS) in development, carried out in the municipality of Ubatuba, on the North Coast of the State of Sao Paulo, Brazil, in 2015-2017. The work involved: systematizing the literature on the IAD framework; mapping and selecting literature representative of other theoretical and conceptual approaches; critically using and adapting the framework to the case studied. The project provided the opportunity to observe how these dynamics take place in a relatively small-scale (while heavily interconnected) context. While the IAD framework helped us to analyse the institutional, political, and governance issues affecting knowledge production and circulation, we observed the higher complexity of our action arena, shedding light on the fact that natural and knowledge commons are the two dimensions of the same \"commoning\" process."
              ],
              "metadata": null
            },
            {
              "id": "(Arthur et al., 2021)",
              "snippets": [
                "Open publishing services-such as the Open Library of Humanities, Open Book Publishers, Open Humanities Press, OpenEdition, Knowledge Unlatched, Ubiquity Press, and Language Science Press-are developing new approaches to assist in covering the APC and BPC costs of open access. Open Humanities Press is sustained in partnership with the University of Michigan Library and charges no article processing fees, while the Open Library of Humanities has developed a model of library partnership aimed at overcoming the current situation where academics produce publications that are given sometimes at a cost to publishers, who then sell these publications back to the academic libraries. Others, like Knowledge Unlatched, use membership models and the pooling of library consortia (Eve, 2017). While Knowledge Unlatched is the largest initiative in the humanities, it has recently changed from being a British not-for-profit company to a German-based private for-profit company, yet it affirms its goal to become the \"central open access platform,\" with emphasis on transparency from publishers regarding data usage  (Ross-Hellauer et al., 2018)."
              ],
              "metadata": null
            },
            {
              "id": "(Adema et al., 2021)",
              "snippets": [
                "Scaling small also critiques the idea that publishing needs to be able to scale 'globally' instead of serving local communities, for example (such as is achieved by local or regional activist research or certain language communities (Kiesewetter, 2020)M\u00e9ndez Cota, 2018)). Similarly, as Gary Hall from Open Humanities Press has argued, 'global scale', risks 'repeating and maintaining the kind of centre/periphery relationality of power we want to challenge' as part of the geopolitics of knowledge through our publishing endeavours, where a few countries in the so-called 'Global North' end up universalising what counts as valid knowledge."
              ],
              "metadata": null
            },
            {
              "id": "(Kiesewetter, 2020)",
              "snippets": [
                "In this article, I argue to open out from critical strands within the Open Access (OA) movement, to propose a genealogy that embraces the activism of feminist, queer, anti-colonial, anti-racist, and labour movements active since the 1980s. By discussing contemporary forms of feminist and intersectional approaches to OA publishing against a background of grassroots activism since the 1980s, I aim to open out from the engagement of \u2018concerned academics\u2019 towards those activists who share a politics of struggle against capitalist, colonialist, and patriarchal domination \u2013 across epistemological, disciplinary, and geographical boundaries. With this, I seek to tentatively articulate an approach to academic OA publishing in which academic and activist work is not perceived as something divided but as something that embodies different aspects of the same praxis online and offline."
              ],
              "metadata": null
            },
            {
              "id": "(Alonso-Alvarez et al., 2024)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Economic Models and Sustainability",
          "text": "\nThe financial crisis of 2008 served as a catalyst for reconsidering traditional subscription models in academic publishing, creating opportunities for alternative approaches <Paper corpusId=\"153899832\" paperTitle=\"(Scott, 2012)\" isShortName></Paper>. In response, several economic models for open access publishing have emerged. These include the \"green\" route (self-archiving in repositories), the \"gold\" route (publisher charges fees to make articles openly accessible), and the \"platinum\" or \"diamond\" route (journals operate without charging authors or readers) <Paper corpusId=\"154980746\" paperTitle=\"(Weller, 2013)\" isShortName></Paper> <Paper corpusId=\"233593264\" paperTitle=\"(Rosnay, 2021)\" isShortName></Paper>.\n\nThe economic sustainability of these models varies significantly. In the gold open access model, Article Processing Charges (APCs) for journals and Book Processing Charges (BPCs) for monographs present substantial barriers, particularly for humanities scholars who often lack grant funding to cover such costs. Well-established publishers typically charge high BPCs that limit who can publish open access books, creating inequities in participation <Paper corpusId=\"169562776\" paperTitle=\"(Schrader et al., 2018)\" isShortName></Paper>.\n\nTo address these economic challenges, innovative funding models have emerged. University presses have become important players in the open access ecosystem, publishing books with significantly lower or no charges compared to commercial publishers, though they often face limitations in staffing and technical capabilities <Paper corpusId=\"169562776\" paperTitle=\"(Schrader et al., 2018)\" isShortName></Paper>. \"Pure gold\" open access publications\u2014those with neither subscription fees nor APCs\u2014represent another important economic model supported by libraries. The Open Journal Systems (OJS) software platform alone supports more than twenty thousand such journals <Paper corpusId=\"35088764\" paperTitle=\"(Mangiafico et al., 2014)\" isShortName></Paper>.\n\nCollaborative economic models are also gaining traction. The Open Library of Humanities has developed a library partnership model that challenges the traditional cycle where academics produce publications given to publishers who then sell these publications back to academic libraries. Similarly, Knowledge Unlatched employs membership models and pooling of library consortia resources <Paper corpusId=\"240532032\" paperTitle=\"(Arthur et al., 2021)\" isShortName></Paper> <Paper corpusId=\"65281328\" paperTitle=\"(Eve, 2017)\" isShortName></Paper>. Other initiatives like Open Humanities Press partner with university libraries to sustain their operations without charging APCs <Paper corpusId=\"240532032\" paperTitle=\"(Arthur et al., 2021)\" isShortName></Paper>.\n\nThe hybrid journal model represents a complicated middle ground, where traditional subscription-based journals offer authors the option to make specific articles open access by paying an APC. This approach has been criticized for potentially creating a \"double-dipping\" scenario where publishers collect both subscription fees and APCs <Paper corpusId=\"35088764\" paperTitle=\"(Mangiafico et al., 2014)\" isShortName></Paper>.\n\nThe funding organization-commissioned platforms represent another emerging economic model. While promising for controlling costs and reducing administrative burdens on researchers, these platforms raise concerns about potential conflicts of interest, scaling difficulties, and the risk of institutional lock-in <Paper corpusId=\"240532032\" paperTitle=\"(Arthur et al., 2021)\" isShortName></Paper> <Paper corpusId=\"49319171\" paperTitle=\"(Ross-Hellauer et al., 2018)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Scott, 2012)",
              "snippets": [
                "The 'creative destruction' of the 2008 global financial crisis has produced new opportunities and forced legislators, administrators, academics and librarians to consider alternatives to traditional subscription models."
              ],
              "metadata": null
            },
            {
              "id": "(Weller, 2013)",
              "snippets": [
                "Open access publishing has been growing steadily in acceptance as not only a valid, but, rather the best, model of disseminating research publications (e.g. (Davis, 2010). Instead of academics publishing in proprietary journals access to which is then purchased by libraries or on article basis by individuals, open access makes publications freely accessible to all. There are different models for achieving this, the so-called green route, whereby the author places the article on their own site or the institutions repository, the gold route where the publisher charges a fee to make the article openly available and the platinum route, where the journal operates for free."
              ],
              "metadata": null
            },
            {
              "id": "(Rosnay, 2021)",
              "snippets": [
                "Open access models include APCs (gold, green, platinum, diamond and even black with pirate or shadow libraries), and a range of these policies, governance, economic and technical features of publishing platforms can be studied as infrastructure commons."
              ],
              "metadata": null
            },
            {
              "id": "(Schrader et al., 2018)",
              "snippets": [
                "Across the world, there is a growing interest in Open Access (OA) publishing. Therefore, OA publishing has become a trend and is of key importance to the scientific community. However, observing the publication landscape in Germany leads to a striking finding of very different approaches. In particular, OA book publishing is still in relatively early stages, leading to OA books being much less frequently published than OA journal articles. However, although well-established publishers offer the publication of OA books, only certain researchers can actually publish, because of high Book Processing Charges (BPCs). In contrast to such publishers, university presses publish books as OA without any or at significantly lower charges; however, university presses are often inadequately staffed and do not have the technical know-how of the state-of-the-art publishing of OA books possessed by well-established publishers."
              ],
              "metadata": null
            },
            {
              "id": "(Mangiafico et al., 2014)",
              "snippets": [
                "Another wrinkle in the author-pays model of open-access publishing is the so-called hybrid journal, which publishes most articles in a traditional subscription format but offers authors an option to \"bail out\" a specific article by paying an author-side APC",
                "The most common form of gold open access supported by libraries (and that is part of the open-access landscape in general) is what we call \"pure gold\"-where there is neither a subscription fee nor an APC, and all articles are available to anyone with an Internet connection. The organization that developed the most commonly used software platform for publishing such journals-the Open Journal Systems (OJS) from the Public Knowledge Project-reports that more than twenty thousand journals are being published using that platform."
              ],
              "metadata": null
            },
            {
              "id": "(Arthur et al., 2021)",
              "snippets": [
                "Open publishing services-such as the Open Library of Humanities, Open Book Publishers, Open Humanities Press, OpenEdition, Knowledge Unlatched, Ubiquity Press, and Language Science Press-are developing new approaches to assist in covering the APC and BPC costs of open access. Open Humanities Press is sustained in partnership with the University of Michigan Library and charges no article processing fees, while the Open Library of Humanities has developed a model of library partnership aimed at overcoming the current situation where academics produce publications that are given sometimes at a cost to publishers, who then sell these publications back to the academic libraries. Others, like Knowledge Unlatched, use membership models and the pooling of library consortia (Eve, 2017). While Knowledge Unlatched is the largest initiative in the humanities, it has recently changed from being a British not-for-profit company to a German-based private for-profit company, yet it affirms its goal to become the \"central open access platform,\" with emphasis on transparency from publishers regarding data usage  (Ross-Hellauer et al., 2018)."
              ],
              "metadata": null
            },
            {
              "id": "(Eve, 2017)",
              "snippets": [
                "EDITOR'S SUMMARYOpen access (OA) has been shaping and benefitting the scientific community for years now, but this new wave of disseminating research freely has not quite taken hold in the field of humanities. Though humanities publishers could also benefit from an OA model, many have been resistant, citing possible issues with plagiarism or appropriation of an author's work for less than ideal uses. There are also challenges with the cost of publishing OA content, which for humanities could be much higher than in the scientific community due to the length of works produced. Some institutions have taken to charging authors to publish their content in an OA model, while others have opted for a membership structure or joining a consortium of other institutions, such as Knowledge Unlatched or the Open Library of Humanities. While many great strides have been taken to bring OA to humanities, there is still work to do to make the study of humanity accessible to all."
              ],
              "metadata": null
            },
            {
              "id": "(Ross-Hellauer et al., 2018)",
              "snippets": [
                "As open access (OA) to publications continues to gather momentum, we should continuously question whether it is moving in the right direction. A novel intervention in this space is the creation of OA publishing platforms commissioned by funding organizations. Examples include those of the Wellcome Trust and the Gates Foundation, as well as recently announced initiatives from public funders like the European Commission and the Irish Health Research Board. As the number of such platforms increases, it becomes urgently necessary to assess in which ways, for better or worse, this emergent phenomenon complements or disrupts the scholarly communications landscape. This article examines ethical, organizational, and economic strengths and weaknesses of such platforms, as well as usage and uptake to date, to scope the opportunities and threats presented by funder OA platforms in the ongoing transition to OA. The article is broadly supportive of the aims and current implementations of such platforms, finding them a novel intervention which stands to help increase OA uptake, control costs of OA, lower administrative burden on researchers, and demonstrate funders\u2019 commitment to fostering open practices. However, the article identifies key areas of concern about the potential for unintended consequences, including the appearance of conflicts of interest, difficulties of scale, potential lock-in, and issues of the branding of research. The article ends with key recommendations for future consideration which include a focus on open scholarly infrastructure."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Institutional Adoption and Support",
          "text": "\nAcademic and research libraries have emerged as fundamental stakeholders in the open access ecosystem over the past decade. Their role has evolved from simply supporting open access practices and building institutional repositories to actively developing innovative digital publishing services and providing guidance on tools and practices in the new open science environment. <Paper corpusId=\"169090431\" paperTitle=\"(Ruggieri et al., 2018)\" isShortName></Paper> This institutional support has been critical for expanding the reach and sustainability of open access publishing.\n\nUniversities recognize that well-developed open access publishing infrastructure is essential for enhancing the competitiveness and impact of their research outputs. By adopting such infrastructure, institutions can provide their authors with immediate global visibility for their work. <Paper corpusId=\"204451753\" paperTitle=\"(Karlova et al., 2019)\" isShortName></Paper> This institutional backing is particularly valuable for innovative cross-disciplinary research that might struggle to find its audience through traditional publishing channels.\n\nLibrary publishing programs operate under a fundamentally different principle than commercial publishers. Without the constraint of profit requirements, these programs can invest in open access publishing of quality peer-reviewed research as the best approach to make research visible globally while shifting control from commercial entities back to the academy. This freedom from profit imperatives enables library publishers to provide homes for cross-disciplinary journals that break new ground, even if they take time to establish an audience. <Paper corpusId=\"219144326\" paperTitle=\"(Reed, 2019)\" isShortName></Paper>\n\nThe economic structure of library publishing programs offers significant advantages to journal editors. The platforms for hosting journals are typically provided at little or no cost, making library publishing particularly attractive for launching new journals. Once the infrastructure is operational, the marginal cost of adding new journals is minimal, as the technology maintenance costs are already covered. This dramatically lowers financial barriers to starting new journals, allowing editors to focus on publishing excellent peer-reviewed research rather than fundraising. <Paper corpusId=\"219144326\" paperTitle=\"(Reed, 2019)\" isShortName></Paper>\n\nJournal platforms used by library publishers are designed to automatically implement best practices and standards, such as those outlined by the Open Archives Initiative Protocol for Metadata Harvesting (OAI-MPH). This technical compliance ensures that content is readily discoverable through internet search engines and integrates machine-readable licenses that clearly indicate how content can be used. <Paper corpusId=\"219144326\" paperTitle=\"(Reed, 2019)\" isShortName></Paper> These technical advantages help maximize the reach and impact of research published through institutional platforms.\n\nUniversities also play a crucial role in supporting student-led open access publishing initiatives. Beyond financial assistance for website development, marketing, and hosting, institutions can provide valuable technical support, mentorship, and direction. This support often includes workshops on journal editing, peer review procedures, and copyright concerns to help students navigate the complexities of academic publishing. <Paper corpusId=\"261378749\" paperTitle=\"(Bansal et al., 2023)\" isShortName></Paper> By leveraging their resources and expertise, universities can serve as effective platforms for open access journals by hosting these publications on their websites or providing connectivity to them.\n\nThe growing institutional role in academic publishing represents a significant shift in the landscape. A survey of 998 journals using Open Journal Systems (OJS), a freely available open source platform for publishing scholarly journals, found that many of these publications maintain traditional scholarly standards with respect to peer review while being exceptional in terms of open access availability, low operating budgets, and participation from developing countries. <Paper corpusId=\"27259097\" paperTitle=\"(Taylor et al., 2013)\" isShortName></Paper> <Paper corpusId=\"30016074\" paperTitle=\"(Edgar et al., 2010)\" isShortName></Paper> This suggests that institutionally-supported open access journals are creating an alternative \"third path\" to scholarly publishing, distinct from traditional society and commercial publishing routes.",
          "citations": [
            {
              "id": "(Ruggieri et al., 2018)",
              "snippets": [
                "Over the last decade academic and research libraries have had a fundamental role in supporting Open access practices, in the construction of Institutional repositories, in digitization programs and in the development of additional services for their community. More recently they are moving toward Open science and are identified as one of the main stakeholders in promoting visibility and access of locally-produced scholarship by developing innovative digital publishing service, as well as providing support and guidance on tools and practices in the new open science environment."
              ],
              "metadata": null
            },
            {
              "id": "(Karlova et al., 2019)",
              "snippets": [
                "As Open Access publishing has been playing a more important role in scholarly communication, well-developed Open Access publishing infrastructure should be adopted by universities in order to make their outputs more competitive and provide its authors the immediate impact of their work."
              ],
              "metadata": null
            },
            {
              "id": "(Reed, 2019)",
              "snippets": [
                "In order for innovative cross-disciplinary research to find its audience, it must be easily discovered by scholars, professional practitioners, and the public. Journal publishing programs in libraries operate under the principle that investment in open access publishing of quality peer-reviewed research is the best way to make that research visible to a global audience and to shift control of publishing from commercial entities to the academy. Library publishers are therefore not constrained, as commercial publishers are, by the need to publish only research that will ensure a profit. This means that library publishing programs can provide a home for cross-disciplinary journals that break new ground and that may take time to find an audience.\n\nThe lack of a profit imperative for library publishing programs also means that the platform for hosting journals is provided to journals at little or no cost, which makes library publishing very attractive to editors looking for a place to publish a new journal. Once the infrastructure is operational, the cost to add a new journal to the system is negligible because the costs of maintaining the technology are already covered. This lowers the financial barriers to starting new journals, allowing editors to focus on the task of finding and publishing excellent peer-reviewed research instead of fundraising.\n\nJournal platforms used by library publishers are designed so that journals published on those systems automatically follow best practices and standards, such as those outlined by the Open Archives Initiative Protocol for Metadata Harvesting (OAI-MPH) that make the content readily discoverable by internet search engines. These platforms also integrate the use of machine-readable licenses that clearly indicate how the content can be used."
              ],
              "metadata": null
            },
            {
              "id": "(Bansal et al., 2023)",
              "snippets": [
                "Universities can play a crucial role in supporting the efforts of students who wish to launch their own open-access and peer-reviewed academic journals. In addition to financial assistance, such as funding for website development, marketing, and hosting costs, universities can provide technical support, mentorship, and direction to these students. This can include workshops on journal editing, peer review procedures, and copyright concerns, helping students navigate the complexities of academic publishing. Universities can also provide access to essential infrastructure, such as library resources and technical support services, which can be instrumental in facilitating the production and dissemination of open-access publications. By leveraging their resources and expertise, universities can serve as a platform for open-access journals by hosting these publications on their websites or by providing links to them. The provision of these resources and services by universities can enable students to focus on producing high-quality content, engaging in scholarly dialogue, and advancing their academic careers through publication."
              ],
              "metadata": null
            },
            {
              "id": "(Taylor et al., 2013)",
              "snippets": [
                "Brown, Griffiths and Rascoff, reporting on an extensive study of university publishing in the USconducted for Ithaka, discuss the present situation of increasing marginalization of university presses and argue for a stronger and more central presence for university publishing, touching on the emerging role of libraries in academic publishing [8]",
                "Edgar and Willinsky conducted a survey of journals using Open Journal Systems (OJS), a freely available open source platform for publishing scholarly journals (Edgar et al., 2010). This survey is highly relevant, as many Canadian open access journals are known to use OJS. This survey of 998 journals found that the journals are traditional with respect to peer reviewing and other aspects of scholarly publishing, but are exceptional in the number of open access titles, journals with extremely low operating budgets, and journals from developing countries. Edgar and Willinsky suggest that a number of these journals illustrate a third path to scholarly publishing, an alternative to traditional society and commercial publishing routes."
              ],
              "metadata": null
            },
            {
              "id": "(Edgar et al., 2010)",
              "snippets": [
                "A survey of 998 scholarly journals that use Open Journal Systems (OJS), an open source journal software platform, captures the characteristics of an emerging class of scholar-publisher open access journals. The journals in the sample follow traditional norms for peer-reviewing, acceptance rates, and disciplinary focus, but as a group are distinguished by the number that offer open access to their content, growth rates in new titles, participation rates from developing countries, and extremely low operating budgets. The survey also documents the limited degree to which open source software can alter a field of communication, for OJS appears to have created a third path, dedicated to maximizing access to research and scholarship, as an alternative to traditional scholarly society and commercial publishing routes. Normal 0 0 1 129 739 6 1 907 11.773 0 0 0"
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Collaborative and Community Approaches",
          "text": "\nOpen access publishing has fostered a distinctive collaborative ethos that explicitly rejects the competitive logics dominant in commercial publishing. Academic and library-led publishers have increasingly sought mutual support through collective organization, demonstrating a commitment to cooperation over competition. This collaborative approach has become one of the most noteworthy features of academic and library-led book publishing in recent years. <Paper corpusId=\"204442991\" paperTitle=\"(Deville et al., 2019)\" isShortName></Paper>\n\nSeveral key collectives exemplify this community-oriented approach. The Radical Open Access collective serves as a repository for tools and expertise shared across the open access publishing community, with the explicit goal of building \"horizontal alliances.\" Similarly, the ScholarLed collective, comprising six small academic-led presses, focuses attention on the need to open not just academic content but also the infrastructures of publishing themselves. <Paper corpusId=\"204442991\" paperTitle=\"(Deville et al., 2019)\" isShortName></Paper>\n\nThis emphasis on infrastructure control has become particularly urgent as large commercial publishers acquire community-built platforms for open research. Concerns have emerged about what scholars have termed the \"enclosure of scholarly infrastructures,\" seen in examples like Elsevier's takeovers of bepress and SSRN. <Paper corpusId=\"204442991\" paperTitle=\"(Deville et al., 2019)\" isShortName></Paper> <Paper corpusId=\"16046941\" paperTitle=\"(Lariviere et al., 2015)\" isShortName></Paper> These acquisitions highlight the risks of allowing commercial entities to dominate what many consider should remain community-controlled spaces.\n\nThe collaborative model offers a strategic advantage for small publishers navigating the contemporary publishing landscape. By working together, these smaller entities can generate the economies of scale necessary to develop competitive infrastructure while preserving their unique identities and forms of engagement with academic and non-academic communities. <Paper corpusId=\"204442991\" paperTitle=\"(Deville et al., 2019)\" isShortName></Paper> This approach represents a deliberate attempt to disrupt what has been characterized as an \"oligopoly\" of incumbent commercial publishers. <Paper corpusId=\"225380919\" paperTitle=\"(Beck et al., 2020)\" isShortName></Paper> <Paper corpusId=\"16046941\" paperTitle=\"(Lariviere et al., 2015)\" isShortName></Paper>\n\nThis collaborative spirit extends to the technological realm, where open-source publishing tools and platforms have become instrumental in supporting community-governed publishing initiatives. Projects like Educopia's Next Generation Library Publishing (NGLP) explicitly aim to develop \"community governed, open solutions that rival best-of-breed commercial tools and advance scholarly communication in important ways.\" <Paper corpusId=\"258594943\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper> Such initiatives represent a collective effort to ensure that the technical infrastructure supporting open access publishing remains aligned with the values of openness and community control that define the movement.",
          "citations": [
            {
              "id": "(Deville et al., 2019)",
              "snippets": [
                "One of the most noteworthy features about academic-and library-led book publishing in recent years has been the tendency for publishers to look to each other for forms of support. This, both implicitly and more explicitly, marks a rejection of the logics of competition that characterise commercial publishing. A number of the publishers referenced here are, for instance, part of and contributors to the Radical Open Access collective (2019), whose explicit goal is to build 'horizontal alliances'. Part of the function of the collective is to act as a repository for tools and forms of expertise that can be shared across the open access publishing community. Another prominent example is the recently formed ScholarLed collective (2018), which at the time of writing counts six presses of small sized presses all led by academics. Their explicit aim is to focus the attention of the open access community on the need to open up not just academic content, but the infrastructures of publishing. As Bilder et al. (2015) argue, \"[e]verything we have gained by opening content and data will be under threat if we allow the enclosure of scholarly infrastructures\". This is a particularly pressing issue in the context of large commercial entities taking over important community-built platforms for open research (e.g. Elsevier's takeovers of be.press and SSRN and many more examples exist (Bosman, J. & Kramer, B. (2016))). What such instances of collaboration promise is the possibility of generating the economies of scale necessary to develop the infrastructure necessary to be successful in the landscape of contemporary publishing, while allowing small publishers to retain their unique identities and forms of engagement with the academic and non-academic communities they serve."
              ],
              "metadata": null
            },
            {
              "id": "(Lariviere et al., 2015)",
              "snippets": [
                "The consolidation of the scientific publishing industry has been the topic of much debate within and outside the scientific community, especially in relation to major publishers\u2019 high profit margins. However, the share of scientific output published in the journals of these major publishers, as well as its evolution over time and across various disciplines, has not yet been analyzed. This paper provides such analysis, based on 45 million documents indexed in the Web of Science over the period 1973-2013. It shows that in both natural and medical sciences (NMS) and social sciences and humanities (SSH), Reed-Elsevier, Wiley-Blackwell, Springer, and Taylor & Francis increased their share of the published output, especially since the advent of the digital era (mid-1990s). Combined, the top five most prolific publishers account for more than 50% of all papers published in 2013. Disciplines of the social sciences have the highest level of concentration (70% of papers from the top five publishers), while the humanities have remained relatively independent (20% from top five publishers). NMS disciplines are in between, mainly because of the strength of their scientific societies, such as the ACS in chemistry or APS in physics. The paper also examines the migration of journals between small and big publishing houses and explores the effect of publisher change on citation impact. It concludes with a discussion on the economics of scholarly publishing."
              ],
              "metadata": null
            },
            {
              "id": "(Beck et al., 2020)",
              "snippets": [
                "Key debates around open access hinge on the role of incumbent commercial publishers, with new actors from library publishers to funders vying to disrupt what has been termed an 'oligopoly' (Larivi\u00e8re et al., 2015) with the help of open-source publishing tools and platforms (Maxwell et al. 2019)."
              ],
              "metadata": null
            },
            {
              "id": "(Ma et al., 2023)",
              "snippets": [
                "Library publishers use a range of open source and proprietary publishing platforms such as open journal systems (OJS), Janeway, Manifold and Digital Commons to publish books, textbooks and journals on an open access basis. Educopia's Next Generation Library Publishing Project (NGLP) is currently researching 'community governed, open solutions that rival best-of-breed commercial tools and advance scholarly communication in important ways'."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Impact on Research Accessibility and Visibility",
          "text": "\nOpen access publishing platforms have fundamentally transformed how research outputs are accessed and discovered globally. By making scholarly communication freely available online, these platforms have significantly enhanced the visibility and reach of academic work beyond traditional channels. Open access infrastructure adopted by universities helps make their research outputs \"more competitive and provide its authors the immediate impact of their work\" <Paper corpusId=\"204451753\" paperTitle=\"(Karlova et al., 2019)\" isShortName></Paper>. This immediate visibility is particularly valuable for innovative cross-disciplinary research that might struggle to find its audience through traditional publishing routes.\n\nThe enhanced discoverability of open access content represents one of its most significant impacts. Journal platforms used by library publishers implement best practices and standards such as the Open Archives Initiative Protocol for Metadata Harvesting (OAI-MPH), making content \"readily discoverable by internet search engines.\" These platforms also integrate machine-readable licenses that clearly indicate how content can be used, further enhancing accessibility <Paper corpusId=\"219144326\" paperTitle=\"(Reed, 2019)\" isShortName></Paper>. This technical infrastructure ensures that research can be easily found by scholars, practitioners, and the general public worldwide.\n\nFor researchers, open access publishing offers new pathways to enhance the visibility, accessibility, and impact of their work. By utilizing \"open-access platforms, and innovative methods of sharing information,\" scholars can connect with wider audiences, engage with new research communities, and increase the exposure of their work beyond conventional academic channels <Paper corpusId=\"273856487\" paperTitle=\"(Kouam, 2024)\" isShortName></Paper>. This expanded reach represents a fundamental shift in how academic knowledge circulates globally.\n\nThe impact on research accessibility is particularly significant for regions and countries traditionally marginalized in global knowledge production. Open access helps \"level the playing field and provide a voice for developing country publications\" by removing price barriers and most permission barriers to scholarly literature <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. The 2005 Salvador Declaration on Open Access highlighted this potential for promoting equity, with goals aligned with the United Nations' Millennium Development Goals, \"including large-scale collaborative partnerships for more equitable access to information, especially among developing countries\" <Paper corpusId=\"260715586\" paperTitle=\"(Arthur et al., 2023)\" isShortName></Paper>.\n\nHowever, despite the idealistic vision of creating \"sustainable 'knowledge societies'\" through open access, significant barriers to equitable participation remain. \"Restricted Internet access, limited digital literacy, the primacy of English language publishing, and crucially, the cost of making research openly available, have become barriers reinforcing the global North-South imbalance and inequities within and across borders\" <Paper corpusId=\"260715586\" paperTitle=\"(Arthur et al., 2023)\" isShortName></Paper> <Paper corpusId=\"240532032\" paperTitle=\"(Arthur et al., 2021)\" isShortName></Paper>. These persistent challenges highlight the complex relationship between technological infrastructure, economic models, and cultural factors in shaping global knowledge distribution.\n\nRecent developments in open bibliographic infrastructure are addressing some of these inequities. Open databases like OpenAlex offer \"enhanced coverage of humanities, non-English languages, and the Global South\" compared to proprietary databases like Scopus and Web of Science. The unrestricted accessibility of such resources is \"particularly significant for researchers in lower-income countries, where accessing proprietary data sources may entail a higher economic burden\" <Paper corpusId=\"272368021\" paperTitle=\"(Alonso-Alvarez et al., 2024)\" isShortName></Paper>. These open infrastructure initiatives represent important steps toward creating more equitable systems for knowledge discovery and use.\n\nThe impact of library publishing programs has been especially noteworthy. Operating without profit requirements, these programs provide a home for \"cross-disciplinary journals that break new ground and that may take time to find an audience\" <Paper corpusId=\"219144326\" paperTitle=\"(Reed, 2019)\" isShortName></Paper>. This support for diverse and experimental scholarship demonstrates how institutional backing for open access can foster intellectual innovation that might otherwise struggle in commercial publishing environments.",
          "citations": [
            {
              "id": "(Karlova et al., 2019)",
              "snippets": [
                "As Open Access publishing has been playing a more important role in scholarly communication, well-developed Open Access publishing infrastructure should be adopted by universities in order to make their outputs more competitive and provide its authors the immediate impact of their work."
              ],
              "metadata": null
            },
            {
              "id": "(Reed, 2019)",
              "snippets": [
                "In order for innovative cross-disciplinary research to find its audience, it must be easily discovered by scholars, professional practitioners, and the public. Journal publishing programs in libraries operate under the principle that investment in open access publishing of quality peer-reviewed research is the best way to make that research visible to a global audience and to shift control of publishing from commercial entities to the academy. Library publishers are therefore not constrained, as commercial publishers are, by the need to publish only research that will ensure a profit. This means that library publishing programs can provide a home for cross-disciplinary journals that break new ground and that may take time to find an audience.\n\nThe lack of a profit imperative for library publishing programs also means that the platform for hosting journals is provided to journals at little or no cost, which makes library publishing very attractive to editors looking for a place to publish a new journal. Once the infrastructure is operational, the cost to add a new journal to the system is negligible because the costs of maintaining the technology are already covered. This lowers the financial barriers to starting new journals, allowing editors to focus on the task of finding and publishing excellent peer-reviewed research instead of fundraising.\n\nJournal platforms used by library publishers are designed so that journals published on those systems automatically follow best practices and standards, such as those outlined by the Open Archives Initiative Protocol for Metadata Harvesting (OAI-MPH) that make the content readily discoverable by internet search engines. These platforms also integrate the use of machine-readable licenses that clearly indicate how the content can be used."
              ],
              "metadata": null
            },
            {
              "id": "(Kouam, 2024)",
              "snippets": [
                "The growing digital landscape provides scholars with emerging scholarly communication and alternative publication pathways, enabling them to enhance the visibility, accessibility, and impact of their research results. Adopting alternative publishing methods entails utilizing digital technologies, open-access platforms, and innovative methods of sharing information to connect with a wide range of readers, interact with new research communities, and increase the exposure of academic work beyond conventional academic channels. \n\nA crucial approach for scholars is to adopt open-access publication models and repositories that offer complimentary and unimpeded access to research outputs, thus enhancing the accessibility of scholarly knowledge to researchers, policymakers, practitioners, and the general public. By disseminating their research via open-access journals or uploading preprints to open repositories, academics can enhance the discoverability and influence of their work, connect with scholars from different regions and fields, and foster the exchange of knowledge and collaboration among varied audiences."
              ],
              "metadata": null
            },
            {
              "id": "(Arthur et al., 2023)",
              "snippets": [
                "The open scholarship movement has sought to make scholarly communication and research results freely available through the support of a nonprofit ecosystem whose cooperative work promotes a more sustainable, inclusive, and participatory \"knowledge commons\" (Albagli et al., 2018)[14](Hynek, 2008)(Rhoten et al., 2007)[17]. Accordingly, since its emergence at the start of the 21st century, many statements have advocated for greater equity in access to publications, open data, copyright revision, open educational resources, open government data, alternative metrics and assessment, and responsive research and development, with some calling for the implementation of Findable, Accessible, Interoperable and Reusable (FAIR) practices to facilitate greater public access, collaboration, and multi-stakeholder and citizen engagement (Hampson, 2020)(Crick, 2019)[20][21](Wilsdon et al., 2015). For example, the 2005 Salvador Declaration on Open Access highlighted the importance of open access in promoting equity, envisaging outcomes commensurate with the United Nations' Millennium Development Goals, including large-scale collaborative partnerships for more equitable access to information, especially among developing countries. \n\nYet, two decades after the foundational Budapest Open Access Initiative (2002) was first drafted and optimism that the Internet would assist in developing and supporting sustainable \"knowledge societies,\" the opportunities are unequally distributed [23]. Despite drives for greater equity of access to and engagement with education through networked platforms and repositories to bridge social divides (Beaulieu et al., 2018), restricted Internet access, limited digital literacy, the primacy of English language publishing, and crucially, the cost of making research openly available, have become barriers reinforcing the global North-South imbalance and inequities within and across borders (Arthur et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Arthur et al., 2021)",
              "snippets": [
                "Open publishing services-such as the Open Library of Humanities, Open Book Publishers, Open Humanities Press, OpenEdition, Knowledge Unlatched, Ubiquity Press, and Language Science Press-are developing new approaches to assist in covering the APC and BPC costs of open access. Open Humanities Press is sustained in partnership with the University of Michigan Library and charges no article processing fees, while the Open Library of Humanities has developed a model of library partnership aimed at overcoming the current situation where academics produce publications that are given sometimes at a cost to publishers, who then sell these publications back to the academic libraries. Others, like Knowledge Unlatched, use membership models and the pooling of library consortia (Eve, 2017). While Knowledge Unlatched is the largest initiative in the humanities, it has recently changed from being a British not-for-profit company to a German-based private for-profit company, yet it affirms its goal to become the \"central open access platform,\" with emphasis on transparency from publishers regarding data usage  (Ross-Hellauer et al., 2018)."
              ],
              "metadata": null
            },
            {
              "id": "(Alonso-Alvarez et al., 2024)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Challenges and Limitations",
          "text": "\nDespite the impressive growth of open access publishing, several significant challenges and limitations continue to constrain its global impact. Financial barriers remain one of the most persistent obstacles, particularly for book publishing. While open access has made substantial inroads in journal publishing, its adoption in the book market has been comparatively delayed, despite books continuing to be the leading publishing format for social sciences and humanities. <Paper corpusId=\"65173306\" paperTitle=\"(Arias, 2018)\" isShortName></Paper>\n\nThe economic structure of open access book publishing creates significant inequities in participation. Well-established publishers typically charge high Book Processing Charges (BPCs) that effectively limit who can publish open access books. This financial barrier means that \"only certain researchers can actually publish\" through these established channels. University presses offer an alternative by publishing open access books without charges or at significantly lower costs, but they often face substantial limitations in staffing and technical capabilities compared to commercial publishers. <Paper corpusId=\"169562776\" paperTitle=\"(Schrader et al., 2018)\" isShortName></Paper>\n\nTechnical limitations of existing open source platforms present another significant challenge. No single tool or platform can effectively support the entire spectrum of scholarly publishing activities. While newer platforms like PubPub offer improved contemporary web functions and future-oriented features, more established platforms like Open Journal Systems (OJS) may be valued for stability and documentation but are less equipped for supporting new content formats (such as HTML alongside PDF) or leveraging the experimental possibilities of digital-only journals. This creates a tension between innovation and stability in publishing infrastructure. <Paper corpusId=\"241180134\" paperTitle=\"(Collective et al., 2021)\" isShortName></Paper>\n\nDespite the idealistic vision that the internet would support sustainable \"knowledge societies,\" opportunities for participation in open access publishing remain unequally distributed across the globe. Multiple barriers reinforce the global North-South imbalance, including restricted internet access, limited digital literacy, and the primacy of English-language publishing. Additionally, the costs associated with making research openly available create substantial inequities within and across borders. <Paper corpusId=\"260715586\" paperTitle=\"(Arthur et al., 2023)\" isShortName></Paper> <Paper corpusId=\"240532032\" paperTitle=\"(Arthur et al., 2021)\" isShortName></Paper>\n\nThese challenges highlight the complex interplay between technical, financial, and social factors in the open access ecosystem. While open access has made significant progress in democratizing access to knowledge, addressing these limitations requires coordinated efforts that consider the diverse needs of scholarly communities worldwide, particularly those in traditionally marginalized regions. The 2005 Salvador Declaration on Open Access recognized this need, highlighting the importance of large-scale collaborative partnerships for more equitable access to information, especially among developing countries. <Paper corpusId=\"260715586\" paperTitle=\"(Arthur et al., 2023)\" isShortName></Paper> However, nearly two decades after foundational initiatives like the Budapest Open Access Initiative (2002), significant work remains to achieve the movement's full potential for global equity in knowledge production and dissemination.",
          "citations": [
            {
              "id": "(Arias, 2018)",
              "snippets": [
                "Open Access has matured for journals, but its uptake in the book market is still delayed, despite the fact that books continue to be the leading publishing format for social sciences and humanities."
              ],
              "metadata": null
            },
            {
              "id": "(Schrader et al., 2018)",
              "snippets": [
                "Across the world, there is a growing interest in Open Access (OA) publishing. Therefore, OA publishing has become a trend and is of key importance to the scientific community. However, observing the publication landscape in Germany leads to a striking finding of very different approaches. In particular, OA book publishing is still in relatively early stages, leading to OA books being much less frequently published than OA journal articles. However, although well-established publishers offer the publication of OA books, only certain researchers can actually publish, because of high Book Processing Charges (BPCs). In contrast to such publishers, university presses publish books as OA without any or at significantly lower charges; however, university presses are often inadequately staffed and do not have the technical know-how of the state-of-the-art publishing of OA books possessed by well-established publishers."
              ],
              "metadata": null
            },
            {
              "id": "(Collective et al., 2021)",
              "snippets": [
                "We note this here not to propose an idealized workflow for journal management, but rather to highlight the insufficiency of any one tool or platform to effectively support the entire spectrum of ESTS activities. This is especially true of working in an open source environment: despite the emergence of new technologies, business models, and infrastructural players resulting from vigorous debates on the social, technical, and economic conditions of academic research publication practice over the past twenty years, 1 OA today still comes with significant hurdles and requires investment of resources often beyond what academic organizations can commit (see Eve and Gray (2020) for more on the latest developments in OA scholarly publishing)",
                "we explored alternative open source platforms for hosting ESTS, but in the end concluded that while other platforms (PubPub especially) offered better future-oriented features and contemporary web-functions, OJS was still a more stable and well-documented platform for providing standard journal production functions. OJS's more conventional imagination of scholarly journals, however, has meant that it is less infrastructured for supporting the production of content in new formats (HTML in addition to PDF, for example, as we have implemented starting this issue) and leveraging the experimental possibilities of virtual space for a digital only journal like ESTS."
              ],
              "metadata": null
            },
            {
              "id": "(Arthur et al., 2023)",
              "snippets": [
                "The open scholarship movement has sought to make scholarly communication and research results freely available through the support of a nonprofit ecosystem whose cooperative work promotes a more sustainable, inclusive, and participatory \"knowledge commons\" (Albagli et al., 2018)[14](Hynek, 2008)(Rhoten et al., 2007)[17]. Accordingly, since its emergence at the start of the 21st century, many statements have advocated for greater equity in access to publications, open data, copyright revision, open educational resources, open government data, alternative metrics and assessment, and responsive research and development, with some calling for the implementation of Findable, Accessible, Interoperable and Reusable (FAIR) practices to facilitate greater public access, collaboration, and multi-stakeholder and citizen engagement (Hampson, 2020)(Crick, 2019)[20][21](Wilsdon et al., 2015). For example, the 2005 Salvador Declaration on Open Access highlighted the importance of open access in promoting equity, envisaging outcomes commensurate with the United Nations' Millennium Development Goals, including large-scale collaborative partnerships for more equitable access to information, especially among developing countries. \n\nYet, two decades after the foundational Budapest Open Access Initiative (2002) was first drafted and optimism that the Internet would assist in developing and supporting sustainable \"knowledge societies,\" the opportunities are unequally distributed [23]. Despite drives for greater equity of access to and engagement with education through networked platforms and repositories to bridge social divides (Beaulieu et al., 2018), restricted Internet access, limited digital literacy, the primacy of English language publishing, and crucially, the cost of making research openly available, have become barriers reinforcing the global North-South imbalance and inequities within and across borders (Arthur et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Arthur et al., 2021)",
              "snippets": [
                "Open publishing services-such as the Open Library of Humanities, Open Book Publishers, Open Humanities Press, OpenEdition, Knowledge Unlatched, Ubiquity Press, and Language Science Press-are developing new approaches to assist in covering the APC and BPC costs of open access. Open Humanities Press is sustained in partnership with the University of Michigan Library and charges no article processing fees, while the Open Library of Humanities has developed a model of library partnership aimed at overcoming the current situation where academics produce publications that are given sometimes at a cost to publishers, who then sell these publications back to the academic libraries. Others, like Knowledge Unlatched, use membership models and the pooling of library consortia (Eve, 2017). While Knowledge Unlatched is the largest initiative in the humanities, it has recently changed from being a British not-for-profit company to a German-based private for-profit company, yet it affirms its goal to become the \"central open access platform,\" with emphasis on transparency from publishers regarding data usage  (Ross-Hellauer et al., 2018)."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How have open-source publishing platforms impacted the global distribution of academic publishing?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should provide a brief overview of open access and name arXiv as one of the most famous platforms for that.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Open Access (OA) refers to the removal of major obstacles to accessing, sharing and re-using the outputs of scholarly research.",
              "Open access (OA) publishing has dramatically altered the landscape of academic publishing, making research outputs freely available to readers worldwide (49, Bar-Anan et al., 2012). This model, which funds publishing through publication fees rather than reader subscriptions, has emerged as a cost-effective option for disseminating scholarly research  (21, Johnson, 2005).",
              "The best known of these is arXiv, in which researchers during its eleven year old history have deposited 225,000 publications from the fields of physics, mathematics, and information technology (arXiv)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention some advantages of open access platforms that can affect the publication landscape, such as increased accessibility and reach, and localized publishing.",
            "weight": 0.19999999999999998,
            "evidence": [
              "### **Increased Accessibility and Reach**1. **Global Participation**: Open-source platforms have enabled researchers from diverse backgrounds and regions to publish their work, increasing global participation in academic publishing. For instance, the Public Knowledge Project's (PKP) open-source software has facilitated the publication of over 1.46 million articles in 60 languages from more than 146 countries, with a significant proportion from the Global South.2. **Language Diversity**: These platforms support multilingual publishing, allowing researchers to publish in their native languages, thereby promoting linguistic diversity and inclusivity. This is evident in the use of PKP's software, which supports publications in multiple languages.",
              "OA publishing breaks traditional financial barriers and allows unrestricted, equal access to scholarly information to people all over the globe. Due to the high prices of journal subscriptions, developing countries struggle with access just as in developed countries, but to a greater extent and consequently with greater negative repercussions.",
              "### **Bibliodiversity and Localized Publishing**1. **Decentralization**: Open-source platforms have decentralized publishing, allowing local institutions and communities to establish their own journals and publishing initiatives. This has promoted bibliodiversity, as local voices and perspectives are now more easily represented in the global scholarly landscape."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should argue that despite these advantages, OA can not guarantee even distribution of publications due to the author-pays model. In addition, the author-pays model can decrease the quality of publications.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Critics argue that the author-pays model may incentivize quantity over quality and create new barriers for researchers from less-funded institutions or developing countries."
            ]
          }
        ]
      }
    }
  },
  {
    "question": "What are leading methods for generating hard examples of the boolean satisfiability problem, and what are their strengths and weaknesses?",
    "response": {
      "sections": [
        {
          "title": "Introduction and Background",
          "text": "\nThe Boolean satisfiability problem (SAT) asks whether there exists an assignment of truth values to variables in a Boolean formula that makes the entire formula evaluate to true. As the first problem proven to be NP-complete, SAT has theoretical significance and practical applications in areas such as hardware verification, planning, and combinatorial optimization. Despite its worst-case complexity, modern SAT solvers can efficiently handle many large instances arising in practice. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThis gap between theoretical hardness and practical performance has motivated researchers to develop methods for generating genuinely difficult SAT instances. Hard instances serve several purposes: they help benchmark and compare solver performance, reveal algorithmic weaknesses, and guide solver development. The difficulty of a SAT instance is typically measured by the time or computational resources required to solve it, or by specific structural properties known to challenge certain solving techniques. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nOver decades of research, various approaches to generating hard SAT instances have emerged. These range from random generation with specific parameters to carefully crafted instances with hidden structures, as well as methods that transform easy problems into harder ones. Each generation method produces instances with different characteristics, making them suitable for testing different aspects of solver performance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe evolution of SAT instance generation has closely followed developments in solving technology. As solvers improved, generation methods adapted to create new challenges. This constant interplay between increasingly sophisticated solving techniques and more challenging benchmarks continues to drive progress in the field. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
          "citations": []
        },
        {
          "title": "Random SAT Generation Methods",
          "text": "\nRandom generation approaches have long been a cornerstone of creating challenging SAT instances for evaluating solver performance. The most fundamental method involves generating random k-SAT formulas where the clause-to-variable ratio is carefully controlled to produce difficult instances. This approach leverages the well-known phase transition phenomenon, where problems transition from being mostly satisfiable to mostly unsatisfiable at a critical ratio, with the hardest instances concentrated around this threshold <Paper corpusId=\"14897453\" paperTitle=\"(Simons et al., 2000)\" isShortName></Paper> <Paper corpusId=\"2729521\" paperTitle=\"(Xu et al., 2003)\" isShortName></Paper>.\n\nA significant challenge in random SAT generation is creating instances that are both hard and guaranteed satisfiable, which is crucial for evaluating incomplete search algorithms. One intuitive approach involves first generating a random truth assignment and then only accepting random clauses that are satisfied by this assignment. However, this \"forced satisfiability\" method has been shown to produce instances that are too easy because they typically have many satisfying assignments clustered around the chosen truth assignment <Paper corpusId=\"2729521\" paperTitle=\"(Xu et al., 2003)\" isShortName></Paper>.\n\nTo address these limitations, researchers have developed more sophisticated random generation methods. The controlled and multi-component models extend the fixed clause length model for k-SAT, allowing for greater control over instance hardness. These approaches enable the generation of \"super-hard\" instances by varying not only the clause-to-variable ratio but also the number of components, with experiments showing that even a small number of components can lead to extremely challenging problems <Paper corpusId=\"3609500\" paperTitle=\"(Amendola et al., 2018)\" isShortName></Paper>.\n\nAnother approach to generating hard satisfiable instances involves \"hiding\" solutions in a way that balances the influence of the hidden assignment. For example, some methods hide both an assignment and its complement to cancel the bias toward the hidden solution <Paper corpusId=\"674460\" paperTitle=\"(Achlioptas et al., 2004)\" isShortName></Paper> <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper> <Paper corpusId=\"1023\" paperTitle=\"(Jia et al., 2005)\" isShortName></Paper>. More sophisticated generators tune the probability of selecting clauses based on how many literals they satisfy, allowing creators to generate formulas with no bias or even deceptively pointing away from the hidden assignment <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper> <Paper corpusId=\"16823134\" paperTitle=\"(Barthel et al., 2001)\" isShortName></Paper>.\n\nAlternative methods for generating hard satisfiable instances include techniques based on quasigroup completion problems, which provide more structured benchmarks. However, these approaches often require a large number of variables in their clausal encoding to create instances that challenge modern SAT solvers <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper>. Similarly, regular random k-SAT and other restricted models have been developed to generate empirically hard instances with predictable properties <Paper corpusId=\"451\" paperTitle=\"(Xu et al., 2005)\" isShortName></Paper>.\n\nThe development of these random generation methods has been essential for creating benchmark instances that can reliably test SAT solver performance, both for complete solvers that determine satisfiability with certainty and for incomplete stochastic algorithms that search for satisfying assignments <Paper corpusId=\"3609500\" paperTitle=\"(Amendola et al., 2018)\" isShortName></Paper> <Paper corpusId=\"3037467\" paperTitle=\"(Narizzano et al., 2006)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Simons et al., 2000)",
              "snippets": [
                "The problems are chosen such that the clause to atom ratio is 4.258 + 58.26a \u22125/3 , where a is the number of atoms, since this particular ratio determines a region of hard satisfiability problems (Crawford et al., 1996)."
              ],
              "metadata": null
            },
            {
              "id": "(Xu et al., 2003)",
              "snippets": [
                "As mentioned before, the finding of phase transitions in NP-complete problems provides a good method for generating random hard instances which are very useful in the evaluation of algorithms",
                "To evaluate the efficiency of such incomplete algorithms, we need a source to generate only hard satisfiable instances [3].However, since the probability of being satisfiable is about 0.5 at the threshold point where the hardest instances are concentrated, the generator based on phase transitions will usually produce a mixture of satisfiable and unsatisfiable instances.\n\nIn fact, for constraint satisfaction and Boolean satisfiability problems, there is a natural strategy to generate instances that are guaranteed to have at least one satisfying assignment.The strategy is as follows [3]: first generate a random truth assignment t, and then generate a certain number of random constraints or clauses one by one to form a random instance, where any clause or constraint violating t will be rejected",
                "But unfortunately, this strategy was proved to be unsuitable for random 3-SAT because it in fact produces a biased sampling of instances with many satisfying assignments (clustered around t), and experiments also show that these instances are much easier to solve than random satisfiable instances [3].In the following, for convenience, we will call the satisfiable instances generated using the strategy as forced satisfiable instances.\n\nAlthough there have been some other ways to generate hard satisfiable instances empirically, e.g. the quasigroup method [3], we think that the simple and natural method presented in this paper, based on models (i.e.Model RB/RD) with exact phase transitions and many hard instances, should be well worth further investigation."
              ],
              "metadata": null
            },
            {
              "id": "(Amendola et al., 2018)",
              "snippets": [
                "Models of random propositional formulas and QBFs that can reliably generate large numbers of instances of a desired hardness are important (Gent et al., 1999). Inherently hard instances for SAT and QBF solvers are essential for designing and testing search methods employed by solvers (Achlioptas, 2009), and are used to assess their performance in solver competitions [27](Narizzano et al., 2006)(Calimeri et al., 2016). On the flip side, large collections of easy instances support the so-called fuzz testing, used to reveal problems in solver implementation, as well as defects in solver design (Brummayer et al., 2010)",
                ".In this paper we proposed the controlled and multi-component models for random propositional formulas, and disjunctive logic programs. The models extend the wellknown fixed clause length model for k-SAT, and the Chen-Interian model for QBF",
                ".Comparing models, we observed that the controlled model allows one to generate random instances that are much harder than those obtained with the Chen-Interian results in the generation of programs and formulas that are \"super-hard\" to evaluate",
                ".First, the hardness of formulas/programs can be controlled and, unlike in the earlier models, not only in terms of the ratio of clauses to variables. Our experiments showed that the hardness strongly depends on the number of components. Thus, it can also be controlled by varying that parameter, and even a small number of components can lead to extremely hard instances."
              ],
              "metadata": null
            },
            {
              "id": "(Achlioptas et al., 2004)",
              "snippets": [
                "Perhaps the most natural candidate for generating random hard satisfiable 3-SAT formulas is the following."
              ],
              "metadata": null
            },
            {
              "id": "(Haanpaa et al., 2006)",
              "snippets": [
                "Among the well-known sources of empirically hard instances are the random k-SAT (Crawford et al., 1996) model and its restrictions such as regular random k-SAT (Boufkhad et al., 2005). However, the satisfiability of random k-SAT instances cannot be determined efficiently beforehand. This problem has been addressed by \"hiding\" solutions by generating only clauses that are satisfied by truth assignments selected beforehand [2](Barthel et al., 2001)(Jia et al., 2005). More structured satisfiable benchmark instances have been developed based on, for example, quasigroup completion (Achlioptas et al., 2000)(Kautz et al., 2001). However, a large number of variables is required in the clausal encoding of quasigroup completion to obtain a challenging instance for state-of-the-art SAT solvers."
              ],
              "metadata": null
            },
            {
              "id": "(Jia et al., 2005)",
              "snippets": [
                "To test incomplete search algorithms for constraint satisfaction problems such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisfied by A. However, this method tends to produce easy problems, since the majority of literals point toward the \"hidden\" assignment A. Last year, (Achlioptas, Jia, & Moore 2004) proposed a problem generator that cancels this effect by hiding both A and its complement A. While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time. \n \nHere we propose a new method to cancel the attraction to A, by choosing a clause with t > 0 literals satisfied by A with probability proportional to qt for some q < 1. By varying q, we can generate formulas whose variables have no bias, i.e., which are equally likely to be true or false; we can even cause the formula to \"deceptively\" point away from A. We present theoretical and experimental results suggesting that these formulas are exponentially hard both for DPLL algorithms and for incomplete algorithms such as WalkSAT."
              ],
              "metadata": null
            },
            {
              "id": "(Barthel et al., 2001)",
              "snippets": [
                "A major problem in evaluating stochastic local search algorithms for NP-complete problems is the need for a systematic generation of hard test instances having previously known properties of the optimal solutions. On the basis of statistical mechanics results, we propose random generators of hard and satisfiable instances for the 3-satisfiability problem. The design of the hardest problem instances is based on the existence of a first order ferromagnetic phase transition and the glassy nature of excited states. The analytical predictions are corroborated by numerical results obtained from complete as well as stochastic local algorithms."
              ],
              "metadata": null
            },
            {
              "id": "(Xu et al., 2005)",
              "snippets": [
                "Although there are some other ways to generate hard satisfiable instances, e.g. QWH (Frieze et al., 2006) or 2hidden (Frieze et al., 2006)] instances, we think that the simple and natural method presented in this paper, based on models with exact phase transitions and many hard instances, should be well worth further investigation."
              ],
              "metadata": null
            },
            {
              "id": "(Narizzano et al., 2006)",
              "snippets": [
                "This paper reports about the 2005 comparative evaluation of solvers for quantified Boolean formulas (QBFs), the third in a series of non-competitive events established with the aim of assessing the advancements in the field of QBF reasoning and related research.\u00a0 We evaluated thirteen solvers on a test set of more than three thousands QBFs, selected from instances submitted to the evaluation and from those available at www.qbflib.org.\u00a0 In the paper we present the evaluation infrastructure, from the criteria used to assemble the test set to the hardware set up, and we show different views about the results obtained, highlighting the strength of different solvers and the relative hardness of the instances included in the test set."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Solution-Guided Generation Methods",
          "text": "\nSolution-guided generation methods address a fundamental challenge in SAT research: creating instances that are both guaranteed to be satisfiable and sufficiently difficult to solve. This capability is particularly important for evaluating incomplete search algorithms that require known-satisfiable benchmarks <Paper corpusId=\"2729521\" paperTitle=\"(Xu et al., 2003)\" isShortName></Paper>.\n\nThe most straightforward approach, often called \"forced satisfiability,\" involves first generating a random truth assignment and then only accepting random clauses that are satisfied by this assignment. While this ensures satisfiability, it unfortunately produces instances that are too easy for modern solvers. This occurs because the generated instances tend to have many satisfying assignments clustered around the chosen truth assignment, creating a problem landscape that solvers can navigate relatively easily <Paper corpusId=\"2729521\" paperTitle=\"(Xu et al., 2003)\" isShortName></Paper>.\n\nMore sophisticated solution-guided methods have been developed to overcome this limitation. One approach involves \"hiding\" solutions in a way that balances their influence. For example, some techniques hide both an assignment and its complement simultaneously to cancel out the bias toward the hidden solution <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper> <Paper corpusId=\"1023\" paperTitle=\"(Jia et al., 2005)\" isShortName></Paper>. This balancing act creates instances that remain challenging for deterministic DPLL-based algorithms, though they may still be solvable in polynomial time by stochastic methods like WalkSAT <Paper corpusId=\"1023\" paperTitle=\"(Jia et al., 2005)\" isShortName></Paper>.\n\nFurther refinements allow even more precise control over instance difficulty. One notable technique adjusts the probability of selecting clauses based on how many literals they satisfy, using a parameter q < 1 to control this probability. By varying q, researchers can generate formulas whose variables have no bias or even create \"deceptive\" instances that point solvers away from the hidden assignment <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper> <Paper corpusId=\"1023\" paperTitle=\"(Jia et al., 2005)\" isShortName></Paper>. The hardest instances emerge from specific parameter settings that create a challenging search landscape where the hidden solution is neither too obvious nor completely obscured <Paper corpusId=\"16823134\" paperTitle=\"(Barthel et al., 2001)\" isShortName></Paper>.\n\nRecent advances in solution-guided generation have incorporated additional structural elements. For example, some generators now consider community structures when creating SAT formulas with predefined solutions. These methods control the quality of community structures by managing both \"intra-community clauses\" (clauses whose variables belong to a common community) and \"intra-community variables\" (variables belonging to only one community) <Paper corpusId=\"67855661\" paperTitle=\"(Hu et al., 2019)\" isShortName></Paper>. This approach allows for the creation of instances with varying levels of modularity, which can significantly impact solver performance.\n\nWhile solution-guided methods have evolved considerably, researchers continue to explore alternatives such as quasigroup completion problems for generating more structured satisfiable benchmarks. However, these approaches often require a large number of variables to create instances that challenge state-of-the-art solvers <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper>. The search for optimal ways to generate hard satisfiable instances remains an active area of research, with methods based on models with exact phase transitions showing particular promise <Paper corpusId=\"451\" paperTitle=\"(Xu et al., 2005)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Xu et al., 2003)",
              "snippets": [
                "As mentioned before, the finding of phase transitions in NP-complete problems provides a good method for generating random hard instances which are very useful in the evaluation of algorithms",
                "To evaluate the efficiency of such incomplete algorithms, we need a source to generate only hard satisfiable instances [3].However, since the probability of being satisfiable is about 0.5 at the threshold point where the hardest instances are concentrated, the generator based on phase transitions will usually produce a mixture of satisfiable and unsatisfiable instances.\n\nIn fact, for constraint satisfaction and Boolean satisfiability problems, there is a natural strategy to generate instances that are guaranteed to have at least one satisfying assignment.The strategy is as follows [3]: first generate a random truth assignment t, and then generate a certain number of random constraints or clauses one by one to form a random instance, where any clause or constraint violating t will be rejected",
                "But unfortunately, this strategy was proved to be unsuitable for random 3-SAT because it in fact produces a biased sampling of instances with many satisfying assignments (clustered around t), and experiments also show that these instances are much easier to solve than random satisfiable instances [3].In the following, for convenience, we will call the satisfiable instances generated using the strategy as forced satisfiable instances.\n\nAlthough there have been some other ways to generate hard satisfiable instances empirically, e.g. the quasigroup method [3], we think that the simple and natural method presented in this paper, based on models (i.e.Model RB/RD) with exact phase transitions and many hard instances, should be well worth further investigation."
              ],
              "metadata": null
            },
            {
              "id": "(Haanpaa et al., 2006)",
              "snippets": [
                "Among the well-known sources of empirically hard instances are the random k-SAT (Crawford et al., 1996) model and its restrictions such as regular random k-SAT (Boufkhad et al., 2005). However, the satisfiability of random k-SAT instances cannot be determined efficiently beforehand. This problem has been addressed by \"hiding\" solutions by generating only clauses that are satisfied by truth assignments selected beforehand [2](Barthel et al., 2001)(Jia et al., 2005). More structured satisfiable benchmark instances have been developed based on, for example, quasigroup completion (Achlioptas et al., 2000)(Kautz et al., 2001). However, a large number of variables is required in the clausal encoding of quasigroup completion to obtain a challenging instance for state-of-the-art SAT solvers."
              ],
              "metadata": null
            },
            {
              "id": "(Jia et al., 2005)",
              "snippets": [
                "To test incomplete search algorithms for constraint satisfaction problems such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisfied by A. However, this method tends to produce easy problems, since the majority of literals point toward the \"hidden\" assignment A. Last year, (Achlioptas, Jia, & Moore 2004) proposed a problem generator that cancels this effect by hiding both A and its complement A. While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time. \n \nHere we propose a new method to cancel the attraction to A, by choosing a clause with t > 0 literals satisfied by A with probability proportional to qt for some q < 1. By varying q, we can generate formulas whose variables have no bias, i.e., which are equally likely to be true or false; we can even cause the formula to \"deceptively\" point away from A. We present theoretical and experimental results suggesting that these formulas are exponentially hard both for DPLL algorithms and for incomplete algorithms such as WalkSAT."
              ],
              "metadata": null
            },
            {
              "id": "(Barthel et al., 2001)",
              "snippets": [
                "A major problem in evaluating stochastic local search algorithms for NP-complete problems is the need for a systematic generation of hard test instances having previously known properties of the optimal solutions. On the basis of statistical mechanics results, we propose random generators of hard and satisfiable instances for the 3-satisfiability problem. The design of the hardest problem instances is based on the existence of a first order ferromagnetic phase transition and the glassy nature of excited states. The analytical predictions are corroborated by numerical results obtained from complete as well as stochastic local algorithms."
              ],
              "metadata": null
            },
            {
              "id": "(Hu et al., 2019)",
              "snippets": [
                "Although some generating algorithms have been proposed to generate SAT formulas with predefined solutions, community structures of SAT formulas are not considered. We propose a 3-SAT formula generating algorithm that not only guarantees the existence of a predefined solution, but also simultaneously considers community structures and clause distributions. The proposed 3-SAT formula generating algorithm controls the quality of community structures through controlling (1) the number of clauses whose variables have a common community, which we call intra-community clauses, and (2) the number of variables that only belong to one community, which we call intra-community variables."
              ],
              "metadata": null
            },
            {
              "id": "(Xu et al., 2005)",
              "snippets": [
                "Although there are some other ways to generate hard satisfiable instances, e.g. QWH (Frieze et al., 2006) or 2hidden (Frieze et al., 2006)] instances, we think that the simple and natural method presented in this paper, based on models with exact phase transitions and many hard instances, should be well worth further investigation."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Structure-Based Generation Methods",
          "text": "\nStructure-based generation methods focus on creating SAT instances with specific structural characteristics that make them challenging for solvers. Unlike random generation approaches, these methods deliberately engineer instances with properties often found in industrial or real-world problems. One key approach involves controlling community structures within SAT formulas. Researchers have developed algorithms that not only guarantee the existence of predefined solutions but also carefully manage community structures by controlling both \"intra-community clauses\" (clauses whose variables share a common community) and \"intra-community variables\" (variables belonging to only one community) <Paper corpusId=\"67855661\" paperTitle=\"(Hu et al., 2019)\" isShortName></Paper>.\n\nFor generating instances that resemble industrial problems, several methods have been developed to capture their unique structural characteristics. SATGEN and G2SAT are notable examples that attempt to generate pseudo-industrial instances by following particular structural patterns and problem scales. SATGEN employs an unsupervised generative model to implicitly capture intrinsic features of industrial instances, while G2SAT uses node-merging and splitting algorithms to generate bipartite graphs that represent structured SAT problems <Paper corpusId=\"247318637\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>. These hand-crafted formula generation methods typically identify one or two structural metrics and then implement algorithms capable of controlling these metrics to match properties observed in industrial benchmarks <Paper corpusId=\"256615981\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nA particularly effective structure-based approach involves hiding cardinality constraints within SAT instances. This technique has produced some of the most challenging problems in major SAT competitions. The sgen6 algorithm, for example, can generate instances with fewer than a thousand literals that no solver can solve in under 10 minutes <Paper corpusId=\"72941052\" paperTitle=\"(Escamocher et al., 2019)\" isShortName></Paper> <Paper corpusId=\"2801416\" paperTitle=\"(Jarvisalo et al., 2012)\" isShortName></Paper>. While these instances are extremely difficult when first encountered, they can often be solved efficiently once their structural properties are recognized and specific solution techniques are applied.\n\nAnother structure-based concept is the Minimally Unsatisfiable Subset (MUS) or Core. For unsatisfiable instances, an MUS represents the smallest possible subset of clauses that remains unsatisfiable. Understanding and manipulating these core structures allows for the creation of instances that are particularly challenging for solvers to prove unsatisfiable <Paper corpusId=\"272969151\" paperTitle=\"(Cotnareanu et al., 2024)\" isShortName></Paper>. By carefully constructing problems around these minimal cores, generators can create instances where solvers must explore large portions of the search space before determining unsatisfiability.",
          "citations": [
            {
              "id": "(Hu et al., 2019)",
              "snippets": [
                "Although some generating algorithms have been proposed to generate SAT formulas with predefined solutions, community structures of SAT formulas are not considered. We propose a 3-SAT formula generating algorithm that not only guarantees the existence of a predefined solution, but also simultaneously considers community structures and clause distributions. The proposed 3-SAT formula generating algorithm controls the quality of community structures through controlling (1) the number of clauses whose variables have a common community, which we call intra-community clauses, and (2) the number of variables that only belong to one community, which we call intra-community variables."
              ],
              "metadata": null
            },
            {
              "id": "(Guo et al., 2022)",
              "snippets": [
                "SAT instance generation aims to provide sufficient representative training/test samples in industrial scenarios. Different from the generation of random and combinatorial instances, generating pseudo-industrial instances requires following certain structural characteristics, as well as the problem scale. Typical examples of pseudo-industrial SAT instance generation include SATGEN (Zhang et al., 2021) and G2SAT (Zhang et al., 2021), where SATGEN uses an unsupervised generative model that implicitly portrays the intrinsic features, and G2SAT proposes a node-merging (-splitting) algorithm to generate bipartite graphs from (to) a forest."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al., 2023)",
              "snippets": [
                "Hand-crafted formula generation methods [21]22] first identify one or two structural metrics and then specify an algorithm capable of controlling these metrics to match the industrial benchmarks."
              ],
              "metadata": null
            },
            {
              "id": "(Escamocher et al., 2019)",
              "snippets": [
                "In major SAT competitions (J\u00e4rvisalo et al., 2012), the smallest instances that cannot be solved within the time limit are obtained by hiding cardinality constraints. They offer a great deal of difficulty to solvers when presented for the first time, but are usually exploitable by a particular trick when identified. As an illustration, the sgen6 algorithm is able to generate instances with less than a thousand literals that no solver can solve in less than 10 minutes (Spence, 2015), but these instances can be reduced to a simple matching problem [8]."
              ],
              "metadata": null
            },
            {
              "id": "(Jarvisalo et al., 2012)",
              "snippets": [
                "The International SAT Solver Competition is today an established series of competitive events aiming at objectively evaluating the progress in state-of-the-art procedures for solving Boolean satisfiability (SAT) instances. Over the years, the competitions have significantly contributed to the fast progress in SAT solver technology that has made SAT a practical success story of computer science. This short article provides an overview of the SAT solver competitions."
              ],
              "metadata": null
            },
            {
              "id": "(Cotnareanu et al., 2024)",
              "snippets": [
                "Core Definition Given an unsatisfiable (UNSAT) instance U, there is a subset of clauses called a Minimally Unsatisfiable Subset (MUS) or a Core. This subset is the smallest possible subset of clauses from U that is UNSAT (Biere et al., 2009, Chapter 11)."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Transformation-Based Methods",
          "text": "\nTransformation-based methods take a fundamentally different approach to generating hard SAT instances by converting problems from other domains known to be computationally difficult. One prominent example is the transformation of integer factorization problems into SAT instances. Researchers have developed algorithms that efficiently transform integer factorization challenges into Boolean satisfiability problems using the Chinese Remainder Theorem. This approach can produce remarkably compact yet extremely difficult instances\u2014for example, SAT formulas with approximately 5,600 variables that are as challenging as factoring 100-bit integers <Paper corpusId=\"15965809\" paperTitle=\"(Horie et al., 1998)\" isShortName></Paper>.\n\nThe appeal of transformation-based methods lies in their ability to leverage the established computational hardness of well-studied problems in other domains. Unlike purely random generation approaches where the difficulty cannot be precisely controlled, transformed instances inherit the complexity characteristics of their source problems. This makes them valuable for creating benchmarks with predictable difficulty levels.\n\nThese transformation approaches complement other generation techniques by providing a different class of hard problems. While random k-SAT models and their variants generate challenging instances, their satisfiability cannot be efficiently determined beforehand <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper>. Similarly, methods that \"hide\" solutions by generating only clauses satisfied by predetermined truth assignments address the need for known-satisfiable instances but may not always produce sufficiently challenging problems <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper> <Paper corpusId=\"1023\" paperTitle=\"(Jia et al., 2005)\" isShortName></Paper> <Paper corpusId=\"16823134\" paperTitle=\"(Barthel et al., 2001)\" isShortName></Paper>.\n\nThe transformation approach offers a middle ground\u2014instances with known properties derived from the original problem but with difficulty controlled by the inherent complexity of that source problem. While structured satisfiable benchmarks like quasigroup completion problems provide alternatives, they typically require a large number of variables to create instances that challenge modern SAT solvers <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper>. In contrast, transformation-based methods can often generate more compact yet still challenging instances.\n\nA key advantage of transformation-based methods is that they often produce instances with structural characteristics that differ significantly from both random and hand-crafted instances. This diversity makes them particularly valuable for comprehensive solver evaluation, as they can expose weaknesses in solution strategies that might perform well on other types of instances but fail on these transformed problems.",
          "citations": [
            {
              "id": "(Horie et al., 1998)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Haanpaa et al., 2006)",
              "snippets": [
                "Among the well-known sources of empirically hard instances are the random k-SAT (Crawford et al., 1996) model and its restrictions such as regular random k-SAT (Boufkhad et al., 2005). However, the satisfiability of random k-SAT instances cannot be determined efficiently beforehand. This problem has been addressed by \"hiding\" solutions by generating only clauses that are satisfied by truth assignments selected beforehand [2](Barthel et al., 2001)(Jia et al., 2005). More structured satisfiable benchmark instances have been developed based on, for example, quasigroup completion (Achlioptas et al., 2000)(Kautz et al., 2001). However, a large number of variables is required in the clausal encoding of quasigroup completion to obtain a challenging instance for state-of-the-art SAT solvers."
              ],
              "metadata": null
            },
            {
              "id": "(Jia et al., 2005)",
              "snippets": [
                "To test incomplete search algorithms for constraint satisfaction problems such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisfied by A. However, this method tends to produce easy problems, since the majority of literals point toward the \"hidden\" assignment A. Last year, (Achlioptas, Jia, & Moore 2004) proposed a problem generator that cancels this effect by hiding both A and its complement A. While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time. \n \nHere we propose a new method to cancel the attraction to A, by choosing a clause with t > 0 literals satisfied by A with probability proportional to qt for some q < 1. By varying q, we can generate formulas whose variables have no bias, i.e., which are equally likely to be true or false; we can even cause the formula to \"deceptively\" point away from A. We present theoretical and experimental results suggesting that these formulas are exponentially hard both for DPLL algorithms and for incomplete algorithms such as WalkSAT."
              ],
              "metadata": null
            },
            {
              "id": "(Barthel et al., 2001)",
              "snippets": [
                "A major problem in evaluating stochastic local search algorithms for NP-complete problems is the need for a systematic generation of hard test instances having previously known properties of the optimal solutions. On the basis of statistical mechanics results, we propose random generators of hard and satisfiable instances for the 3-satisfiability problem. The design of the hardest problem instances is based on the existence of a first order ferromagnetic phase transition and the glassy nature of excited states. The analytical predictions are corroborated by numerical results obtained from complete as well as stochastic local algorithms."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Comparative Analysis: Strengths and Weaknesses",
          "text": "\nWhen comparing the various approaches to generating hard SAT instances, each method demonstrates characteristic strengths and weaknesses that make it suitable for different evaluation scenarios. Random SAT generation methods provide a theoretical foundation with the advantage of statistical predictability around phase transition points, where problems transition from mostly satisfiable to mostly unsatisfiable. This approach reliably produces challenging instances that are valuable for evaluating solver performance <Paper corpusId=\"2729521\" paperTitle=\"(Xu et al., 2003)\" isShortName></Paper>. However, a significant limitation is that at the threshold where the hardest instances are concentrated, these generators produce a mixture of satisfiable and unsatisfiable instances with approximately equal probability, making them unsuitable for exclusively testing satisfiable instances <Paper corpusId=\"2729521\" paperTitle=\"(Xu et al., 2003)\" isShortName></Paper>.\n\nSolution-guided generation methods address this limitation by guaranteeing satisfiability, which is particularly important for evaluating incomplete search algorithms that can only find solutions but cannot prove unsatisfiability. The simplest approach\u2014generating random clauses that don't violate a predetermined truth assignment\u2014unfortunately produces instances that are too easy due to bias toward the hidden solution <Paper corpusId=\"2729521\" paperTitle=\"(Xu et al., 2003)\" isShortName></Paper>. More sophisticated approaches that hide both an assignment and its complement can generate harder instances, but these may still be vulnerable to specific solving techniques. For example, while such \"2hidden\" instances can challenge deterministic DPLL-based algorithms, they might be solvable in polynomial time by stochastic methods like WalkSAT <Paper corpusId=\"451\" paperTitle=\"(Xu et al., 2005)\" isShortName></Paper> <Paper corpusId=\"1023\" paperTitle=\"(Jia et al., 2005)\" isShortName></Paper>.\n\nStructure-based generation methods excel at creating instances that mimic the complexity of real-world problems. Their strength lies in producing SAT instances with specific structural properties that can be precisely controlled and tuned to challenge different aspects of solver performance. Methods that hide cardinality constraints can generate remarkably difficult instances\u2014the sgen6 algorithm can create problems with fewer than a thousand literals that no solver can solve within conventional time limits <Paper corpusId=\"72941052\" paperTitle=\"(Escamocher et al., 2019)\" isShortName></Paper> <Paper corpusId=\"2801416\" paperTitle=\"(Jarvisalo et al., 2012)\" isShortName></Paper>. However, a key weakness is that once their structure is identified, specialized techniques can often exploit these patterns, making them less valuable for long-term benchmarking <Paper corpusId=\"72941052\" paperTitle=\"(Escamocher et al., 2019)\" isShortName></Paper>.\n\nTransformation-based methods offer the advantage of inheriting the computational hardness of well-studied problems from other domains. This leads to instances with predictable difficulty levels and properties derived from the source problem. These instances often have structural characteristics that differ significantly from both random and hand-crafted instances, making them valuable for comprehensive solver evaluation. However, they may lack the fine-grained control over specific structural features that other methods provide.\n\nThe controlled and multi-component models extend traditional random generation by allowing greater control over instance hardness. A significant strength of these approaches is the ability to generate \"super-hard\" instances by varying not just the clause-to-variable ratio but also the number of components <Paper corpusId=\"3609500\" paperTitle=\"(Amendola et al., 2018)\" isShortName></Paper>. This flexibility makes them particularly valuable for both testing solver limits and conducting fuzz testing with easier instances <Paper corpusId=\"3609500\" paperTitle=\"(Amendola et al., 2018)\" isShortName></Paper> <Paper corpusId=\"3037467\" paperTitle=\"(Narizzano et al., 2006)\" isShortName></Paper>.\n\nMore structured satisfiable benchmarks, such as those based on quasigroup completion problems, provide alternatives with guaranteed satisfiability. However, a notable weakness is that they typically require a large number of variables to create instances that challenge modern SAT solvers <Paper corpusId=\"514432\" paperTitle=\"(Haanpaa et al., 2006)\" isShortName></Paper>. This limitation can make them less practical for some evaluation scenarios where compact yet difficult instances are preferable.\n\nEach generation method ultimately serves different purposes in the SAT research ecosystem. Random methods provide theoretical foundations, solution-guided approaches ensure satisfiability, structure-based methods mimic real-world complexity, and transformation-based techniques offer predictable difficulty. The choice between them depends on the specific aspects of solver performance being evaluated and whether the focus is on complete or incomplete solving algorithms <Paper corpusId=\"16823134\" paperTitle=\"(Barthel et al., 2001)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Xu et al., 2003)",
              "snippets": [
                "As mentioned before, the finding of phase transitions in NP-complete problems provides a good method for generating random hard instances which are very useful in the evaluation of algorithms",
                "To evaluate the efficiency of such incomplete algorithms, we need a source to generate only hard satisfiable instances [3].However, since the probability of being satisfiable is about 0.5 at the threshold point where the hardest instances are concentrated, the generator based on phase transitions will usually produce a mixture of satisfiable and unsatisfiable instances.\n\nIn fact, for constraint satisfaction and Boolean satisfiability problems, there is a natural strategy to generate instances that are guaranteed to have at least one satisfying assignment.The strategy is as follows [3]: first generate a random truth assignment t, and then generate a certain number of random constraints or clauses one by one to form a random instance, where any clause or constraint violating t will be rejected",
                "But unfortunately, this strategy was proved to be unsuitable for random 3-SAT because it in fact produces a biased sampling of instances with many satisfying assignments (clustered around t), and experiments also show that these instances are much easier to solve than random satisfiable instances [3].In the following, for convenience, we will call the satisfiable instances generated using the strategy as forced satisfiable instances.\n\nAlthough there have been some other ways to generate hard satisfiable instances empirically, e.g. the quasigroup method [3], we think that the simple and natural method presented in this paper, based on models (i.e.Model RB/RD) with exact phase transitions and many hard instances, should be well worth further investigation."
              ],
              "metadata": null
            },
            {
              "id": "(Xu et al., 2005)",
              "snippets": [
                "Although there are some other ways to generate hard satisfiable instances, e.g. QWH (Frieze et al., 2006) or 2hidden (Frieze et al., 2006)] instances, we think that the simple and natural method presented in this paper, based on models with exact phase transitions and many hard instances, should be well worth further investigation."
              ],
              "metadata": null
            },
            {
              "id": "(Jia et al., 2005)",
              "snippets": [
                "To test incomplete search algorithms for constraint satisfaction problems such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisfied by A. However, this method tends to produce easy problems, since the majority of literals point toward the \"hidden\" assignment A. Last year, (Achlioptas, Jia, & Moore 2004) proposed a problem generator that cancels this effect by hiding both A and its complement A. While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time. \n \nHere we propose a new method to cancel the attraction to A, by choosing a clause with t > 0 literals satisfied by A with probability proportional to qt for some q < 1. By varying q, we can generate formulas whose variables have no bias, i.e., which are equally likely to be true or false; we can even cause the formula to \"deceptively\" point away from A. We present theoretical and experimental results suggesting that these formulas are exponentially hard both for DPLL algorithms and for incomplete algorithms such as WalkSAT."
              ],
              "metadata": null
            },
            {
              "id": "(Escamocher et al., 2019)",
              "snippets": [
                "In major SAT competitions (J\u00e4rvisalo et al., 2012), the smallest instances that cannot be solved within the time limit are obtained by hiding cardinality constraints. They offer a great deal of difficulty to solvers when presented for the first time, but are usually exploitable by a particular trick when identified. As an illustration, the sgen6 algorithm is able to generate instances with less than a thousand literals that no solver can solve in less than 10 minutes (Spence, 2015), but these instances can be reduced to a simple matching problem [8]."
              ],
              "metadata": null
            },
            {
              "id": "(Jarvisalo et al., 2012)",
              "snippets": [
                "The International SAT Solver Competition is today an established series of competitive events aiming at objectively evaluating the progress in state-of-the-art procedures for solving Boolean satisfiability (SAT) instances. Over the years, the competitions have significantly contributed to the fast progress in SAT solver technology that has made SAT a practical success story of computer science. This short article provides an overview of the SAT solver competitions."
              ],
              "metadata": null
            },
            {
              "id": "(Amendola et al., 2018)",
              "snippets": [
                "Models of random propositional formulas and QBFs that can reliably generate large numbers of instances of a desired hardness are important (Gent et al., 1999). Inherently hard instances for SAT and QBF solvers are essential for designing and testing search methods employed by solvers (Achlioptas, 2009), and are used to assess their performance in solver competitions [27](Narizzano et al., 2006)(Calimeri et al., 2016). On the flip side, large collections of easy instances support the so-called fuzz testing, used to reveal problems in solver implementation, as well as defects in solver design (Brummayer et al., 2010)",
                ".In this paper we proposed the controlled and multi-component models for random propositional formulas, and disjunctive logic programs. The models extend the wellknown fixed clause length model for k-SAT, and the Chen-Interian model for QBF",
                ".Comparing models, we observed that the controlled model allows one to generate random instances that are much harder than those obtained with the Chen-Interian results in the generation of programs and formulas that are \"super-hard\" to evaluate",
                ".First, the hardness of formulas/programs can be controlled and, unlike in the earlier models, not only in terms of the ratio of clauses to variables. Our experiments showed that the hardness strongly depends on the number of components. Thus, it can also be controlled by varying that parameter, and even a small number of components can lead to extremely hard instances."
              ],
              "metadata": null
            },
            {
              "id": "(Narizzano et al., 2006)",
              "snippets": [
                "This paper reports about the 2005 comparative evaluation of solvers for quantified Boolean formulas (QBFs), the third in a series of non-competitive events established with the aim of assessing the advancements in the field of QBF reasoning and related research.\u00a0 We evaluated thirteen solvers on a test set of more than three thousands QBFs, selected from instances submitted to the evaluation and from those available at www.qbflib.org.\u00a0 In the paper we present the evaluation infrastructure, from the criteria used to assemble the test set to the hardware set up, and we show different views about the results obtained, highlighting the strength of different solvers and the relative hardness of the instances included in the test set."
              ],
              "metadata": null
            },
            {
              "id": "(Haanpaa et al., 2006)",
              "snippets": [
                "Among the well-known sources of empirically hard instances are the random k-SAT (Crawford et al., 1996) model and its restrictions such as regular random k-SAT (Boufkhad et al., 2005). However, the satisfiability of random k-SAT instances cannot be determined efficiently beforehand. This problem has been addressed by \"hiding\" solutions by generating only clauses that are satisfied by truth assignments selected beforehand [2](Barthel et al., 2001)(Jia et al., 2005). More structured satisfiable benchmark instances have been developed based on, for example, quasigroup completion (Achlioptas et al., 2000)(Kautz et al., 2001). However, a large number of variables is required in the clausal encoding of quasigroup completion to obtain a challenging instance for state-of-the-art SAT solvers."
              ],
              "metadata": null
            },
            {
              "id": "(Barthel et al., 2001)",
              "snippets": [
                "A major problem in evaluating stochastic local search algorithms for NP-complete problems is the need for a systematic generation of hard test instances having previously known properties of the optimal solutions. On the basis of statistical mechanics results, we propose random generators of hard and satisfiable instances for the 3-satisfiability problem. The design of the hardest problem instances is based on the existence of a first order ferromagnetic phase transition and the glassy nature of excited states. The analytical predictions are corroborated by numerical results obtained from complete as well as stochastic local algorithms."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are leading methods for generating hard examples of the boolean satisfiability problem, and what are their strengths and weaknesses?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define the boolean satisfiability problem.",
            "weight": 0.17142857142857143,
            "evidence": [
              "The Boolean satisfiability problem (SAT) is a fundamental NP-complete decision problem in automated reasoning and mathematical logic. As evidenced by the results of SAT competitions, the performance of SAT solvers varies substantially between different SAT categories (random, crafted, and industrial).",
              "What is SAT? Given a propositional logic (Boolean) formula, find a variable assignment such that the formula evaluates to true, or prove that no such assignment exists"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should point that why generating examples of the boolean satisfiability problem is hard",
            "weight": 0.17142857142857143,
            "evidence": [
              "A key limitation of current problem generators concerns their use in the evaluation of incomplete local search methods. This is because the generators generally produce a mixture of solvable (satisfiable) and unsolvable (unsatisfiable) instances. When a local search style method does not find a solution, it can be difficult to determine whether this is because the algorithm fails to find a solution or because the instance itself is unsolvable."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should include a list of methods for generating hard examples of the boolean satisfiability problem with a brief description",
            "weight": 0.17142857142857143,
            "evidence": [
              "## 1. Random k-SAT Generators### DescriptionRandom k-SAT involves generating formulas by randomly selecting clauses, each consisting of k literals, from a given set of variables. The famous 3-SAT variant is particularly well-studied.### Strengths**Parameter Control**: Allows fine-tuning through parameters like the ratio of clauses to variables (r = m/n), which influences the problem's hardness.**Study of Phase Transitions**: Near the critical threshold (~4.26 for 3-SAT), instances of SAT exhibit a sharp jump in difficulty, providing a natural source of hard instances.### Weaknesses**Unpredictability**: While near the critical threshold can yield hard instances, the problem's hardness isn't guaranteed.**Scalability**: Beyond certain sizes and clause lengths, the generation process may become impractically slow.",
              "## 2. Structured SAT Generators### DescriptionStructured SAT generators create instances based on specific combinatorial structures or particularly crafted mathematical properties. Examples include graph-based instances and Grid-based SAT.### Strengths**Controlled Hardness**: More predictable instance difficulty due to embedded structures.**Domain-specific**: Useful for SAT applications in specific fields where structured instances naturally occur, such as circuit design.### Weaknesses**Limited Generalizability**: Hardness might not translate to other problem settings.**Design Complexity**: Creating structured configurations that ensure hardness can be more complex compared to random generation.### Key References",
              "Graph-based methods:- High-girth bipartite incidence graphs: This approach incrementally constructs graphs with high expansion properties, which implies high resolution width and results in hard k-SAT instances (29, Ansotegui et al., 2008).- Eulerian graph transformations: SAT instances based on Eulerian graphs are designed to be challenging for resolution-based SAT solvers  (26, Markstrom, 2006).- Digraph-based generators: Weak models generated from strong digraphs can produce minimal unsatisfiable SAT instances, which are particularly hard to solve (57, Biro et al., 2020).",
              "### 5. Evolutionary AlgorithmsEvolutionary algorithms, such as genetic algorithms, can be used to evolve hard SAT instances. These algorithms iteratively generate and mutate instances based on their hardness. The strength of this method lies in its ability to generate instances that are tailored to the specific strengths and weaknesses of a solver. The weakness is that the generation process can be computationally expensive and may not converge to the hardest instances.",
              "7. Circuit-Based Instance GenerationThis method generates SAT instances by encoding complex Boolean circuits.Method: Design circuits with properties known to be challenging for SAT solvers (e.g., multipliers) and convert them to CNF form.Strengths:- Generates instances with structure similar to real-world verification problems- Difficulty can be controlled by adjusting circuit complexity- Instances have known solutions (the circuit's truth table)Weaknesses:- May require expertise in circuit design- The translation to CNF can impact instance difficulty in unpredictable ways",
              "Markov Chain Monte Carlo (MCMC)-based Methods: A wide variety of MCMC-based algorithms have been proposed in the literature to sample from complex distributions. These include Metropolis algorithm, simulated annealing and the [37, 38, 39]. The core idea of these algorithms is to sample using carefully chosen Markov chains in which the steady state distribution matches the desired distribution. MCMC methods guarantee convergence to uniform distribution only when run for sufficiently long time. Most practical algorithms based on MCMC methods, however, use heuristic adaptations to ensure better performance.",
              "Weighted Binary Decision Diagram (BDD) based Methods: A new approach based on sampling from a set of constraints based on weighted binary decision diagrams [41] was proposed in [42, 43]. The core idea of the algorithm is to construct a BDD-based on the input constraints and then generate uniform samples in a single pass over the BDD. The approach works well for small to medium-sized examples but does not scale to larger problems. Hence it is not scalable to large problems in practice. A detailed analysis of the scalability limitations of BDD-based methods is presented in [35]. An alternative approach to uniform generation based on BDDs was proposed in [44]. This approach relies on constructing an equivalent circuit for BDD constraints [44]. Unfortunately, this approach fails to provide guarantees of uniformity.",
              "Interval-propagation-based sampling: Interval-propagation-based sampling techniques have been used by some researchers to address the scalability challenges posed by uniform generation in practice [45]. The central idea underlying these techniques is to maintain intervals of values that a variable can take and generate samples by performing random sampling over these intervals. The simplicity of such approaches provides good performance in practice but the distributions generated can deviate significantly from the uniform distribution [35].",
              "Belief networks: Another class of methods based on Constraint satisfaction problems (CSP), particularly belief propagation, have been proposed in [46, 47]. The proposed techniques improve on the traditional MCMC based methods by integrating sampling with back-jumping search and no-good learning. Experimental comparisons, however, have shown that these techniques perform poorly compared to MCMC based techniques with random walk and simulated annealing heuristics, as in SampleSAT [47].",
              "Hashing-based techniques Sipser pioneered hashing-based approach in [34] building upon the universal hashing introduced by Carter and Wegman [48]. This has subsequently been used in theoretical [18] and practical [33] treatments of uniform sampling. The key idea in these works is to randomly partition the solution space into \"small cells\" of roughly equal size. The act of picking a solution randomly chosen cell provides the required guarantees.",
              "In this paper, we present empirical results showing that random instances of satisfiability can be generated in such a way that easy and hard sets of instincts for a particular sat procedure, anyway) are predictable in advance",
              "In this paper, we have shown, both theoretically and practically, that the models RB (and RD) can be used to produce, very easily, hard random instances. More importantly, the same result holds for instances that are forced to be satisfiable."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should explain the different between solving (checking) and generating boolean satisfiability problem",
            "weight": 0.08571428571428572,
            "evidence": [
              "Apart from the classical problem of checking Boolean satisfiability, the problems of generating satisfying assignments uniformly at random, and of counting the total number of satisfying assignments have also attracted significant theoretical and practical interest over the years"
            ]
          }
        ]
      }
    }
  },
  {
    "question": "What is unique k-sat and what theoretical results are known about it?",
    "response": {
      "sections": [
        {
          "title": "Introduction to k-SAT",
          "text": "\nThe k-SAT problem is a combinatorial discrete optimization problem involving N Boolean variables subject to M constraints (clauses), where each clause is in the form of an OR logical operator of k literals (variables or their negations) <Paper corpusId=\"10775248\" paperTitle=\"(Marino et al., 2015)\" isShortName></Paper>. Formally, a k-SAT formula is the conjunction of clauses, where each clause is the disjunction of k literals, having the form \u22c0\u1d50\u1d62\u208c\u2081\u22c1\u1d4f\u2c7c\u208c\u2081b\u1d62\u2c7c, where b\u1d62\u2c7c is either a variable x\u2096 or its negation \u00acx\u2096 <Paper corpusId=\"11932153\" paperTitle=\"(Aggarwal et al., 2017)\" isShortName></Paper>.\n\nThe problem is considered solved when there exists at least one configuration of the variables that satisfies all constraints <Paper corpusId=\"10775248\" paperTitle=\"(Marino et al., 2015)\" isShortName></Paper>. For k \u2265 3, k-SAT is NP-complete, making it a central problem in computational complexity theory <Paper corpusId=\"10775248\" paperTitle=\"(Marino et al., 2015)\" isShortName></Paper> <Paper corpusId=\"4920536\" paperTitle=\"(Weaver et al., 2014)\" isShortName></Paper>. The k = 2 case, however, is solvable in polynomial time <Paper corpusId=\"4920536\" paperTitle=\"(Weaver et al., 2014)\" isShortName></Paper>.\n\nRandom k-SAT is a widely studied variant used to investigate the average-case complexity of Boolean satisfiability. In this model, a random formula \u03a6 with n variables, m clauses, and k literals per clause is generated uniformly in conjunctive normal form <Paper corpusId=\"102483745\" paperTitle=\"(Friedrich et al., 2019)\" isShortName></Paper>. The study of random k-SAT has revealed interesting phase transition phenomena related to satisfiability, which has made it a paradigmatic model system in theoretical computer science <Paper corpusId=\"219636110\" paperTitle=\"(Zhao et al., 2020)\" isShortName></Paper>.\n\nOne of the most prominent questions in random k-SAT is the satisfiability threshold conjecture, which states that for a random k-SAT formula, there exists a critical ratio r\u2096 of clauses to variables such that when the ratio m/n is below r\u2096, the formula is almost certainly satisfiable, and when the ratio is above r\u2096, it is almost certainly unsatisfiable <Paper corpusId=\"102483745\" paperTitle=\"(Friedrich et al., 2019)\" isShortName></Paper> <Paper corpusId=\"266741631\" paperTitle=\"(Perkins, 2024)\" isShortName></Paper>. This conjecture has been proven for k = 2, with r\u2082 = 1 <Paper corpusId=\"102483745\" paperTitle=\"(Friedrich et al., 2019)\" isShortName></Paper> <Paper corpusId=\"9954684\" paperTitle=\"(Bollobas et al., 1999)\" isShortName></Paper>.\n\nFor larger values of k, exact threshold values have been established for sufficiently large k by Ding, Sly, and Sun <Paper corpusId=\"102483745\" paperTitle=\"(Friedrich et al., 2019)\" isShortName></Paper> <Paper corpusId=\"1185385\" paperTitle=\"(Ding et al., 2014)\" isShortName></Paper>. For intermediate values of k, upper and lower bounds have been established, with statistical physics methods providing numerical estimates such as r\u2083 \u2248 4.26 <Paper corpusId=\"102483745\" paperTitle=\"(Friedrich et al., 2019)\" isShortName></Paper>. In general, for large k, the threshold is approximately 2\u1d4f ln 2 - O(k) <Paper corpusId=\"4920536\" paperTitle=\"(Weaver et al., 2014)\" isShortName></Paper> <Paper corpusId=\"265100786\" paperTitle=\"(Achlioptas et al., 2004)\" isShortName></Paper>.\n\nBeyond the satisfiability threshold, random k-SAT exhibits other interesting phase transitions as the clause density increases, including:\n1. The emergence of solution communities <Paper corpusId=\"219636110\" paperTitle=\"(Zhao et al., 2020)\" isShortName></Paper> <Paper corpusId=\"17292209\" paperTitle=\"(Zhou et al., 2009)\" isShortName></Paper>\n2. The breaking of ergodicity in the solution space (clustering or dynamical transition) <Paper corpusId=\"219636110\" paperTitle=\"(Zhao et al., 2020)\" isShortName></Paper> <Paper corpusId=\"10018706\" paperTitle=\"(Krzakala et al., 2006)\" isShortName></Paper>\n3. The condensation transition, where a sub-exponential number of solution clusters dominates <Paper corpusId=\"219636110\" paperTitle=\"(Zhao et al., 2020)\" isShortName></Paper> <Paper corpusId=\"8095011\" paperTitle=\"(Montanari et al., 2008)\" isShortName></Paper>\n\nThese phase transitions have been extensively studied using statistical physics approaches such as the cavity method and replica symmetry breaking <Paper corpusId=\"219636110\" paperTitle=\"(Zhao et al., 2020)\" isShortName></Paper> <Paper corpusId=\"36512754\" paperTitle=\"(Mezard et al., 2002)\" isShortName></Paper>. The structure of the solution space has important implications for the performance of algorithms, as instances near the threshold are computationally hard <Paper corpusId=\"266741631\" paperTitle=\"(Perkins, 2024)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Marino et al., 2015)",
              "snippets": [
                "The K-satisfiability (K-SAT) problem is a combinatorial discrete optimization problem of N Boolean variables, x = {x i } i=1,N , submitted to M constraints. Each constraint, called clause, is in the form of an OR logical operator of K literals (variables or their negations), and the problem is solvable when there exists at least one configuration of the variables, among the 2 N possible ones, that satisfies all constraints. The K-SAT problem for K \u2265 3 is a central problem in combinatorial optimization: it was among the first problems shown to be N P -complete (Cook, 1971)2,3] and is still very much studied. Efforts from the theoretical computer science community have been especially devoted to the study of the random K-SAT ensemble [4,5], where each formula is generated by randomly choosing M = \u03b1N clauses of K literals [6]; indeed formulas from this ensemble become extremely hard to solve when the clause to variable ratio \u03b1 grows [6] and still the locally tree-like structure of the factor graph (Kschischang et al., 2001), representing the interaction network among variables, makes the random K-SAT ensemble a perfect candidate for an analytic solution."
              ],
              "metadata": null
            },
            {
              "id": "(Aggarwal et al., 2017)",
              "snippets": [
                "Recall that, for integer k \u2265 2, a k-SAT formula is the conjunction of clauses, where each clause is the disjunction of k literals. I.e., k-SAT formulas have the form m i=1 k j=1 b i,j , where b i,j = x k or b i,j = \u00acx k for some boolean variable x k",
                "For \u03b7 \u2208 (0, 1) and k \u2265 2, the promise problem Gap-k-SAT \u03b7 is defines as follows. The input is a k-SAT formula with m clauses. It is a YES instance if the formula is satisfiable, and it is a NO instance if the maximal number of simultaneously satisfiable clauses is strictly less than \u03b7m."
              ],
              "metadata": null
            },
            {
              "id": "(Weaver et al., 2014)",
              "snippets": [
                "A random k-SAT instance is a set of clauses drawn uniformly, independently, and with replacement from the set of all width k clauses (Franco et al., 1983). Random k-SAT instances exhibit quite regular behavior in terms of the clauses-to-variables ratio. Specifically, this ratio determines almost certainly (i.e., with high probability) the satisfiability of the set of clauses drawn [1].\n\nA clause can be thought of as a constraint on a putative satisfying assignment. Therefore, a collection of clauses can be thought of as a set of constraints on a putative satisfying assignment. Given a random k-SAT instance X , the strength of X (as a set of constraints) can be measured in terms of the ratio \u03b1 X = m n where m = |X | and n = |Vars|. 2. Intuitively, each constraint represents the same \"strength\" (each only depends on its length, k). Yet, the ratio \u03b1 X determines with high probability the satisfiability of X . Specifically, given a fixed k there exists a number \u03b1 k such that whenever \u03b1 X < \u03b1 k then X is almost certainly satisfiable, and whenever \u03b1 X > \u03b1 k then X is almost certainly unsatisfiable. In (Achlioptas et al., 2004) it was found that \u03b1 k = 2 k ln 2 \u2212 O(k).\n\nIn addition to these theoretical results that prove the bound on the growth of \u03b1 k but do not provide its closed form, experimental results have established approximate values of \u03b1 k for small values of k. These values are given next and are reproduced from (Mertens et al., 2003). Polynomial time algorithms exist for solving random-k SAT instances with k < 3. For k \u2265 3, k-SAT is NP-complete, and no polynomial time algorithms are known for random instances near the satisfiability threshold, \u03b1 k . As well, there is evidence of an algorithmic barrier, i.e., efficient algorithms for solving random k-SAT instances with m/n \u2265 2 k ln k k , for large k, are unlikely to exist [2]."
              ],
              "metadata": null
            },
            {
              "id": "(Friedrich et al., 2019)",
              "snippets": [
                "Random k-SAT is used to study the average-case complexity of Boolean Satisfiability. In the model, a random formula \u03a6 with n variables, m clauses, and k literals per clause is generated in conjunctive normal form. Each of these formulas has the same uniform probability to be generated. Therefore, we also refer to this model as uniform random k-SAT.\n\nOne of the most prominent questions related to studying uniform random k-SAT is trying to prove the satisfiability threshold conjecture. The conjecture states that for a uniform random k-SAT formula \u03a6 with n variables and m clauses there is a real number r k such that lim n\u2192\u221e Pr (\u03a6 is satisfiable) = 1 m/n < r k ; 0 m/n > r k .\n\nChvatal and Reed (Chv\u00e1tal et al., 1992) and, independently, Goerdt (Goerdt, 1992) proved the conjecture for k = 2 and showed that r 2 = 1. For larger values of k upper and lower bounds have been established, e. g., (Ans\u00f3tegui et al., 2009).52 r 3 4.4898 (D\u00edaz et al., 2009)(2003)(Kaporis et al., 2002). Methods from statistical mechanics (M\u00e9zard et al., 2002) were used to derive a numerical estimate of r 3 \u2248 4.26. Coja-Oghlan and Panagiotou (Coja-Oghlan, 2014)(Coja-Oghlan, 2014) showed a bound (up to lower order terms) for k 3 with r k = 2 k log 2 \u2212 1 2 (1 + log 2) \u00b1 o k (1). Finally, Ding, Sly, and Sun (Ding et al., 2014) proved the exact position of the threshold for sufficiently large values of k. Still, for k between 3 and the values determined by Ding, Sly, and Sun the conjecture remains open."
              ],
              "metadata": null
            },
            {
              "id": "(Zhao et al., 2020)",
              "snippets": [
                "The random K-satisfiability (K-SAT) problem is a paradigmatic model system of theoretical computer science [1]. It has been widely adopted to understand the typical-case computational complexity of nondeterministic polynomial complete (NP-complete) optimization problems. It also serves as a convenient test ground for various empirical search algorithms. There are only two parameters: the number K of variables involved in each constraint, and the ratio \u03b1 between the number of constraints and the number of variables (the clause density). Phase transitions in this system has been extensively investigated in the statistical physics community following the initial empirical observations of Cheeseman, Kirkpatrick, and colleagues [2,3] and the theoretical attempts of Monasson and Zecchina [4,5].\n\nDeep insights have been achieved on the statistical properties of the random K-SAT solution space over the last two decades [6][7][8][9][10][11][12][13][14][15][16][17][18][19]. It is now widely accepted that random K-SAT will experience a satisfiability phase transition as the clause density \u03b1 exceeds certain critical value, \u03b1 s . The numerical value of \u03b1 s as a function of K can be computed with high precision by the zero-temperature limit of the first-step replica-symmetry-breaking (1RSB) mean field theory of statistical physics [6,7,11]. Before the satisfiability phase transition occurs at \u03b1 s , the random K-SAT problem will first experiences several other interesting phase transitions as the clause density \u03b1 increases, such as the emergence of solution communities [17,18,20], the breaking of ergodicity in the solution space (the clustering or dynamical transition) [9,10,12,15,16], and the dominance of a sub-exponential number of solution clusters (the condensation transition) [12][13][14]."
              ],
              "metadata": null
            },
            {
              "id": "(Perkins, 2024)",
              "snippets": [
                "In 1992, Mitchell, Selman, and Levesque [107] (working on problems in Artificial Intelligence and computational deductive reasoning) generated uniformly random 3-SAT instances on n variables with m constraints, for fairly large n and different values of m. They ran standard heuristic SAT solving and SAT refutation algorithms and observed the following: the running time required to find a solution (or find a proof that none existed) showed a sharp peak (as a function of m) right around the point at which an estimate of the probability of such a random instance being satisfiable made a sharp decrease from near 1 to near 0. These two empirical observations -that random k-SAT exhibits a sharp threshold and that instances near the threshold are computationally hard -set off an explosion of work on random k-SAT and related models (random graph coloring, k-NAE-SAT, k-XOR-SAT, etc.) in many different directions.\n\nWe can model random k-SAT in the setting of this paper. For a given n and p, let F k (n, p) be a random k-SAT formula generated by including each of the 2n k possible k-clauses independently with probability p. The property of being satisfiable is a non-trivial property with a monotone complement (and thus has a threshold function).\n\nOne long-standing conjecture is that the random k-SAT model exhibits a sharp threshold.\n\nConjecture 11 (Satisfiability Conjecture). For each k \u2265 2, there exists c k > 0 so that for every \u03b5 > 0, the following hold: 1) , then whp F k (n, p) is unsatisfiable.\n\nConjecture 11 was proved early on for the special case of random 2-SAT [36]; and in fact the precise scaling window was determined [26]. The reason for this is that determining satisfiability of a 2-SAT formula can be reduced to determining the strongly connected components of the implication graph on literals; this gives a linear-time algorithm for 2-SAT in the worst case and gives a strong analogy between the random 2-SAT threshold and the emergence of a giant component in a random graph. The case k \u2265 3 is fundamentally different."
              ],
              "metadata": null
            },
            {
              "id": "(Bollobas et al., 1999)",
              "snippets": [
                "We consider the random 2\u2010satisfiability (2\u2010SAT) problem, in which each instance is a formula that is the conjunction of m clauses of the form x\u2228y, chosen uniformly at random from among all 2\u2010clauses on n Boolean variables and their negations. As m and n tend to infinity in the ratio m/n\u2192\u03b1, the problem is known to have a phase transition at \u03b1c=1, below which the probability that the formula is satisfiable tends to one and above which it tends to zero. We determine the finite\u2010size scaling about this transition, namely the scaling of the maximal window W(n,\u2009\u03b4)=(\u03b1\u2212(n,\u03b4), \u03b1+(n,\u03b4)) such that the probability of satisfiability is greater than 1\u2212\u03b4 for \u03b1\u03b1+. We show that W(n,\u03b4)=(1\u2212\u0398(n\u22121/3), 1+\u0398(n\u22121/3)), where the constants implicit in \u0398 depend on \u03b4. We also determine the rates at which the probability of satisfiability approaches one and zero at the boundaries of the window. Namely, for m=(1+\u03b5)n, where \u03b5 may depend on n as long as |\u03b5| is sufficiently small and |\u03b5|n1/3 is sufficiently large, we show that the probability of satisfiability decays like exp(\u2212\u0398(n\u03b53)) above the window, and goes to one like 1\u2212\u0398(n\u22121|\u03b5|\u22123 below the window. We prove these results by defining an order parameter for the transition and establishing its scaling behavior in n both inside and outside the window. Using this order parameter, we prove that the 2\u2010SAT phase transition is continuous with an order parameter critical exponent of 1. We also determine the values of two other critical exponents, showing that the exponents of 2\u2010SAT are identical to those of the random graph.\u2003\u00a9 2001 John Wiley & Sons, Inc.\u2003Random Struct. Alg., 18: 201\u2013256 2001"
              ],
              "metadata": null
            },
            {
              "id": "(Ding et al., 2014)",
              "snippets": [
                "We establish the satisfiability threshold for random k-SAT for all k \u2265 k0. That is, there exists a limiting density \u03b1s(k) such that a random k-SAT formula of clause density \u03b1 is with high probability satisfiable for \u03b1 < \u03b1s, and unsatisfiable for \u03b1 > \u03b1s. The satisfiability threshold \u03b1s is given explicitly by the one-step replica symmetry breaking (1SRB) prediction from statistical physics. We believe that our methods may apply to a range of random constraint satisfaction problems in the 1RSB class."
              ],
              "metadata": null
            },
            {
              "id": "(Achlioptas et al., 2004)",
              "snippets": [
                "Let F k ( n, m ) be a random k -CNF formula formed by selecting uniformly and independently m out of all possible k -clauses on n variables. It is well-known that if r \u2265 2 k log 2, then F k ( n, rn ) is unsatis\ufb01able with probability that tends to 1 as n \u2192 \u221e . We prove that if r \u2264 2 k log 2 \u2212 t k , where t k = O ( k ), then F k ( n, rn ) is satis\ufb01able with probability that tends to 1 as n \u2192 \u221e . Our technique, in fact, yields an explicit lower bound for the random k -SAT threshold for every k . For k \u2265 4 our bounds improve all previously known such bounds."
              ],
              "metadata": null
            },
            {
              "id": "(Zhou et al., 2009)",
              "snippets": [
                "The solution space of a K-satisfiability (K-SAT) formula is a collection of solution clusters, each of which contains all the solutions that are mutually reachable through a sequence of single-spin flips. Knowledge of the statistical property of solution clusters is valuable for a complete understanding of the solution space structure and the computational complexity of the random K-SAT problem. This paper explores single solution clusters of random 3- and 4-SAT formulas through unbiased and biased random-walk processes and the replica-symmetric cavity method of statistical physics. We find that the giant connected component of the solution space has already formed many different communities when the constraint density of the formula is still lower than the solution space clustering transition point. Solutions of the same community are more similar with each other and more densely connected with each other than with the other solutions. The entropy density of a solution community is calculated using belief propagation and is found to be different for different communities of the same cluster. When the constraint density is beyond the clustering transition point, the same behavior is observed for the solution clusters reached by several stochastic search algorithms. Taking together, the results of this work suggest a refined picture on the evolution of the solution space structure of the random K-SAT problem; they may also be helpful for designing heuristic algorithms."
              ],
              "metadata": null
            },
            {
              "id": "(Krzakala et al., 2006)",
              "snippets": [
                "An instance of a random constraint satisfaction problem defines a random subset \ud835\udcae (the set of solutions) of a large product space XN (the set of assignments). We consider two prototypical problem ensembles (random k-satisfiability and q-coloring of random regular graphs) and study the uniform measure with support on S. As the number of constraints per variable increases, this measure first decomposes into an exponential number of pure states (\"clusters\") and subsequently condensates over the largest such states. Above the condensation point, the mass carried by the n largest states follows a Poisson-Dirichlet process. For typical large instances, the two transitions are sharp. We determine their precise location. Further, we provide a formal definition of each phase transition in terms of different notions of correlation between distinct variables in the problem. The degree of correlation naturally affects the performances of many search/sampling algorithms. Empirical evidence suggests that local Monte Carlo Markov chain strategies are effective up to the clustering phase transition and belief propagation up to the condensation point. Finally, refined message passing techniques (such as survey propagation) may also beat this threshold."
              ],
              "metadata": null
            },
            {
              "id": "(Montanari et al., 2008)",
              "snippets": [
                "We study the set of solutions of random k-satisfiability formulas through the cavity method. It is known that, for an interval of the clause-to-variables ratio, this decomposes into an exponential number of pure states (clusters). We refine substantially this picture by: (i) determining the precise location of the clustering transition; (ii) uncovering a second \u2018condensation\u2019 phase transition in the structure of the solution set for k\u22654. These results both follow from computing the large deviation rate of the internal entropy of pure states. From a technical point of view our main contributions are a simplified version of the cavity formalism for special values of the Parisi replica symmetry breaking parameter m (in particular for m = 1 via a correspondence with the tree reconstruction problem) and new large-k expansions."
              ],
              "metadata": null
            },
            {
              "id": "(Mezard et al., 2002)",
              "snippets": [
                "We study the problem of satisfiability of randomly chosen clauses, each with K Boolean variables. Using the cavity method at zero temperature, we find the phase diagram for the K=3 case. We show the existence of an intermediate phase in the satisfiable region, where the proliferation of metastable states is at the origin of the slowdown of search algorithms. The fundamental order parameter introduced in the cavity method, which consists of surveys of local magnetic fields in the various possible states of the system, can be computed for one given sample. These surveys can be used to invent new types of algorithms for solving hard combinatorial optimizations problems. One such algorithm is shown here for the K=3 satisfiability problem, with very good performances."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Definition of Unique k-SAT",
          "text": "\nUnique k-SAT is a specialized variant of the k-SAT problem with an important constraint: the input formula is promised to have either zero or exactly one satisfying assignment <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"558479\" paperTitle=\"(Lee et al., 2016)\" isShortName></Paper>. This promise version of k-SAT represents an interesting restriction of the general problem where multiple satisfying assignments might exist.\n\nOne might intuitively expect that having fewer possible solutions would make the problem easier to solve. However, contrary to this intuition, Unique k-SAT has been proven to be essentially as difficult as general k-SAT <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"210839762\" paperTitle=\"(Liu, 2020)\" isShortName></Paper>. Calabro et al. showed that Unique k-SAT is no easier than k-SAT, not just for polynomial-time algorithms but also for super-polynomial time algorithms <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper>.\n\nThe relationship between the complexity of these problems has been precisely characterized: if Unique k-SAT can be solved in 2^(n(1-f(k)/k)) time for an unbounded function f, then k-SAT can be solved in 2^(n(1-f(k)(1-\u03b5)/k)) time for every \u03b5 > 0 <Paper corpusId=\"219182340\" paperTitle=\"(Vyas et al., 2020)\" isShortName></Paper>. This tight relationship implies that refuting the Super Strong Exponential Time Hypothesis (SETH) for Unique k-SAT would refute it for general k-SAT as well <Paper corpusId=\"219182340\" paperTitle=\"(Vyas et al., 2020)\" isShortName></Paper>.\n\nSeveral reductions between k-SAT and Unique k-SAT have been developed. The Valiant-Vazirani theorem provided a polynomial-time randomized reduction from SAT to Unique SAT, showing that deciding whether a CNF formula has zero or one solution is essentially as difficult as general SAT <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper>. However, this reduction doesn't preserve clause width. Later, Calabro et al. developed a randomized polynomial-time reduction with one-sided error from k-SAT to Unique k-SAT, which was further improved by Traxler <Paper corpusId=\"219182340\" paperTitle=\"(Vyas et al., 2020)\" isShortName></Paper>.\n\nDespite its hardness, Unique k-SAT has been useful in algorithm analysis. For instance, the PPSZ algorithm for SAT has been shown to perform well on Unique k-SAT instances <Paper corpusId=\"9551321\" paperTitle=\"(Hertli et al., 2010)\" isShortName></Paper>, and analyzing algorithm running times is often easier for formulas with a unique satisfying assignment <Paper corpusId=\"43594026\" paperTitle=\"(Scheder et al., 2017)\" isShortName></Paper>. For Unique 3-SAT specifically, researchers have improved the time bound to O(1.307^n) <Paper corpusId=\"195259544\" paperTitle=\"(Hansen et al., 2019)\" isShortName></Paper>.\n\nAn interesting property of Unique k-SAT relates to variable assignments. In Unique k-SAT, all variables are \"frozen,\" meaning they have the same value in all satisfying assignments (since there is at most one such assignment) <Paper corpusId=\"2152587\" paperTitle=\"(Hertli, 2011)\" isShortName></Paper>. This property has been leveraged in algorithmic analysis.\n\nNotably, despite all the work on Unique k-SAT, its complexity status\u2014specifically whether it is P^NP-complete\u2014remains unresolved <Paper corpusId=\"277824219\" paperTitle=\"(Nagao et al., 2025)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Fu et al., 2020)",
              "snippets": [
                "Unique k-SAT is the promised version of k-SAT where the given formula has 0 or 1 solution and is proved to be as difficult as the general k-SAT",
                "Unique k-SAT denotes the promise search problem of k-SAT where the number of solutions is either 0 or 1. The harder instances should have fewer solutions. But Calabro and Paturi in (Calabro et al., 2009) proved that the exponential complexity of deciding whether a k-CNF formula has a solution is the same as that of deciding whether it has exactly one solution, both when it is promised and when it is not promised that the input formula has a solution",
                "Valiant and Vazirani in (Valiant et al., 1985) gave a randomized polynomial time reduction from SAT to Unique SAT, and showed that deciding whether a CNF formula has zero or one solution is essentially as difficult as SAT in general. Calabro et al. in (Calabro et al., 2003) proved that Unique k-SAT is no easier than k-SAT, not just for polynomial time algorithms but also super-polynomial time algorithms. They in (Calabro et al., 2009) pointed out it does not matter whether there has a promise that a formula has a solution. Matthews in (Matthews et al., 2010) studied the complexity of UNIQUE-(k, s)-SAT and proved that f (k) \u2264 u(k) \u2264 f (k) + 2 for k \u2265 3, where u(k) is the minimal value of s so that uniquely satisfiable (k, s)-CNF formulas exist and f (k) represents the maximal value of s such that all (k, s)-CNF formulas are satisfiable."
              ],
              "metadata": null
            },
            {
              "id": "(Lee et al., 2016)",
              "snippets": [
                "Unique k-SAT is the promise problem where the input is a k-CNF formula F with the promise that F has at most one satisfying assignment, and the question is whether F is satisfiable. Calabro et al. (Calabro et al., 2003) show that under SETH, k-SAT and Unique k-SAT have the same asymptotic complexity. In other words, for every \u03b5 > 0 there is a k \u2208 N such that if Unique k-SAT can be solved in time O * (2 (1\u2212\u03b5)n ), then SETH is false."
              ],
              "metadata": null
            },
            {
              "id": "(Liu, 2020)",
              "snippets": [
                "It should be mentioned that the behavior of PPSZ was not completely understood at the first time it was invented: There is an exponential loss in the upper bound for General k-SAT, comparing with that for Unique k-SAT (the formula guarantees to have at most one solution). Nevertheless, Unique k-SAT is believed to be at least as hard as General k-SAT (Calabro et al., 2003)."
              ],
              "metadata": null
            },
            {
              "id": "(Vyas et al., 2020)",
              "snippets": [
                "The Unique k-SAT problem is the special case where there is at most one satisfying assignment. It is natural to hypothesize that the worst-case (exponential-time) complexity of Unique k-SAT is substantially less than that of k-SAT. Improving prior reductions, we show the time complexities of Unique k-SAT and k-SAT are very tightly related: if Unique k-SAT is in 2n(1\u2212f(k)/k) time for an unbounded f, then k-SAT is in 2n(1\u2212f(k)(1\u2212\u025b)/k) time for every \u03b5 > 0. Thus, refuting Super Strong ETH in the unique solution case would refute Super Strong ETH in general.\n\n(Valiant and Vazirani 1986) gave a poly-time randomized reduction from SAT instances F on n variables to Unique-SAT instances F on n variables such that, if F is SAT, then F is a unique satisfying assignment with probability at least \u03a9(1/n), and if F is UNSAT then F is UNSAT. This reduction does not apply to convert k-SAT instances to Unique k-SAT instances, as they do not preserve the clause width (nor do they preserve the number of variables, when transformed into k-SAT instances in the natural way). To address this, (Calabro et al. 2008) gave a randomized polynomialtime reduction with one-sided error from k-SAT to Unique k-SAT which works with probability 2\u2212O(n log 2(k)/k). The probability bound was further improved by (Traxler 2008) to 2\u2212O(n log(k)/k)."
              ],
              "metadata": null
            },
            {
              "id": "(Hertli et al., 2010)",
              "snippets": [
                "The unique k-sat problem appears in this paper as \"unique k-SAT,\" which refers to a CNF formula that has exactly one satisfying assignment. For unique k-SAT, the PPSZ algorithm has a success probability that is \"quite good (for 3-SAT, it is \u03a9(1.308^-n)), and the analysis is highly elegant.\""
              ],
              "metadata": null
            },
            {
              "id": "(Scheder et al., 2017)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Hansen et al., 2019)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Hertli, 2011)",
              "snippets": [
                "A variable x of F is called frozen if it has the same value in all satisfying assignments, and non-frozen otherwise",
                "In Unique k-SAT it was shown that the probability that a variable is guessed is bounded from above by some quantity S depending on k on \u01eb. Using this, it is not hard to show that there exists an algorithm for Unique k-SAT running in time O(2 S\u2022n ).\n\nFor k = 3, we can show that S 3 = 2 ln 2\u22121. For small k, S k and 2 In [7] it was shown using Jensen's inequality how to use a bound for the probability that a variable is guessed to give an upper bound for the running time of Unique k-SAT. For k \u2265 5, the same bound holds also for k-SAT using a more elaborate argument.\n\nTheorem 7 ([7]). For \u01eb > 0, there exists a randomized algorithm for Unique k-SAT with one-sided error that runs in time O(2 S k n+\u01ebn ). For k \u2265 5, this is also true for k-SAT.\n\nIn this paper we prove that for k-SAT we have the same bound as Unique k-SAT for all k. For k \u2265 5, this can be seen as an alternative proof for general k-SAT."
              ],
              "metadata": null
            },
            {
              "id": "(Nagao et al., 2025)",
              "snippets": [
                "A well-known example is the Unique k-SAT problem, whose complexity status-whether it is P NP -complete-remains unresolved (Blass et al., 1982)."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Complexity Results for Unique k-SAT",
          "text": "\nThe complexity of Unique k-SAT has been extensively studied, with surprising results regarding its relationship to general k-SAT. Contrary to what might be expected, having at most one solution does not make the problem substantially easier. Calabro et al. proved that Unique k-SAT is no easier than k-SAT, not just for polynomial-time algorithms but also for super-polynomial time algorithms <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper>.\n\nThis complexity relationship has been precisely characterized: if Unique k-SAT can be solved in 2^(n(1-f(k)/k)) time for an unbounded function f, then k-SAT can be solved in 2^(n(1-f(k)(1-\u03b5)/k)) time for every \u03b5 > 0 <Paper corpusId=\"219182340\" paperTitle=\"(Vyas et al., 2020)\" isShortName></Paper> <Paper corpusId=\"558479\" paperTitle=\"(Lee et al., 2016)\" isShortName></Paper>. In other words, under the Strong Exponential Time Hypothesis (SETH), k-SAT and Unique k-SAT have the same asymptotic complexity <Paper corpusId=\"558479\" paperTitle=\"(Lee et al., 2016)\" isShortName></Paper>. This implies that refuting the Super Strong Exponential Time Hypothesis for Unique k-SAT would refute it for general k-SAT as well <Paper corpusId=\"219182340\" paperTitle=\"(Vyas et al., 2020)\" isShortName></Paper>.\n\nThe complexity equivalence between Unique k-SAT and general k-SAT also applies when considering the promise that a formula has a solution. Calabro and Paturi proved that the exponential complexity of deciding whether a k-CNF formula has a solution is the same as deciding whether it has exactly one solution, both with and without the promise that the input formula has a solution <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper>.\n\nAnother important complexity result comes from Matthews, who studied UNIQUE-(k,s)-SAT and proved that f(k) \u2264 u(k) \u2264 f(k) + 2 for k \u2265 3, where u(k) is the minimal value of s such that uniquely satisfiable (k,s)-CNF formulas exist, and f(k) represents the maximal value of s such that all (k,s)-CNF formulas are satisfiable <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper>.\n\nDespite all these results establishing the tight relationship between Unique k-SAT and general k-SAT, an important question about the complexity classification of Unique k-SAT remains open. Specifically, the question of whether Unique k-SAT is P^NP-complete remains unresolved <Paper corpusId=\"277824219\" paperTitle=\"(Nagao et al., 2025)\" isShortName></Paper>. This unresolved status makes Unique k-SAT an interesting subject for continued theoretical investigation.\n\nThe hardness of Unique k-SAT has also influenced algorithm analysis. The PPSZ algorithm, for instance, performs better on Unique k-SAT than on general k-SAT, with an exponential gap in the upper bounds of their running times <Paper corpusId=\"210839762\" paperTitle=\"(Liu, 2020)\" isShortName></Paper>. However, this performance difference does not contradict the theoretical hardness results, as the worst-case complexity remains essentially equivalent.",
          "citations": [
            {
              "id": "(Fu et al., 2020)",
              "snippets": [
                "Unique k-SAT is the promised version of k-SAT where the given formula has 0 or 1 solution and is proved to be as difficult as the general k-SAT",
                "Unique k-SAT denotes the promise search problem of k-SAT where the number of solutions is either 0 or 1. The harder instances should have fewer solutions. But Calabro and Paturi in (Calabro et al., 2009) proved that the exponential complexity of deciding whether a k-CNF formula has a solution is the same as that of deciding whether it has exactly one solution, both when it is promised and when it is not promised that the input formula has a solution",
                "Valiant and Vazirani in (Valiant et al., 1985) gave a randomized polynomial time reduction from SAT to Unique SAT, and showed that deciding whether a CNF formula has zero or one solution is essentially as difficult as SAT in general. Calabro et al. in (Calabro et al., 2003) proved that Unique k-SAT is no easier than k-SAT, not just for polynomial time algorithms but also super-polynomial time algorithms. They in (Calabro et al., 2009) pointed out it does not matter whether there has a promise that a formula has a solution. Matthews in (Matthews et al., 2010) studied the complexity of UNIQUE-(k, s)-SAT and proved that f (k) \u2264 u(k) \u2264 f (k) + 2 for k \u2265 3, where u(k) is the minimal value of s so that uniquely satisfiable (k, s)-CNF formulas exist and f (k) represents the maximal value of s such that all (k, s)-CNF formulas are satisfiable."
              ],
              "metadata": null
            },
            {
              "id": "(Vyas et al., 2020)",
              "snippets": [
                "The Unique k-SAT problem is the special case where there is at most one satisfying assignment. It is natural to hypothesize that the worst-case (exponential-time) complexity of Unique k-SAT is substantially less than that of k-SAT. Improving prior reductions, we show the time complexities of Unique k-SAT and k-SAT are very tightly related: if Unique k-SAT is in 2n(1\u2212f(k)/k) time for an unbounded f, then k-SAT is in 2n(1\u2212f(k)(1\u2212\u025b)/k) time for every \u03b5 > 0. Thus, refuting Super Strong ETH in the unique solution case would refute Super Strong ETH in general.\n\n(Valiant and Vazirani 1986) gave a poly-time randomized reduction from SAT instances F on n variables to Unique-SAT instances F on n variables such that, if F is SAT, then F is a unique satisfying assignment with probability at least \u03a9(1/n), and if F is UNSAT then F is UNSAT. This reduction does not apply to convert k-SAT instances to Unique k-SAT instances, as they do not preserve the clause width (nor do they preserve the number of variables, when transformed into k-SAT instances in the natural way). To address this, (Calabro et al. 2008) gave a randomized polynomialtime reduction with one-sided error from k-SAT to Unique k-SAT which works with probability 2\u2212O(n log 2(k)/k). The probability bound was further improved by (Traxler 2008) to 2\u2212O(n log(k)/k)."
              ],
              "metadata": null
            },
            {
              "id": "(Lee et al., 2016)",
              "snippets": [
                "Unique k-SAT is the promise problem where the input is a k-CNF formula F with the promise that F has at most one satisfying assignment, and the question is whether F is satisfiable. Calabro et al. (Calabro et al., 2003) show that under SETH, k-SAT and Unique k-SAT have the same asymptotic complexity. In other words, for every \u03b5 > 0 there is a k \u2208 N such that if Unique k-SAT can be solved in time O * (2 (1\u2212\u03b5)n ), then SETH is false."
              ],
              "metadata": null
            },
            {
              "id": "(Nagao et al., 2025)",
              "snippets": [
                "A well-known example is the Unique k-SAT problem, whose complexity status-whether it is P NP -complete-remains unresolved (Blass et al., 1982)."
              ],
              "metadata": null
            },
            {
              "id": "(Liu, 2020)",
              "snippets": [
                "It should be mentioned that the behavior of PPSZ was not completely understood at the first time it was invented: There is an exponential loss in the upper bound for General k-SAT, comparing with that for Unique k-SAT (the formula guarantees to have at most one solution). Nevertheless, Unique k-SAT is believed to be at least as hard as General k-SAT (Calabro et al., 2003)."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Algorithms for Unique k-SAT",
          "text": "\nThe PPSZ algorithm, developed by Paturi, Pudlak, Saks, and Zane, has been particularly effective for Unique k-SAT. For uniquely satisfiable 3-CNF formulas, PPSZ can find the solution in O(1.3071^n) expected running time <Paper corpusId=\"7991609\" paperTitle=\"(Rolf, 2006)\" isShortName></Paper>. This is significantly better than its performance on general k-SAT instances, where the bound gets worse as the number of solutions increases <Paper corpusId=\"7991609\" paperTitle=\"(Rolf, 2006)\" isShortName></Paper>. The algorithm's superior performance on Unique k-SAT is reflected in its success probability, which for Unique 3-SAT is \u03a9(1.308^-n) <Paper corpusId=\"9551321\" paperTitle=\"(Hertli et al., 2010)\" isShortName></Paper>.\n\nThe strong performance of PPSZ on Unique k-SAT can be attributed to the \"frozen\" property of variables in uniquely satisfiable formulas. A variable is considered \"frozen\" if it has the same value in all satisfying assignments <Paper corpusId=\"2152587\" paperTitle=\"(Hertli, 2011)\" isShortName></Paper>. In Unique k-SAT, all variables are frozen by definition (since there is at most one satisfying assignment), which allows for tighter bounds on the probability that a variable needs to be guessed <Paper corpusId=\"2152587\" paperTitle=\"(Hertli, 2011)\" isShortName></Paper>.\n\nHertli showed that this property can be leveraged to create an algorithm for Unique k-SAT running in time O(2^(S\u00b7n)), where S depends on k. For k = 3, S\u2083 = 2 ln 2 - 1, and more generally, there exists a randomized algorithm for Unique k-SAT with one-sided error that runs in time O(2^(S_k\u00b7n+\u03b5\u00b7n)) for any \u03b5 > 0 <Paper corpusId=\"2152587\" paperTitle=\"(Hertli, 2011)\" isShortName></Paper>.\n\nRecent improvements have pushed these bounds even further. Hansen et al. introduced a biased version of the PPSZ algorithm that improved the bound for Unique 3-SAT from 1.308^n to 1.307^n <Paper corpusId=\"220514246\" paperTitle=\"(Rennela et al., 2020)\" isShortName></Paper> <Paper corpusId=\"195259544\" paperTitle=\"(Hansen et al., 2019)\" isShortName></Paper>. Scheder further improved the analysis, showing that PPSZ performs exponentially better than previously known for all k\u22653, bounding the running time for Unique 3-SAT by O(1.306973^n) <Paper corpusId=\"220514246\" paperTitle=\"(Rennela et al., 2020)\" isShortName></Paper> <Paper corpusId=\"235204883\" paperTitle=\"(Scheder, 2022)\" isShortName></Paper>.\n\nThe dncPPSZ algorithm (a variant of PPSZ) has been proven to have a runtime of 2^((\u03b3_k+\u03b5)n), matching the best-known SAT algorithm performance <Paper corpusId=\"220514246\" paperTitle=\"(Rennela et al., 2020)\" isShortName></Paper>. Interestingly, these results for Unique k-SAT can often be generalized to general k-SAT, though typically with different bounds or more complex proofs <Paper corpusId=\"220514246\" paperTitle=\"(Rennela et al., 2020)\" isShortName></Paper>.\n\nBeyond PPSZ-based approaches, other algorithmic techniques have been developed. Calabro, Impagliazzo, Kabanets, and Paturi developed a hashing scheme for reducing k-SAT to Unique-k-SAT <Paper corpusId=\"23140191\" paperTitle=\"(Dell et al., 2017)\" isShortName></Paper>. Additionally, algorithms based on counting unsatisfiable variations of clauses have been shown to work for both Unique-SAT and #SAT (the problem of counting all satisfying assignments) <Paper corpusId=\"267028568\" paperTitle=\"(Sarriguren, 2024)\" isShortName></Paper>.\n\nThe study of algorithms for Unique k-SAT continues to be an active research area, with insights from this restriction often leading to improvements for general k-SAT algorithms as well.",
          "citations": [
            {
              "id": "(Rolf, 2006)",
              "snippets": [
                "In [7], Paturi, Pudlak, Saks, and Zane proved that for a uniquely satisfiable 3-CNF, the solution can be found in O(1.3071^n) expected running time at most. We refer to their algorithm as the PPSZ Algorithm. This is the best randomized bound known for Unique-3-SAT and it is possible to derandomize it, essentially yielding the same bound deterministically, cf. [8]. But paradoxically, the bound gets worse when the number of solutions increases."
              ],
              "metadata": null
            },
            {
              "id": "(Hertli et al., 2010)",
              "snippets": [
                "The unique k-sat problem appears in this paper as \"unique k-SAT,\" which refers to a CNF formula that has exactly one satisfying assignment. For unique k-SAT, the PPSZ algorithm has a success probability that is \"quite good (for 3-SAT, it is \u03a9(1.308^-n)), and the analysis is highly elegant.\""
              ],
              "metadata": null
            },
            {
              "id": "(Hertli, 2011)",
              "snippets": [
                "A variable x of F is called frozen if it has the same value in all satisfying assignments, and non-frozen otherwise",
                "In Unique k-SAT it was shown that the probability that a variable is guessed is bounded from above by some quantity S depending on k on \u01eb. Using this, it is not hard to show that there exists an algorithm for Unique k-SAT running in time O(2 S\u2022n ).\n\nFor k = 3, we can show that S 3 = 2 ln 2\u22121. For small k, S k and 2 In [7] it was shown using Jensen's inequality how to use a bound for the probability that a variable is guessed to give an upper bound for the running time of Unique k-SAT. For k \u2265 5, the same bound holds also for k-SAT using a more elaborate argument.\n\nTheorem 7 ([7]). For \u01eb > 0, there exists a randomized algorithm for Unique k-SAT with one-sided error that runs in time O(2 S k n+\u01ebn ). For k \u2265 5, this is also true for k-SAT.\n\nIn this paper we prove that for k-SAT we have the same bound as Unique k-SAT for all k. For k \u2265 5, this can be seen as an alternative proof for general k-SAT."
              ],
              "metadata": null
            },
            {
              "id": "(Rennela et al., 2020)",
              "snippets": [
                "In this section, we prove that dncPPSZ (see Algorithm 2) is correct and has a runtime of 2 (\u03b3 k +\u03b5)n , i.e., it equals the best-known SAT algorithm as recent analysis of PPSZ confirms (Scheder, 2022) (it surpassed Biased PPSZ (Hansen et al., 2019) again). We focus here on unique-k-SAT, in which the input formula has a unique satisfying assignment if satisfiable. (The results can be generalized to general k-SAT based on (Scheder, 2022).)"
              ],
              "metadata": null
            },
            {
              "id": "(Hansen et al., 2019)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Scheder, 2022)",
              "snippets": [
                "PPSZ, for long time the fastest known algorithm for $k$-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses). We show that PPSZ performs exponentially better than previously known, for all $k\\geq 3$. For Unique-3-SAT we bound its running time by $O(1.306973^{n})$, which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick. All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write \"pretend\" since this can be done without any actual change to the algorithm.22The full version of this paper can be found under https://eccc.weizmann.ac.il/report/2021/069/"
              ],
              "metadata": null
            },
            {
              "id": "(Dell et al., 2017)",
              "snippets": [
                "We therefore make use of a hashing scheme developed by Calabro, Impagliazzo, Kabanets, and Paturi (Calabro et al., 2003) for a related problem, that of reducing k-SAT to Unique-k-SAT."
              ],
              "metadata": null
            },
            {
              "id": "(Sarriguren, 2024)",
              "snippets": [
                "Moreover, this complete algorithm based on counting unsatisfiable variations of clauses is also a complete algorithm for variations of SAT explained in [1] such as the previously mentionated Unique-SAT or propositional model counting #SAT (a variation of SAT that calculates the total number of satisfying assignments)."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Relationship Between Unique k-SAT and General k-SAT",
          "text": "\nThe relationship between Unique k-SAT and general k-SAT reveals a surprising complexity equivalence despite their different constraints. While one might intuitively expect that having at most one solution would make Unique k-SAT easier to solve, extensive theoretical work has established that both problems share essentially the same hardness. Calabro et al. proved that Unique k-SAT is no easier than general k-SAT, not just for polynomial-time algorithms but also for super-polynomial time algorithms <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"210839762\" paperTitle=\"(Liu, 2020)\" isShortName></Paper>.\n\nThis equivalence has been precisely characterized: if Unique k-SAT can be solved in 2^(n(1-f(k)/k)) time for an unbounded function f, then k-SAT can be solved in 2^(n(1-f(k)(1-\u03b5)/k)) time for every \u03b5 > 0 <Paper corpusId=\"219182340\" paperTitle=\"(Vyas et al., 2020)\" isShortName></Paper>. Under the Strong Exponential Time Hypothesis (SETH), k-SAT and Unique k-SAT have the same asymptotic complexity <Paper corpusId=\"558479\" paperTitle=\"(Lee et al., 2016)\" isShortName></Paper>. This means that refuting the Super Strong ETH in the unique solution case would refute it in general <Paper corpusId=\"219182340\" paperTitle=\"(Vyas et al., 2020)\" isShortName></Paper>.\n\nSeveral important reductions have been developed to establish this relationship. The Valiant-Vazirani theorem provided an early polynomial-time randomized reduction from SAT to Unique SAT <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper>. Since this reduction didn't preserve clause width, Calabro et al. later developed a randomized polynomial-time reduction with one-sided error from k-SAT to Unique k-SAT, which was further improved by Traxler <Paper corpusId=\"219182340\" paperTitle=\"(Vyas et al., 2020)\" isShortName></Paper>.\n\nThe complexity equivalence holds even with different promise conditions. Calabro and Paturi proved that the exponential complexity of deciding whether a k-CNF formula has a solution is the same as deciding whether it has exactly one solution, both with and without the promise that the input formula has a solution <Paper corpusId=\"219501817\" paperTitle=\"(Fu et al., 2020)\" isShortName></Paper>.\n\nDespite their theoretical equivalence, algorithms often perform differently on the two problems. The PPSZ algorithm and its variants typically achieve better performance on Unique k-SAT compared to general k-SAT <Paper corpusId=\"210839762\" paperTitle=\"(Liu, 2020)\" isShortName></Paper>. This difference stems from the \"frozen\" property of variables in uniquely satisfiable formulas, where all variables must have the same value in all satisfying assignments (since there is at most one such assignment) <Paper corpusId=\"2152587\" paperTitle=\"(Hertli, 2011)\" isShortName></Paper>.\n\nFor Unique k-SAT, Hertli showed that the probability a variable is guessed is bounded by a quantity S depending on k, leading to an algorithm running in time O(2^(S\u00b7n)). For k = 3, S\u2083 = 2 ln 2 - 1 <Paper corpusId=\"2152587\" paperTitle=\"(Hertli, 2011)\" isShortName></Paper>. While these results were initially specific to Unique k-SAT, many have been generalized to k-SAT, although sometimes with different bounds or more complex proofs <Paper corpusId=\"2152587\" paperTitle=\"(Hertli, 2011)\" isShortName></Paper>.\n\nRecent algorithmic improvements continue to show this pattern. The dncPPSZ algorithm achieves a runtime of 2^((\u03b3_k+\u03b5)n), matching the best-known SAT algorithm performance <Paper corpusId=\"220514246\" paperTitle=\"(Rennela et al., 2020)\" isShortName></Paper>. While initially focused on Unique k-SAT, these results can be generalized to general k-SAT <Paper corpusId=\"220514246\" paperTitle=\"(Rennela et al., 2020)\" isShortName></Paper> <Paper corpusId=\"235204883\" paperTitle=\"(Scheder, 2022)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Fu et al., 2020)",
              "snippets": [
                "Unique k-SAT is the promised version of k-SAT where the given formula has 0 or 1 solution and is proved to be as difficult as the general k-SAT",
                "Unique k-SAT denotes the promise search problem of k-SAT where the number of solutions is either 0 or 1. The harder instances should have fewer solutions. But Calabro and Paturi in (Calabro et al., 2009) proved that the exponential complexity of deciding whether a k-CNF formula has a solution is the same as that of deciding whether it has exactly one solution, both when it is promised and when it is not promised that the input formula has a solution",
                "Valiant and Vazirani in (Valiant et al., 1985) gave a randomized polynomial time reduction from SAT to Unique SAT, and showed that deciding whether a CNF formula has zero or one solution is essentially as difficult as SAT in general. Calabro et al. in (Calabro et al., 2003) proved that Unique k-SAT is no easier than k-SAT, not just for polynomial time algorithms but also super-polynomial time algorithms. They in (Calabro et al., 2009) pointed out it does not matter whether there has a promise that a formula has a solution. Matthews in (Matthews et al., 2010) studied the complexity of UNIQUE-(k, s)-SAT and proved that f (k) \u2264 u(k) \u2264 f (k) + 2 for k \u2265 3, where u(k) is the minimal value of s so that uniquely satisfiable (k, s)-CNF formulas exist and f (k) represents the maximal value of s such that all (k, s)-CNF formulas are satisfiable."
              ],
              "metadata": null
            },
            {
              "id": "(Liu, 2020)",
              "snippets": [
                "It should be mentioned that the behavior of PPSZ was not completely understood at the first time it was invented: There is an exponential loss in the upper bound for General k-SAT, comparing with that for Unique k-SAT (the formula guarantees to have at most one solution). Nevertheless, Unique k-SAT is believed to be at least as hard as General k-SAT (Calabro et al., 2003)."
              ],
              "metadata": null
            },
            {
              "id": "(Vyas et al., 2020)",
              "snippets": [
                "The Unique k-SAT problem is the special case where there is at most one satisfying assignment. It is natural to hypothesize that the worst-case (exponential-time) complexity of Unique k-SAT is substantially less than that of k-SAT. Improving prior reductions, we show the time complexities of Unique k-SAT and k-SAT are very tightly related: if Unique k-SAT is in 2n(1\u2212f(k)/k) time for an unbounded f, then k-SAT is in 2n(1\u2212f(k)(1\u2212\u025b)/k) time for every \u03b5 > 0. Thus, refuting Super Strong ETH in the unique solution case would refute Super Strong ETH in general.\n\n(Valiant and Vazirani 1986) gave a poly-time randomized reduction from SAT instances F on n variables to Unique-SAT instances F on n variables such that, if F is SAT, then F is a unique satisfying assignment with probability at least \u03a9(1/n), and if F is UNSAT then F is UNSAT. This reduction does not apply to convert k-SAT instances to Unique k-SAT instances, as they do not preserve the clause width (nor do they preserve the number of variables, when transformed into k-SAT instances in the natural way). To address this, (Calabro et al. 2008) gave a randomized polynomialtime reduction with one-sided error from k-SAT to Unique k-SAT which works with probability 2\u2212O(n log 2(k)/k). The probability bound was further improved by (Traxler 2008) to 2\u2212O(n log(k)/k)."
              ],
              "metadata": null
            },
            {
              "id": "(Lee et al., 2016)",
              "snippets": [
                "Unique k-SAT is the promise problem where the input is a k-CNF formula F with the promise that F has at most one satisfying assignment, and the question is whether F is satisfiable. Calabro et al. (Calabro et al., 2003) show that under SETH, k-SAT and Unique k-SAT have the same asymptotic complexity. In other words, for every \u03b5 > 0 there is a k \u2208 N such that if Unique k-SAT can be solved in time O * (2 (1\u2212\u03b5)n ), then SETH is false."
              ],
              "metadata": null
            },
            {
              "id": "(Hertli, 2011)",
              "snippets": [
                "A variable x of F is called frozen if it has the same value in all satisfying assignments, and non-frozen otherwise",
                "In Unique k-SAT it was shown that the probability that a variable is guessed is bounded from above by some quantity S depending on k on \u01eb. Using this, it is not hard to show that there exists an algorithm for Unique k-SAT running in time O(2 S\u2022n ).\n\nFor k = 3, we can show that S 3 = 2 ln 2\u22121. For small k, S k and 2 In [7] it was shown using Jensen's inequality how to use a bound for the probability that a variable is guessed to give an upper bound for the running time of Unique k-SAT. For k \u2265 5, the same bound holds also for k-SAT using a more elaborate argument.\n\nTheorem 7 ([7]). For \u01eb > 0, there exists a randomized algorithm for Unique k-SAT with one-sided error that runs in time O(2 S k n+\u01ebn ). For k \u2265 5, this is also true for k-SAT.\n\nIn this paper we prove that for k-SAT we have the same bound as Unique k-SAT for all k. For k \u2265 5, this can be seen as an alternative proof for general k-SAT."
              ],
              "metadata": null
            },
            {
              "id": "(Rennela et al., 2020)",
              "snippets": [
                "In this section, we prove that dncPPSZ (see Algorithm 2) is correct and has a runtime of 2 (\u03b3 k +\u03b5)n , i.e., it equals the best-known SAT algorithm as recent analysis of PPSZ confirms (Scheder, 2022) (it surpassed Biased PPSZ (Hansen et al., 2019) again). We focus here on unique-k-SAT, in which the input formula has a unique satisfying assignment if satisfiable. (The results can be generalized to general k-SAT based on (Scheder, 2022).)"
              ],
              "metadata": null
            },
            {
              "id": "(Scheder, 2022)",
              "snippets": [
                "PPSZ, for long time the fastest known algorithm for $k$-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses). We show that PPSZ performs exponentially better than previously known, for all $k\\geq 3$. For Unique-3-SAT we bound its running time by $O(1.306973^{n})$, which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick. All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write \"pretend\" since this can be done without any actual change to the algorithm.22The full version of this paper can be found under https://eccc.weizmann.ac.il/report/2021/069/"
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Related Variants and Problems",
          "text": "\nBeyond Unique k-SAT, researchers have explored several related variants of the k-SAT problem, each with their own theoretical properties and challenges. One important variant is Gap-k-SAT, which introduces a promise problem structure. In Gap-k-SAT\u208d\u03b7\u208e, for some \u03b7 \u2208 (0, 1), an input k-SAT formula with m clauses is classified as a YES instance if it is completely satisfiable, and as a NO instance if at most \u03b7m clauses can be simultaneously satisfied. <Paper corpusId=\"11932153\" paperTitle=\"(Aggarwal et al., 2017)\" isShortName></Paper>\n\nAnother variant is the regular (d,k)-SAT problem, which imposes structural constraints on the formula. In a regular (d,k)-CNF formula, each clause contains exactly k literals, and each variable appears exactly d times in the formula. This creates a symmetric structure that has been studied as the (d,k)-SAT problem. A further refinement is the regular exact 2-(d,k)-SAT problem, which requires that for any satisfying assignment, exactly two literals in each clause must evaluate to true. <Paper corpusId=\"237004638\" paperTitle=\"(Nie et al., 2021)\" isShortName></Paper>\n\nLINEAR-2-RL represents another variant with interesting computational complexity properties. While not fully defined in the available references, research by Kane et al. has established quasi-polynomial lower bounds for this problem under the randomized Exponential Time Hypothesis (rETH), suggesting that it likely does not admit efficient algorithms. The authors noted the potential for extending their techniques to prove even stronger exponential lower bounds. <Paper corpusId=\"246823130\" paperTitle=\"(Kane et al., 2022)\" isShortName></Paper>\n\nThe #SAT problem (or propositional model counting) represents a significant extension from decision problems like Unique k-SAT to counting problems. Rather than determining if a formula has exactly one solution, #SAT aims to count the total number of satisfying assignments. Interestingly, some algorithmic approaches that work for Unique-SAT can be adapted to solve #SAT as well. For example, algorithms based on counting unsatisfiable variations of clauses have been shown to work for both problems. <Paper corpusId=\"267028568\" paperTitle=\"(Sarriguren, 2024)\" isShortName></Paper>\n\nThese variants demonstrate how the k-SAT framework can be extended in multiple directions\u2014by adding constraints on the formula structure (as in regular (d,k)-SAT), by changing the nature of the decision problem (as in Gap-k-SAT), or by moving from decision to counting problems (as in #SAT). Each variant provides different insights into the complexity landscape of Boolean satisfiability while maintaining connections to the fundamental k-SAT problem.",
          "citations": [
            {
              "id": "(Aggarwal et al., 2017)",
              "snippets": [
                "Recall that, for integer k \u2265 2, a k-SAT formula is the conjunction of clauses, where each clause is the disjunction of k literals. I.e., k-SAT formulas have the form m i=1 k j=1 b i,j , where b i,j = x k or b i,j = \u00acx k for some boolean variable x k",
                "For \u03b7 \u2208 (0, 1) and k \u2265 2, the promise problem Gap-k-SAT \u03b7 is defines as follows. The input is a k-SAT formula with m clauses. It is a YES instance if the formula is satisfiable, and it is a NO instance if the maximal number of simultaneously satisfiable clauses is strictly less than \u03b7m."
              ],
              "metadata": null
            },
            {
              "id": "(Nie et al., 2021)",
              "snippets": [
                "In a regular (d,k)-CNF formula, each clause has length k and each variable appears d times. A regular structure such as this is symmetric, and the satisfiability problem of this symmetric structure is called the (d,k)-SAT problem for short. The regular exact 2-(d,k)-SAT problem is that for a (d,k)-CNF formula F, if there is a truth assignment T, then exactly two literals of each clause in F are true."
              ],
              "metadata": null
            },
            {
              "id": "(Kane et al., 2022)",
              "snippets": [
                "In conjunction with our reduction, this gives a quasi-polynomial lower bound for LINEAR-2-RL under rETH. We leave as an open problem if the techniques introduced in this work can be used to prove an exponential lower bound for LINEAR-2-RL under rETH."
              ],
              "metadata": null
            },
            {
              "id": "(Sarriguren, 2024)",
              "snippets": [
                "Moreover, this complete algorithm based on counting unsatisfiable variations of clauses is also a complete algorithm for variations of SAT explained in [1] such as the previously mentionated Unique-SAT or propositional model counting #SAT (a variation of SAT that calculates the total number of satisfying assignments)."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What is unique k-sat and what theoretical results are known about it?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should define the general k-sat problem and unique k-sat problems formally.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Unique k-SAT is a variation of the classical k-SAT (k-Satisfiability) problem, which itself is a well-known problem in computer science and combinatorial optimization. In the k-SAT problem, you are given a boolean formula in conjunctive normal form (CNF), where each clause contains exactly k literals, and you need to determine if there exists an assignment to the variables that satisfies the entire formula. In Unique k-SAT, the additional constraint is that the boolean formula has at most one satisfying assignment.",
              "The general SAT problem is as follows. Given are n Boolean variables x1, . . . , xn, and a formula f(x1, . . . , xn) = (a1,1x1 [?] * * * [?] a1,nxn) [?] * * * [?] (am,1x1 [?] * * * [?] am,nxn), (1) where by ai,j [?] {[?]1, 0, 1} we denote that in each clause i (i.e. the each disjunction contained in parentheses as written above) each variable xi may appear negated, not negated, or not at all.",
              "The formula f is thus the conjunction of m clauses, each of which are disjunctions of at most n literals, and is therefore said to be in conjunctive normal form. The SAT problem is that of determining whether there is an assignment to the variables x1, . . . , xn such that f(x1, . . . , xn) evaluates to true.",
              "In the k-SAT problem, each clause is limited to being a disjunction of at most k literals. In this paper, we will focus on the Unique-k-SAT problem, in which we are promised that the formula in uniquely satisfiable -- that is, that f is true only for one set of values of x1, . . . , xn. The problem is this case is that of finding the satisfying assignment."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should first explain the brute force solution for solving this problem and its complexity.",
            "weight": 0.13333333333333333,
            "evidence": [
              "The naive solution to the Unique-k-SAT problem would be to simply iterate through all possible variable assignments until the satisfying assignment is found. As each variable in an assignment may be either true or false, there are 2n such assignments, and this algorithm will thus take O(2n ) time"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention that this problem is NP-complete since it is harder than the K-sat problem and quote relevant proof for it.",
            "weight": 0.13333333333333333,
            "evidence": [
              "1. **NP-Completeness**: Unique \\(k\\)-SAT is NP-complete for \\(k \\ge 3\\), meaning that it is as difficult to solve as the general \\(k\\)-SAT problem. 2. **Reduction from \\(k\\)-SAT**: There exists a randomized polynomial-time reduction from \\(k\\)-SAT to Unique \\(k\\)-SAT, showing that Unique \\(k\\)-SAT is at least as hard as \\(k\\)-SAT. 3. **Isolation Lemma**: A given satisfiable \\(k\\)-CNF can be efficiently probabilistically reduced to a uniquely satisfiable \\(k\\)-CNF with non-trivial, albeit exponentially small, success probability.4. **Complexity**: If Unique 3-SAT can be solved in time \\(2^{\\epsilon n}\\) for every \\(\\epsilon > 0\\), then so can \\(k\\)-SAT for all \\(k \\ge 3\\)."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should enumerate some existing solutions with their run-times for solving this problem.",
            "weight": 0.13333333333333333,
            "evidence": [
              "3.3 Deterministic Algorithms: Several deterministic algorithms have been developed for Unique k-SAT: - For Unique 3-SAT, there exists a deterministic algorithm running in time O(1.3071^n), where n is the number of variables [4].- For Unique 4-SAT, the best known deterministic algorithm runs in time O(1.4704^n) [5]. These algorithms are significantly faster than the best known algorithms for general k-SAT, showcasing the power of the uniqueness promise.",
              "Pudlak, Saks, and Zane [6] has been improved upon recently to produce a deterministic solution to the Unique-3-SAT solution that runs in O(1.307n ) time [7], as well as a probabilistic solution to the 4-SAT that runs in O(1.46981n ) time [4].",
              "2. **Randomized Algorithms:**- Randomized reductions, as suggested by the Valiant-Vazirani theorem, play an important role in solving Unique k-SAT by converting it to a simpler problem that can then be tackled using probabilistic methods[^2].",
              "5. **Quantum Solutions**: Grover's algorithm can be applied to solve Unique \\(k\\)-SAT with a competitive asymptotic running time of \\(O(2^{n/2})\\), which is competitive with the current best runtime for deterministic and randomized solutions."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could point to the fact that like other NP-hard problems, there may be instances of this problem that can be easily solved.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Not necessarily every instances of k-SAT, for a fixed k [?] 3 show worst-case behaviors.",
              "Research has shown that the average-case complexity of Unique k-SAT demonstrates interesting properties. Specifically, it has been studied under random distributions of problems, showing that the problem remains hard on average, under certain distributions[^4]."
            ]
          }
        ]
      }
    }
  },
  {
    "question": "what are the most important open challenges in using neural networks in combination with PDE solvers for fluid simulation?",
    "response": {
      "sections": [
        {
          "title": "Background: Neural Networks and PDE Solvers for Fluid Simulation",
          "text": "\nComputational fluid dynamics (CFD) has historically relied on solving the Navier-Stokes equations through numerical methods, but these approaches face significant challenges due to the inherent complexity of fluid behavior. Traditional PDE solvers struggle with \"high dimensionality and nonlinearity, which defy closed-form solutions and limit real-time optimization and control efforts\" <Paper corpusId=\"166227999\" paperTitle=\"(Brunton et al., 2019)\" isShortName></Paper>. This computational bottleneck has motivated researchers to explore alternative approaches that can enhance or complement conventional CFD methods.\n\nMachine learning, particularly neural networks, has emerged as a promising tool in fluid mechanics, offering new ways to process and extract meaningful information from vast amounts of fluid dynamics data. As Brunton et al. note, ML provides \"a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications\" <Paper corpusId=\"166227999\" paperTitle=\"(Brunton et al., 2019)\" isShortName></Paper>. However, a significant limitation is that many machine learning models for fluid simulation are \"purely data-driven (DD) and, therefore, ignore the knowledge base accumulated throughout the centuries\" <Paper corpusId=\"221266267\" paperTitle=\"(Iskhakov et al., 2020)\" isShortName></Paper>.\n\nThe integration of neural networks with physics-based PDE solvers represents a hybrid approach that aims to leverage the strengths of both methodologies. This combination seeks to address the limitations of pure data-driven approaches by incorporating established physical principles while simultaneously overcoming the computational barriers of traditional numerical methods. As Iskhakov et al. emphasize, \"the integration of the DD modelling with the knowledge base may tremendously improve and advance further development of the computational fluid dynamics\" <Paper corpusId=\"221266267\" paperTitle=\"(Iskhakov et al., 2020)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Brunton et al., 2019)",
              "snippets": [
                "The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications."
              ],
              "metadata": null
            },
            {
              "id": "(Iskhakov et al., 2020)",
              "snippets": [
                "Incorporating and enforcing known flow physics is a challenge and opportunity for machine learning (ML) algorithms since most of the developed ML models are purely data-driven (DD) and, therefore, ignore the knowledge base accumulated throughout the centuries [2].At the same time, traditional first principles fluid dynamics modelling (e.g. using the discretized Navier-Stokes equations) faces serious challenges associated with current equations-based analysis of fluids, including high dimensionality and nonlinearity, which defy closed-form solutions and limit real-time optimization and control efforts (Brunton et al., 2019).It is clear that the integration of the DD modelling with the knowledge base may tremendously improve and advance further development of the computational fluid dynamics."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Computational Efficiency Challenges",
          "text": "\nDespite their potential, integrating neural networks with PDE solvers for fluid simulation faces substantial computational efficiency hurdles. Traditional PDE solvers benefit from \"more than 50 years of optimization\" and often outperform neural network approaches in terms of raw computational speed <Paper corpusId=\"208799557\" paperTitle=\"(White et al., 2019)\" isShortName></Paper>. One of the most significant challenges is the high computational cost during training, which can be particularly resource-intensive for large-scale fluid dynamics problems <Paper corpusId=\"277419728\" paperTitle=\"(Dyyany et al., 2025)\" isShortName></Paper>.\n\nThe convergence of physics-driven neural network solvers is generally slower than with numerical solvers, and the residuals of the PDEs often remain higher <Paper corpusId=\"259949435\" paperTitle=\"(Lino et al., 2023)\" isShortName></Paper>. This can create a difficult trade-off between computational efficiency and accuracy. As Chen et al. demonstrate, \"optimizing globally-supported neural network weights takes longer than optimizing locally-supported grid values, even if there are fewer neural network weights than the number of grid nodes\" <Paper corpusId=\"252683040\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>. In their experiments, neural network optimization took around 30 minutes per timestep compared to less than 1 minute for the corresponding FEM simulation.\n\nNeural network architectures also face stability challenges during training. MLP-based Physics-Informed Neural Network (PINN) architectures \"often fail to achieve stable training and produce accurate predictions\" <Paper corpusId=\"272986703\" paperTitle=\"(Nagda et al., 2024)\" isShortName></Paper>. This pathological behavior has been attributed to \"multi-scale interactions between different terms in the PINNs loss function, ultimately leading to stiffness in the gradient flow dynamics,\" which subsequently introduces \"stringent stability requirements on the learning rate\" <Paper corpusId=\"272986703\" paperTitle=\"(Nagda et al., 2024)\" isShortName></Paper> <Paper corpusId=\"237386144\" paperTitle=\"(Krishnapriyan et al., 2021)\" isShortName></Paper>.\n\nThe challenge is particularly acute in fluid dynamics because the underlying parameter-to-observable map typically has low regularity (at most Lipschitz continuous). This limited smoothness makes the approximation task significantly harder for neural networks, requiring \"unrealistically large sizes for the approximating networks\" that are \"hard to train and very expensive to compute\" <Paper corpusId=\"71148525\" paperTitle=\"(Lye et al., 2019)\" isShortName></Paper>. Unlike many common big data applications, this creates a scenario where deep learning must accurately approximate maps of low regularity in a relatively data-poor regime.\n\nVarious approaches have been proposed to address these computational challenges, including locally adaptive activation functions <Paper corpusId=\"259949435\" paperTitle=\"(Lino et al., 2023)\" isShortName></Paper> <Paper corpusId=\"260539457\" paperTitle=\"(Jagtap et al., 2020)\" isShortName></Paper>, variational formulations of the loss function <Paper corpusId=\"259949435\" paperTitle=\"(Lino et al., 2023)\" isShortName></Paper> <Paper corpusId=\"209376746\" paperTitle=\"(Khodayi-mehr et al., 2019)\" isShortName></Paper>, and hybrid mesh-neural spatial representations that aim for \"the best of both worlds\" in terms of memory, accuracy, and adaptivity <Paper corpusId=\"252683040\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>. However, the sensitivity to hyperparameter selection remains a significant hurdle, as performance can be highly dependent on the choice of network architecture, learning rates, and other hyperparameters <Paper corpusId=\"277419728\" paperTitle=\"(Dyyany et al., 2025)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(White et al., 2019)",
              "snippets": [
                "These methods struggle to compete with the more than 50 years of optimization behind standard PDE solvers. Instead, methods in PDE solving may be more useful to the machine learning community for faster neural network training [7]."
              ],
              "metadata": null
            },
            {
              "id": "(Dyyany et al., 2025)",
              "snippets": [
                "Despite their strengths, neural network methods face challenges: \n\n\u2022 High Computational Costs During Training: Training deep neural networks can be resource-intensive, especially for large-scale problems [9]. \n\u2022 Sensitivity to Hyperparameter Selection: Performance can be highly dependent on the choice of network architecture, learning rates, and other hyperparameters [9]. \n\u2022 Lack of Robust Theoretical Error Bounds: Neural network-based methods currently lack the comprehensive error analysis available for classical numerical methods [1]."
              ],
              "metadata": null
            },
            {
              "id": "(Lino et al., 2023)",
              "snippets": [
                "The current most significant challenge is that the convergence of physics-driven solvers is generally slower than with numerical solvers, and the residuals of the PDEs remain higher (Jagtap et al., 2019)(Jagtap et al., 2020). Efforts are being made to overcome these problems, for instance, by the design of globally (Jagtap et al., 2019) and locally (Jagtap et al., 2020) adaptive activation functions, and the use of loss functions derived from variational approaches [111](Khodayi-mehr et al., 2019)(Kharazmi et al., 2020)."
              ],
              "metadata": null
            },
            {
              "id": "(Chen et al., 2022)",
              "snippets": [
                "Compared to classic explicit representations (e.g., grids and meshes), our approach offers improved accuracy, reduced memory, and automatic adaptivity. ) Optimizing globally-supported neural network weights takes longer than optimizing locally-supported grid values, even if there are fewer neural network weights than the number of grid nodes. For instance, for the bunny example (Figure 8), our neural network optimization takes around 30 minutes per timestep while the corresponding FEM simulation takes less than 1 minute.\n\nFacing this wall clock vs. memory/accuracy/adaptivity trade-off, we believe an exciting future direction is hybrid mesh-neural spatial representations that aim for the best of both worlds",
                "More challenging physical phenomena, such as turbulence and intricate contacts, are also important future directions. Currently, our work enforces \"soft\" boundary conditions. \n\nEnforcing \"hard\" boundary conditions on a neural network is another exciting direction (Lu et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Nagda et al., 2024)",
              "snippets": [
                "However, in practice MLP-based PINN architectures often fail to achieve stable training and produce accurate predictions. \n\nWang et al. (Wang et al., 2021) attributed this pathological behavior to multi-scale interactions between different terms in the PINNs loss function, ultimately leading to stiffness in the gradient flow dynamics, which, consequently, introduces stringent stability requirements on the learning rate. Other work has indicated certain inherent failure modes, particularly when confronted with PDEs having highfrequency or multiscale features (Raissi, 2018)(Fuks et al., 2020)(Krishnapriyan et al., 2021)(Wang et al., 2020)(Zhao et al., 2023)[22]. In these cases, the predictions collapse to overly smooth trivial solutions. \n\nThis challenge has prompted investigations from four different perspectives: 1. employing different training schemes, 2. using data interpolation techniques, 3. designing new model architectures, or 4. incorporating dependencies present in the domain (Zhao et al., 2023). Different training schemes (Mao et al., 2020)(Krishnapriyan et al., 2021)(Wang et al., 2021)(Wang et al., 2020) can incur high computational costs. For example, Seq2Seq by Krishnapriyan et al. (Krishnapriyan et al., 2021) requires training multiple neural networks sequentially, while other approaches face convergence issues due to error accumulation. Data interpolation strategies [15,23,38,40] employ regularization based on simulations or real-world scenarios [47]8], but acquiring ground truth data remains challenging."
              ],
              "metadata": null
            },
            {
              "id": "(Krishnapriyan et al., 2021)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Lye et al., 2019)",
              "snippets": [
                "The goal in many CFD simulations is the computation of observables (functionals or quantities of interest) for the solutions of nonlinear PDEs that govern fluid flows. Many interesting problems in CFD such as uncertainty quantification (UQ), Bayesian inversion, optimal control and shape optimization require a large number of forward solves of the underlying PDE in order to compute (statistics of) the observable. However, each high-resolution CFD solve can be expensive, even on state of the art HPC systems. Hence, the afore mentioned problems are considered to be prohibitively computationally expensive. Instead of computing every realization of the underlying input parameters to observable map (2.4), we propose in this paper, to harness the power of machine learning and learn or approximate this map by a deep artificial neural network of the form (2.6). However and as explained in section 2.3, the task of finding the correct network architecture and training a network that generalizes well is very challenging in this context, on account of the following hurdles,\n\n\u2022 The underlying parameters to observable map (2.3) is not very regular (smooth) for most problems of interest in fluid dynamics. A priori, one expects that the underlying map is at most Lipschitz continuous (see theorem 2.4). Consequently, applying results on approximation of functions by neural networks (see theorem 2.2) leads to unrealistically large sizes for the approximating networks. Such networks are hard to train and very expensive to compute. Thus, we are confronted with the challenge of finding deep neural networks that can accurately approximate maps of low regularity in a relatively data poor regime. This is in stark contrast to many common big data applications of deep learning."
              ],
              "metadata": null
            },
            {
              "id": "(Jagtap et al., 2020)",
              "snippets": [
                "We propose two approaches of locally adaptive activation functions namely, layer-wise and neuron-wise locally adaptive activation functions, which improve the performance of deep and physics-informed neural networks. The local adaptation of activation function is achieved by introducing a scalable parameter in each layer (layer-wise) and for every neuron (neuron-wise) separately, and then optimizing it using a variant of stochastic gradient descent algorithm. In order to further increase the training speed, an activation slope-based slope recovery term is added in the loss function, which further accelerates convergence, thereby reducing the training cost. On the theoretical side, we prove that in the proposed method, the gradient descent algorithms are not attracted to sub-optimal critical points or local minima under practical conditions on the initialization and learning rate, and that the gradient dynamics of the proposed method is not achievable by base methods with any (adaptive) learning rates. We further show that the adaptive activation methods accelerate the convergence by implicitly multiplying conditioning matrices to the gradient of the base method without any explicit computation of the conditioning matrix and the matrix\u2013vector product. The different adaptive activation functions are shown to induce different implicit conditioning matrices. Furthermore, the proposed methods with the slope recovery are shown to accelerate the training process."
              ],
              "metadata": null
            },
            {
              "id": "(Khodayi-mehr et al., 2019)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Accuracy and Convergence Challenges",
          "text": "\nA fundamental challenge in combining neural networks with PDE solvers for fluid simulation is the lack of theoretical guarantees regarding error bounds and convergence. As Meer et al. note, \"error bounds and convergence guarantees are currently non-existent\" <Paper corpusId=\"211132737\" paperTitle=\"(Meer et al., 2020)\" isShortName></Paper>. This theoretical gap raises concerns about the reliability of neural network solutions, particularly in safety-critical applications where accuracy is paramount.\n\nThe convergence issues are especially pronounced when dealing with the complex dynamics of fluid systems. Li et al. emphasize that \"the convergence and accuracy of PINNs are still of tremendous challenge, especially for Navier-Stokes with multi-scale characteristics\" <Paper corpusId=\"252140234\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. This problem is compounded by the fact that \"the convergence of physics-driven solvers is generally slower than with numerical solvers, and the residuals of the PDEs remain higher\" <Paper corpusId=\"259949435\" paperTitle=\"(Lino et al., 2023)\" isShortName></Paper>.\n\nSpecific failure modes occur when physics-informed neural networks encounter certain types of fluid dynamics problems. Standard MLP-based PINN architectures \"often fail to achieve stable training and produce accurate predictions\" <Paper corpusId=\"272986703\" paperTitle=\"(Nagda et al., 2024)\" isShortName></Paper>. These failures are particularly evident when dealing with \"PDEs having high-frequency or multiscale features\" where \"the predictions collapse to overly smooth trivial solutions\" <Paper corpusId=\"272986703\" paperTitle=\"(Nagda et al., 2024)\" isShortName></Paper> <Paper corpusId=\"221835740\" paperTitle=\"(Fuks et al., 2020)\" isShortName></Paper> <Paper corpusId=\"237386144\" paperTitle=\"(Krishnapriyan et al., 2021)\" isShortName></Paper> <Paper corpusId=\"260125192\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"523487\" paperTitle=\"(Raissi, 2018)\" isShortName></Paper>.\n\nThe physical nature of the problems creates inherent difficulties for neural networks. For instance, when solving nonlinear hyperbolic conservation equations that involve shock waves\u2014common in compressible fluid flows\u2014standard PINN approaches perform poorly. Fuks et al. found that \"PIML strategies that employ the nonlinear hyperbolic conservation equation in the loss function are inadequate\" <Paper corpusId=\"221835740\" paperTitle=\"(Fuks et al., 2020)\" isShortName></Paper>. The researchers discovered that introducing a small amount of artificial diffusion into the equations could help neural networks learn more accurate approximations of solutions containing shocks.\n\nSeveral approaches have been proposed to address these accuracy and convergence challenges. These include \"the design of globally and locally adaptive activation functions, and the use of loss functions derived from variational approaches\" <Paper corpusId=\"259949435\" paperTitle=\"(Lino et al., 2023)\" isShortName></Paper> <Paper corpusId=\"209376746\" paperTitle=\"(Khodayi-mehr et al., 2019)\" isShortName></Paper> <Paper corpusId=\"260539457\" paperTitle=\"(Jagtap et al., 2020)\" isShortName></Paper>. More recent research has explored alternative architectures such as transformer-based models that can better capture temporal dependencies in physical systems <Paper corpusId=\"260125192\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\nDespite these accuracy limitations, neural network approaches may still be valuable in specific contexts. As Meer et al. suggest, \"these methods should not be used to solve a single problem instance of a PDE for which traditional methods are available. However, these methods can likely be used to solve parametrized families of problems\" <Paper corpusId=\"211132737\" paperTitle=\"(Meer et al., 2020)\" isShortName></Paper>. This indicates that while neural networks may not replace traditional numerical solvers for standard fluid simulation tasks, they could offer advantages for parametric studies or design optimization scenarios where multiple related simulations are required.",
          "citations": [
            {
              "id": "(Meer et al., 2020)",
              "snippets": [
                "Other open problems that remain have already been identified by the authors of the original study of [1]: error bounds and convergence guarantees are currently non-existent. The authors also suggest that these methods are not replacements for traditional numerical solvers. Given the computational cost of training a neural network, we agree that these methods should not be used to solve a single problem instance of a PDE for which traditional methods are available. However, these methods can likely be used to solve parametrized families of problems."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al., 2022)",
              "snippets": [
                "However, it has been argued that the convergence and accuracy of PINNs are still of tremendous challenge, especially for Navier-Stokes with multi-scale characteristics."
              ],
              "metadata": null
            },
            {
              "id": "(Lino et al., 2023)",
              "snippets": [
                "The current most significant challenge is that the convergence of physics-driven solvers is generally slower than with numerical solvers, and the residuals of the PDEs remain higher (Jagtap et al., 2019)(Jagtap et al., 2020). Efforts are being made to overcome these problems, for instance, by the design of globally (Jagtap et al., 2019) and locally (Jagtap et al., 2020) adaptive activation functions, and the use of loss functions derived from variational approaches [111](Khodayi-mehr et al., 2019)(Kharazmi et al., 2020)."
              ],
              "metadata": null
            },
            {
              "id": "(Nagda et al., 2024)",
              "snippets": [
                "However, in practice MLP-based PINN architectures often fail to achieve stable training and produce accurate predictions. \n\nWang et al. (Wang et al., 2021) attributed this pathological behavior to multi-scale interactions between different terms in the PINNs loss function, ultimately leading to stiffness in the gradient flow dynamics, which, consequently, introduces stringent stability requirements on the learning rate. Other work has indicated certain inherent failure modes, particularly when confronted with PDEs having highfrequency or multiscale features (Raissi, 2018)(Fuks et al., 2020)(Krishnapriyan et al., 2021)(Wang et al., 2020)(Zhao et al., 2023)[22]. In these cases, the predictions collapse to overly smooth trivial solutions. \n\nThis challenge has prompted investigations from four different perspectives: 1. employing different training schemes, 2. using data interpolation techniques, 3. designing new model architectures, or 4. incorporating dependencies present in the domain (Zhao et al., 2023). Different training schemes (Mao et al., 2020)(Krishnapriyan et al., 2021)(Wang et al., 2021)(Wang et al., 2020) can incur high computational costs. For example, Seq2Seq by Krishnapriyan et al. (Krishnapriyan et al., 2021) requires training multiple neural networks sequentially, while other approaches face convergence issues due to error accumulation. Data interpolation strategies [15,23,38,40] employ regularization based on simulations or real-world scenarios [47]8], but acquiring ground truth data remains challenging."
              ],
              "metadata": null
            },
            {
              "id": "(Fuks et al., 2020)",
              "snippets": [
                "Deep learning techniques have recently been applied to a wide range of computational physics problems. In this paper, we focus on developing a physics-based approach that enables the neural network to learn the solution of a dynamic \ufb02uid-\ufb02ow problem governed by a nonlinear partial differential equation (PDE). The main idea of physics informed machine learning (PIML) approaches is to encode the underlying physical law (i.e., the PDE) into the neural network as prior information. We investigate the applicability of the PIML approach to the forward problem of immiscible two-phase \ufb02uid transport in porous media, which is governed by a nonlinear \ufb01rst-order hyperbolic PDE subject to initial and boundary data. We employ the PIML strategy to solve this forward problem without any additional labeled data in the interior of the domain. Particularly, we are interested in non-convex \ufb02ux functions in the PDE, where the solution involves shocks and mixed waves (shocks and rarefactions). We have found that such a PIML approach fails to provide reasonable approximations to the solution in the presence of shocks in the saturation \ufb01eld. We investigated several architectures and experimented with a large number of neural-network parameters, and the overall \ufb01nding is that PIML strategies that employ the nonlinear hyperbolic conservation equation in the loss function are inadequate. However, we have found that employing a parabolic form of the conservation equation, whereby a small amount of diffusion is added, the neural network is consistently able to learn accurate approximation of the solutions containing shocks and mixed waves."
              ],
              "metadata": null
            },
            {
              "id": "(Krishnapriyan et al., 2021)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Zhao et al., 2023)",
              "snippets": [
                "Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions to partial differential equations (PDEs). However, conventional PINNs, relying on multilayer perceptrons (MLP), neglect the crucial temporal dependencies inherent in practical physics systems and thus fail to propagate the initial condition constraints globally and accurately capture the true solutions under various scenarios. In this paper, we introduce a novel Transformer-based framework, termed PINNsFormer, designed to address this limitation. PINNsFormer can accurately approximate PDE solutions by utilizing multi-head attention mechanisms to capture temporal dependencies. PINNsFormer transforms point-wise inputs into pseudo sequences and replaces point-wise PINNs loss with a sequential loss. Additionally, it incorporates a novel activation function, Wavelet, which anticipates Fourier decomposition through deep neural networks. Empirical results demonstrate that PINNsFormer achieves superior generalization ability and accuracy across various scenarios, including PINNs failure modes and high-dimensional PDEs. Moreover, PINNsFormer offers flexibility in integrating existing learning schemes for PINNs, further enhancing its performance."
              ],
              "metadata": null
            },
            {
              "id": "(Raissi, 2018)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Khodayi-mehr et al., 2019)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Jagtap et al., 2020)",
              "snippets": [
                "We propose two approaches of locally adaptive activation functions namely, layer-wise and neuron-wise locally adaptive activation functions, which improve the performance of deep and physics-informed neural networks. The local adaptation of activation function is achieved by introducing a scalable parameter in each layer (layer-wise) and for every neuron (neuron-wise) separately, and then optimizing it using a variant of stochastic gradient descent algorithm. In order to further increase the training speed, an activation slope-based slope recovery term is added in the loss function, which further accelerates convergence, thereby reducing the training cost. On the theoretical side, we prove that in the proposed method, the gradient descent algorithms are not attracted to sub-optimal critical points or local minima under practical conditions on the initialization and learning rate, and that the gradient dynamics of the proposed method is not achievable by base methods with any (adaptive) learning rates. We further show that the adaptive activation methods accelerate the convergence by implicitly multiplying conditioning matrices to the gradient of the base method without any explicit computation of the conditioning matrix and the matrix\u2013vector product. The different adaptive activation functions are shown to induce different implicit conditioning matrices. Furthermore, the proposed methods with the slope recovery are shown to accelerate the training process."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Generalization and Physical Consistency Challenges",
          "text": "\nA significant limitation of neural network approaches for fluid simulation is their poor generalization capability, particularly when faced with out-of-distribution (OoD) scenarios. Liu et al. note that \"deep learning methods learn patterns directly from the data instead of studying problem-related constraints beyond the PDEs, they cannot grasp the physics of the problem at hand\" <Paper corpusId=\"259202486\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This fundamental limitation leads to predictions that \"can significantly lack any physical significance\" when confronted with fluid scenarios that differ substantially from the training data.\n\nThe quality and quantity of training data directly impact the networks' generalization ability. Guo et al. emphasize that \"the neural network for solving PDEs relies heavily on training data, which often requires more training time when the quality of the training data is poor\" <Paper corpusId=\"225343753\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. This creates a challenging scenario where high-quality, diverse datasets are required to ensure models can handle a wide range of fluid dynamics scenarios. The problem is particularly acute with complex or turbulent flows, where capturing the full range of possible behaviors in a training dataset is often impractical.\n\nEven with advanced architectures, generalization issues persist. Recent approaches like PeRCNN, which uses \"feature map multiplication to construct polynomial combinations for approximating the underlying PDEs,\" demonstrate \"strong generalizability\" with low-resolution and noisy data but still \"suffer from error accumulation, degrading its performance in long-term predictions\" <Paper corpusId=\"275921303\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246430821\" paperTitle=\"(Rao et al., 2022)\" isShortName></Paper>. Similarly, hybrid approaches combining neural networks with numerical methods aim to leverage \"traditional solvers for stability and NNs for accuracy\" but \"often rely heavily on NN capabilities and often requires large amounts of data\" <Paper corpusId=\"275921303\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231741088\" paperTitle=\"(Kochkov et al., 2021)\" isShortName></Paper>.\n\nAnother critical challenge lies in ensuring the physical consistency and trustworthiness of neural network solutions. Ellinas et al. highlight that \"verification of Neural Networks (NNs) that approximate the solution of Partial Differential Equations (PDEs) is a major milestone towards enhancing their trustworthiness\" <Paper corpusId=\"267626904\" paperTitle=\"(Ellinas et al., 2024)\" isShortName></Paper>. This verification is particularly important for safety-critical applications but poses significant challenges as \"it is not straightforward how to efficiently bound them or how to represent the derivative of the NN.\"\n\nThe inherent mathematical nature of fluid dynamics problems further complicates the use of neural networks. Lye et al. explain that \"the underlying parameters to observable map is not very regular (smooth) for most problems of interest in fluid dynamics,\" with the map being \"at most Lipschitz continuous\" <Paper corpusId=\"71148525\" paperTitle=\"(Lye et al., 2019)\" isShortName></Paper>. This low regularity creates a fundamental challenge where networks need to \"accurately approximate maps of low regularity in a relatively data poor regime,\" a scenario that contrasts sharply with the data-rich environments where deep learning typically excels.\n\nTo address these generalization and physical consistency challenges, researchers have proposed various hybrid models that combine deep learning with computational fluid dynamics solvers <Paper corpusId=\"259202486\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. These approaches attempt to integrate the data-driven advantages of neural networks with the physical principles embedded in traditional solvers. Other promising directions include developing probabilistic neural networks that can \"give a probabilistic interpretation of the prediction results\" to better quantify uncertainty <Paper corpusId=\"225343753\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>, as well as model reduction techniques for high-dimensional problems <Paper corpusId=\"225343753\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Liu et al., 2023)",
              "snippets": [
                "However, a major limitation of these approaches is their tendency to suffer from poor generalization performance on out-of-distribution (OoD) samples. As the deep learning methods learn patterns directly from the data instead of studying problem-related constraints beyond the PDEs, they cannot grasp the physics of the problem at hand. The lack of underlying physical laws leads to generalization issues: the predicted results can significantly lack any physical significance, and the learned models often underperform on OoD samples: the deep learning models can predict unseen examples that are close to the training set but perform poorly at solving new problems that significantly differ from the training examples, as the underlying physical laws are not explicitly incorporated into the learning process. In order to address this issue, a number of researchers have proposed the use of hybrid models that combine deep learning with computational fluid dynamics (CFD) solvers."
              ],
              "metadata": null
            },
            {
              "id": "(Guo et al., 2020)",
              "snippets": [
                "The method also faces many problems, such as the neural network for solving PDEs relies heavily on training data, which often requires more training time when the quality of the training data is poor. Therefore, it is also important to investigate how to construct high-quality training datasets to reduce the training time. (5) In this study, we focus on solving PDEs by training physics-informed neural networks, which is a supervised learning task. Currently, several researchers have used unlabeled data to train physics-constrained deep learning models for high-dimensional problems and have quantified the uncertainty of the predictions (Zhu et al., 2019). This inspires us to further improve our neural network, so that it can be trained using unlabeled data and give a probabilistic interpretation of the prediction results [69]. Besides, this paper studies low-dimensional problems, whereas, for high-dimensional problems, model reduction [70] is also an important issue to consider when constructing a neural network model."
              ],
              "metadata": null
            },
            {
              "id": "(Wang et al., 2025)",
              "snippets": [
                "Recent advances in deep learning have introduced neuralbased approaches (Lu et al., 2019)(Li et al., 2020)(Gupta et al., 2022) for solving PDEs. These data-driven methods eliminate the need for explicit theoretical formulations, enabling networks to learn underlying patterns directly from data through end-to-end training. While promising, these approaches face notable challenges, including a heavy dependence on large training datasets and limited generalization. For instance, achieving accurate predictions becomes particularly challenging when models encounter unseen ICs or scenarios beyond the training distribution",
                "Additionally, optimizing complex loss functions (Rathore et al., 2024) and ensuring model interpretability remain challenges",
                "For instance, PeRCNN (Rao et al., 2022)(Kumar, 2023), which uses feature map multiplication to construct polynomial combinations for approximating the underlying PDEs, can capture the latent spatiotemporal dynamics even with low-resolution, noisy, and coarse data, demonstrating strong generalizability. Nevertheless, this method suffers from error accumulation, degrading its performance in long-term predictions. Another approach (Kochkov et al., 2021)(Sun et al., 2023), combining NNs with numerical methods, aims to accelerate the simulation process on coarse grids. These hybrid methods leverage traditional solvers for stability and NNs for accuracy. However, they often rely heavily on NN capabilities and often requires large amounts of data."
              ],
              "metadata": null
            },
            {
              "id": "(Rao et al., 2022)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Kochkov et al., 2021)",
              "snippets": [
                "Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier\u2013Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10\u00d7 finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization."
              ],
              "metadata": null
            },
            {
              "id": "(Ellinas et al., 2024)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Lye et al., 2019)",
              "snippets": [
                "The goal in many CFD simulations is the computation of observables (functionals or quantities of interest) for the solutions of nonlinear PDEs that govern fluid flows. Many interesting problems in CFD such as uncertainty quantification (UQ), Bayesian inversion, optimal control and shape optimization require a large number of forward solves of the underlying PDE in order to compute (statistics of) the observable. However, each high-resolution CFD solve can be expensive, even on state of the art HPC systems. Hence, the afore mentioned problems are considered to be prohibitively computationally expensive. Instead of computing every realization of the underlying input parameters to observable map (2.4), we propose in this paper, to harness the power of machine learning and learn or approximate this map by a deep artificial neural network of the form (2.6). However and as explained in section 2.3, the task of finding the correct network architecture and training a network that generalizes well is very challenging in this context, on account of the following hurdles,\n\n\u2022 The underlying parameters to observable map (2.3) is not very regular (smooth) for most problems of interest in fluid dynamics. A priori, one expects that the underlying map is at most Lipschitz continuous (see theorem 2.4). Consequently, applying results on approximation of functions by neural networks (see theorem 2.2) leads to unrealistically large sizes for the approximating networks. Such networks are hard to train and very expensive to compute. Thus, we are confronted with the challenge of finding deep neural networks that can accurately approximate maps of low regularity in a relatively data poor regime. This is in stark contrast to many common big data applications of deep learning."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Implementation and Technical Challenges",
          "text": "\nImplementing hybrid neural-physical models for fluid simulation involves substantial technical challenges beyond the theoretical concerns. One of the most significant barriers is the integration of neural networks with traditional computational fluid dynamics (CFD) solvers. As Fan et al. note, \"The integration of DNNs to construct hybrid neural solvers poses significant challenges for most traditional legacy solvers due to the lack of AD capability and GPU compatibility\" <Paper corpusId=\"257687228\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper>. While specialized differentiable fluid solvers like PhiFlow <Paper corpusId=\"209334533\" paperTitle=\"(Holl et al., 2020)\" isShortName></Paper> and Jax-CFD <Paper corpusId=\"231741088\" paperTitle=\"(Kochkov et al., 2021)\" isShortName></Paper> have been developed to address these challenges, \"this field is still in its early stages and requires further development, especially for modeling complex multi-physics problems such as two-way coupled FSI dynamics\" <Paper corpusId=\"257687228\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper>.\n\nThe lack of automatic differentiation support in existing solvers creates a significant technical roadblock. Ma et al. highlight that \"end-to-end training of such a PDE correction model over both solver-dependent parameters such as mesh parameters and neural network parameters requires the PDE solver to support automatic differentiation through the iterative numerical process. Such a feature is not readily available in many existing solvers\" <Paper corpusId=\"268033281\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. This limitation severely restricts the types of hybrid approaches that can be practically implemented without developing custom solvers from scratch.\n\nGrid and mesh representations present another major implementation challenge. For higher-dimensional PDEs such as the BGK-Boltzmann equations, \"grid-based representations would require intolerable memory footprints\" <Paper corpusId=\"251066808\" paperTitle=\"(Kang et al., 2022)\" isShortName></Paper>. Although potential solutions include tensor decomposition techniques <Paper corpusId=\"16074195\" paperTitle=\"(Kolda et al., 2009)\" isShortName></Paper> <Paper corpusId=\"247519170\" paperTitle=\"(Chen et al._1, 2022)\" isShortName></Paper>, data compression algorithms, or adaptive methods, integrating these with neural networks remains an \"exciting research area\" <Paper corpusId=\"251066808\" paperTitle=\"(Kang et al., 2022)\" isShortName></Paper>. The challenge is further complicated for irregular meshes, where extending spatial decomposition approaches becomes \"the non-trivial vertex coloring problem\" requiring \"heuristic coloring algorithms\" <Paper corpusId=\"253062211\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\nThe training process itself presents considerable technical challenges, as \"stable model training can be difficult depending on the base architecture\" <Paper corpusId=\"271710466\" paperTitle=\"(Musekamp et al., 2024)\" isShortName></Paper>. This is particularly problematic for iterative approaches like active learning, where models are trained repeatedly with different datasets. Furthermore, hyperparameter selection remains a significant concern, especially when models need to generalize to \"growing, unseen datasets\" <Paper corpusId=\"271710466\" paperTitle=\"(Musekamp et al., 2024)\" isShortName></Paper>. Extending these approaches to \"more complex geometries and boundary conditions, as well as irregular grids\" that would be needed for realistic engineering applications introduces additional implementation challenges <Paper corpusId=\"271710466\" paperTitle=\"(Musekamp et al., 2024)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Fan et al., 2023)",
              "snippets": [
                "The integration of DNNs to construct hybrid neural solvers poses significant challenges for most traditional legacy solvers due to the lack of AD capability and GPU compatibility. To overcome this issue, there are growing efforts to develop differentiable CFD solvers such as PhiFlow (Holl et al., 2020), Jax-CFD (Kochkov et al., 2021), and Jax-Fluids (Bezgin et al., 2022). Although the hybrid differentiable neural modeling paradigm has shown a great promise, this field is still in its early stages and requires further development, especially for modeling complex multi-physics problems such as two-way coupled FSI dynamics."
              ],
              "metadata": null
            },
            {
              "id": "(Holl et al., 2020)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Kochkov et al., 2021)",
              "snippets": [
                "Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier\u2013Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10\u00d7 finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization."
              ],
              "metadata": null
            },
            {
              "id": "(Ma et al., 2024)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Kang et al., 2022)",
              "snippets": [
                "Another natural question related to spatial complexity may arise. The proposed grid-based representations would require intolerable memory footprints for higher dimensional PDEs, such as BGK-Boltzmann equations. To achieve high accuracy, we may need to increase the grid size arbitrarily large. We believe these are open and exciting questions, and combining numerical methods and machine learning techniques may come to the rescue. For example, tensor decomposition techniques (Kolda et al., 2009)(Chen et al., 2022)), data compression algorithm (Le Gall 1991;(Wallace, 1991), or adaptive methods (Nochetto et al. 2009) are few good candidates. We believe we provide a good example that combines neural networks and grid representations for solving PDEs, and marrying existing techniques into the proposed algorithm will be an exciting research area."
              ],
              "metadata": null
            },
            {
              "id": "(Kolda et al., 2009)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Chen et al._1, 2022)",
              "snippets": [
                "We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB)."
              ],
              "metadata": null
            },
            {
              "id": "(Huang et al., 2023)",
              "snippets": [
                "One limitation of our work is that we only define the spatial decomposition over regular meshes, while it turns to the non-trivial vertex coloring problem for irregular meshes. Heuristic coloring algorithms would be useful for this problem, and we would like to explore it in future works."
              ],
              "metadata": null
            },
            {
              "id": "(Musekamp et al., 2024)",
              "snippets": [
                "However, the experiments also showed that stable model training can be difficult depending on the base architecture (2D CNS). Such issues especially impact AL since the model is trained repeatedly with different data sets, and the data selection relies on the model. Hence, more work on the reliability of the surrogate model training is necessary. Another general open issue of AL is the question of how to select hyperparameters that work sufficiently well on the growing, unseen datasets during AL. To be closer to realistic engineering applications, future work should also consider more complex geometries and boundary conditions, as well as irregular grids. AL could be especially helpful in such settings due to the inherently more complex input space from which to select."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Promising Future Directions",
          "text": "\nHere are the most promising future directions for integrating neural networks with PDE solvers in fluid simulation:\n\n1. **Hybrid Mesh-Neural Spatial Representations**: Developing representations that combine the advantages of both neural networks and traditional grid/mesh approaches could provide \"the best of both worlds in terms of memory, accuracy, and adaptivity\" while addressing the computational efficiency trade-offs between neural networks and traditional solvers <Paper corpusId=\"252683040\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>.\n\n2. **Enforcing \"Hard\" Boundary Conditions**: Moving beyond the current \"soft\" boundary condition approaches to develop techniques that can enforce hard boundary constraints in neural networks offers significant potential for improving physical consistency <Paper corpusId=\"252683040\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"231855433\" paperTitle=\"(Lu et al., 2021)\" isShortName></Paper>.\n\n3. **Probabilistic Neural Networks for Uncertainty Quantification**: Training models that can \"give a probabilistic interpretation of the prediction results\" will help quantify the uncertainty inherent in neural network fluid simulations <Paper corpusId=\"225343753\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\n4. **Model Reduction Techniques for High-Dimensional Problems**: Developing better model reduction approaches for high-dimensional fluid dynamics problems could make neural network approaches more practical for complex simulations <Paper corpusId=\"225343753\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\n5. **Advanced Tensor Decomposition and Data Compression**: Incorporating tensor decomposition techniques <Paper corpusId=\"251066808\" paperTitle=\"(Kang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"16074195\" paperTitle=\"(Kolda et al., 2009)\" isShortName></Paper> <Paper corpusId=\"247519170\" paperTitle=\"(Chen et al._1, 2022)\" isShortName></Paper>, data compression algorithms, and adaptive methods to address the memory footprint challenges of grid-based representations for higher-dimensional PDEs.\n\n6. **Differentiable CFD Solvers**: Further development of differentiable fluid solvers like PhiFlow and Jax-CFD to enable end-to-end training of hybrid neural-physical models, particularly for complex multi-physics problems such as two-way coupled fluid-structure interaction dynamics <Paper corpusId=\"257687228\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper> <Paper corpusId=\"209334533\" paperTitle=\"(Holl et al., 2020)\" isShortName></Paper> <Paper corpusId=\"231741088\" paperTitle=\"(Kochkov et al., 2021)\" isShortName></Paper>.\n\n7. **Physics-Informed Neural Networks for Irregular Meshes**: Extending spatial decomposition approaches to work effectively with irregular meshes by developing heuristic coloring algorithms <Paper corpusId=\"253062211\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n8. **Improved Training Stability**: Developing techniques to enhance the stability of model training, particularly for active learning approaches where models are trained repeatedly with different datasets <Paper corpusId=\"271710466\" paperTitle=\"(Musekamp et al., 2024)\" isShortName></Paper>.\n\n9. **Hyperparameter Selection for Growing Datasets**: Creating approaches for selecting hyperparameters that work well on expanding, unseen datasets during active learning <Paper corpusId=\"271710466\" paperTitle=\"(Musekamp et al., 2024)\" isShortName></Paper>.\n\n10. **Neural Network-Encoded Mathematical Expressions**: Encoding mathematical expressions of governing equations using neural networks to enlarge the class of equations that can be handled <Paper corpusId=\"264406190\" paperTitle=\"(Iwata et al., 2023)\" isShortName></Paper> <Paper corpusId=\"248366384\" paperTitle=\"(Kamienny et al., 2022)\" isShortName></Paper>.\n\n11. **Three-Dimensional High Reynolds Number Flows**: Applying neural-physical hybrid methods to tackle more complex three-dimensional fluid flows with high Reynolds numbers <Paper corpusId=\"252407685\" paperTitle=\"(Basir, 2022)\" isShortName></Paper>.\n\n12. **Investigation of Physics Impact on Backpropagated Gradients**: Further research into how physics affects the gradients backpropagated during training could lead to improved initialization schemes for enhancing neural network trainability <Paper corpusId=\"252407685\" paperTitle=\"(Basir, 2022)\" isShortName></Paper>.\n\n13. **Error Accumulation Mitigation in Long-Term Predictions**: Developing techniques to address the error accumulation problem in methods like PeRCNN that currently limits performance in long-term predictions <Paper corpusId=\"275921303\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246430821\" paperTitle=\"(Rao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"267149050\" paperTitle=\"(Kumar, 2023)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Chen et al., 2022)",
              "snippets": [
                "Compared to classic explicit representations (e.g., grids and meshes), our approach offers improved accuracy, reduced memory, and automatic adaptivity. ) Optimizing globally-supported neural network weights takes longer than optimizing locally-supported grid values, even if there are fewer neural network weights than the number of grid nodes. For instance, for the bunny example (Figure 8), our neural network optimization takes around 30 minutes per timestep while the corresponding FEM simulation takes less than 1 minute.\n\nFacing this wall clock vs. memory/accuracy/adaptivity trade-off, we believe an exciting future direction is hybrid mesh-neural spatial representations that aim for the best of both worlds",
                "More challenging physical phenomena, such as turbulence and intricate contacts, are also important future directions. Currently, our work enforces \"soft\" boundary conditions. \n\nEnforcing \"hard\" boundary conditions on a neural network is another exciting direction (Lu et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Lu et al., 2021)",
              "snippets": [
                "Inverse design arises in a variety of areas in engineering such as acoustic, mechanics, thermal/electronic transport, electromagnetism, and optics. Topology optimization is a major form of inverse design, where we optimize a designed geometry to achieve targeted properties and the geometry is parameterized by a density function. This optimization is challenging, because it has a very high dimensionality and is usually constrained by partial differential equations (PDEs) and additional inequalities. Here, we propose a new deep learning method -- physics-informed neural networks with hard constraints (hPINNs) -- for solving topology optimization. hPINN leverages the recent development of PINNs for solving PDEs, and thus does not rely on any numerical PDE solver. However, all the constraints in PINNs are soft constraints, and hence we impose hard constraints by using the penalty method and the augmented Lagrangian method. We demonstrate the effectiveness of hPINN for a holography problem in optics and a fluid problem of Stokes flow. We achieve the same objective as conventional PDE-constrained optimization methods based on adjoint methods and numerical PDE solvers, but find that the design obtained from hPINN is often simpler and smoother for problems whose solution is not unique. Moreover, the implementation of inverse design with hPINN can be easier than that of conventional methods."
              ],
              "metadata": null
            },
            {
              "id": "(Guo et al., 2020)",
              "snippets": [
                "The method also faces many problems, such as the neural network for solving PDEs relies heavily on training data, which often requires more training time when the quality of the training data is poor. Therefore, it is also important to investigate how to construct high-quality training datasets to reduce the training time. (5) In this study, we focus on solving PDEs by training physics-informed neural networks, which is a supervised learning task. Currently, several researchers have used unlabeled data to train physics-constrained deep learning models for high-dimensional problems and have quantified the uncertainty of the predictions (Zhu et al., 2019). This inspires us to further improve our neural network, so that it can be trained using unlabeled data and give a probabilistic interpretation of the prediction results [69]. Besides, this paper studies low-dimensional problems, whereas, for high-dimensional problems, model reduction [70] is also an important issue to consider when constructing a neural network model."
              ],
              "metadata": null
            },
            {
              "id": "(Kang et al., 2022)",
              "snippets": [
                "Another natural question related to spatial complexity may arise. The proposed grid-based representations would require intolerable memory footprints for higher dimensional PDEs, such as BGK-Boltzmann equations. To achieve high accuracy, we may need to increase the grid size arbitrarily large. We believe these are open and exciting questions, and combining numerical methods and machine learning techniques may come to the rescue. For example, tensor decomposition techniques (Kolda et al., 2009)(Chen et al., 2022)), data compression algorithm (Le Gall 1991;(Wallace, 1991), or adaptive methods (Nochetto et al. 2009) are few good candidates. We believe we provide a good example that combines neural networks and grid representations for solving PDEs, and marrying existing techniques into the proposed algorithm will be an exciting research area."
              ],
              "metadata": null
            },
            {
              "id": "(Kolda et al., 2009)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Chen et al._1, 2022)",
              "snippets": [
                "We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB)."
              ],
              "metadata": null
            },
            {
              "id": "(Fan et al., 2023)",
              "snippets": [
                "The integration of DNNs to construct hybrid neural solvers poses significant challenges for most traditional legacy solvers due to the lack of AD capability and GPU compatibility. To overcome this issue, there are growing efforts to develop differentiable CFD solvers such as PhiFlow (Holl et al., 2020), Jax-CFD (Kochkov et al., 2021), and Jax-Fluids (Bezgin et al., 2022). Although the hybrid differentiable neural modeling paradigm has shown a great promise, this field is still in its early stages and requires further development, especially for modeling complex multi-physics problems such as two-way coupled FSI dynamics."
              ],
              "metadata": null
            },
            {
              "id": "(Holl et al., 2020)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Kochkov et al., 2021)",
              "snippets": [
                "Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier\u2013Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10\u00d7 finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization."
              ],
              "metadata": null
            },
            {
              "id": "(Huang et al., 2023)",
              "snippets": [
                "One limitation of our work is that we only define the spatial decomposition over regular meshes, while it turns to the non-trivial vertex coloring problem for irregular meshes. Heuristic coloring algorithms would be useful for this problem, and we would like to explore it in future works."
              ],
              "metadata": null
            },
            {
              "id": "(Musekamp et al., 2024)",
              "snippets": [
                "However, the experiments also showed that stable model training can be difficult depending on the base architecture (2D CNS). Such issues especially impact AL since the model is trained repeatedly with different data sets, and the data selection relies on the model. Hence, more work on the reliability of the surrogate model training is necessary. Another general open issue of AL is the question of how to select hyperparameters that work sufficiently well on the growing, unseen datasets during AL. To be closer to realistic engineering applications, future work should also consider more complex geometries and boundary conditions, as well as irregular grids. AL could be especially helpful in such settings due to the inherently more complex input space from which to select."
              ],
              "metadata": null
            },
            {
              "id": "(Iwata et al., 2023)",
              "snippets": [
                "First, we plan to enlarge the class of governing equations by encoding their mathematical expressions using neural networks (Kamienny et al., 2022). Second, we will use our method for discovering governing equations from observational data by estimating the coefficients of governing equations in our model."
              ],
              "metadata": null
            },
            {
              "id": "(Kamienny et al., 2022)",
              "snippets": [
                "Symbolic regression, the task of predicting the mathematical expression of a function from the observation of its values, is a difficult task which usually involves a two-step procedure: predicting the\"skeleton\"of the expression up to the choice of numerical constants, then fitting the constants by optimizing a non-convex loss function. The dominant approach is genetic programming, which evolves candidates by iterating this subroutine a large number of times. Neural networks have recently been tasked to predict the correct skeleton in a single try, but remain much less powerful. In this paper, we challenge this two-step procedure, and task a Transformer to directly predict the full mathematical expression, constants included. One can subsequently refine the predicted constants by feeding them to the non-convex optimizer as an informed initialization. We present ablations to show that this end-to-end approach yields better results, sometimes even without the refinement step. We evaluate our model on problems from the SRBench benchmark and show that our model approaches the performance of state-of-the-art genetic programming with several orders of magnitude faster inference."
              ],
              "metadata": null
            },
            {
              "id": "(Basir, 2022)",
              "snippets": [
                "In future research, our method will be applied to tackle three-dimensional fluid flows with high Reynolds numbers. We plan to further investigate the impact of physics on contaminating backpropagated gradients to develop improved initialization schemes for enhancing the trainability of neural networks on physics."
              ],
              "metadata": null
            },
            {
              "id": "(Wang et al., 2025)",
              "snippets": [
                "Recent advances in deep learning have introduced neuralbased approaches (Lu et al., 2019)(Li et al., 2020)(Gupta et al., 2022) for solving PDEs. These data-driven methods eliminate the need for explicit theoretical formulations, enabling networks to learn underlying patterns directly from data through end-to-end training. While promising, these approaches face notable challenges, including a heavy dependence on large training datasets and limited generalization. For instance, achieving accurate predictions becomes particularly challenging when models encounter unseen ICs or scenarios beyond the training distribution",
                "Additionally, optimizing complex loss functions (Rathore et al., 2024) and ensuring model interpretability remain challenges",
                "For instance, PeRCNN (Rao et al., 2022)(Kumar, 2023), which uses feature map multiplication to construct polynomial combinations for approximating the underlying PDEs, can capture the latent spatiotemporal dynamics even with low-resolution, noisy, and coarse data, demonstrating strong generalizability. Nevertheless, this method suffers from error accumulation, degrading its performance in long-term predictions. Another approach (Kochkov et al., 2021)(Sun et al., 2023), combining NNs with numerical methods, aims to accelerate the simulation process on coarse grids. These hybrid methods leverage traditional solvers for stability and NNs for accuracy. However, they often rely heavily on NN capabilities and often requires large amounts of data."
              ],
              "metadata": null
            },
            {
              "id": "(Rao et al., 2022)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Kumar, 2023)",
              "snippets": [
                "This article presents a brief study on the review of the Burgers' equation. It also gives some concepts/ideas and techniques to solve Burgers' equation. Applying Burgers' equation to traffic flow requires concentrated effort for the solution. We develop our insights on how to obtain the Navier-Stokes equation through our inquiry into Burgers' equation. We also demonstrate how the Cole-Hopf transformation for the viscous Burgers' equation is derived. Finally, we use Burger's equation function as a model for the flow of traffic. Additionally, by employing the linear system method, we are able to obtain the answer to the one-way traffic flow problem. The Navier-Stokes equation has been derived to get in-viscid Burger's equation. The principle of Traffic flow and Navier-Stokes models have also been derived."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what are the most important open challenges in using neural networks in combination with PDE solvers for fluid simulation?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should highlight the difficulty in numerically solving PDEs that arise in fluid simulation, and the increasing application of neural networks to approximate solutions to these PDEs.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivate the use of deep learning approaches to approximate the PDE solutions",
              "Convolutional neural networks (CNNs) have been shown to achieve a speedup of between two and four orders of magnitude over numerical flow solvers [40-42]."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain how DL methods are applied in this area and note the different DL models (CNNs, GNNs, PINNs) that have been used for approximating solutions to PDE problems.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Data-driven neural solvers learn from observations of fluid dynamics, usually from large datasets generated by numerical simulation, without (in general) any knowledge of the governing equations.",
              "The type of DL model employed for data-driven solvers primarily depends on the nature of the physical problem, with CNNs being a natural choice for simulations where the geometry aligns with the coordinate system, while GNNs are preferred for simulations of more complex geometries and Lagrangian systems.",
              "More recently,physics-informed neural networks(PINNs) have been developed to solve forward and inverse problems where full or partial knowledge of the governing equations is known [23,24,29,30,103,104]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss the challenges of using neural networks for solving PDE problems such as large state space, lack of generalizability to unseen scenarios, computationally intensive training time, and the need for large training datasets.",
            "weight": 0.17142857142857143,
            "evidence": [
              "### 1. High-Dimensionality and Scale**Problem:** Fluid simulations often deal with high-dimensional spaces, particularly in three-dimensional simulations. Neural networks used for such tasks need to cater to vast input and output data scales.**Impact:** Handling high-dimensional input data while ensuring computational tractability remains a significant obstacle. The curse of dimensionality can lead to increased model complexity and training data requirements.",
              "### 2. Generalization and Extrapolation**Problem:** Neural networks often struggle to generalize beyond the training data and may fail to extrapolate well to unseen conditions, which is problematic for fluid simulations that often involve varied and complex domains.**Impact:** Ensuring reliable performance across different fluid flows, geometries, and boundary conditions remains unsolved",
              "### 3. Data Efficiency and Scarcity**Problem:** Generating the high-fidelity simulation data needed to train neural networks can be prohibitively expensive and time-consuming.**Impact:** NNs require large amounts of training data to generalize well. Data-efficient learning methods are critically needed to make the neural network approach feasible for fluid simulations.",
              "### 5. Interpretability and Physics Consistency**Problem:** Neural networks often function as black boxes, providing little insight into how they arrive at their solutions or maintaining physical constraints.**Impact:** Understanding and ensuring that neural network predictions adhere to the underlying physical laws described by the PDEs is crucial for their effective application in fluid simulations."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could provide solutions to some of the discussed challenges such as training and using DL methods for a specific domain and not as an end-to-end solution, and innovative training methods.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Addressing these generalization and adaptability challenges requires innovative approaches in model architecture, training strategies, and data generation. Researchers are exploring techniques such as physics-informed regularization, multi-fidelity training, and adaptive sampling to improve the robustness and transferability of neural network models in fluid simulations [LLM MEMORY | 2024]. However, significant work remains to be done to achieve the level of generalization and adaptability required for widespread adoption in complex fluid dynamics applications.",
              "As a consequence of these challenges, end-to-end flow solvers using DL are not yet reliable for most general-purpose simulations and long-term predictions. However, DL is proving a valuable component to help address some of the numerical challenges of conventional fluid dynamics simulators"
            ]
          }
        ]
      }
    }
  },
  {
    "question": "What are advantages and disadvantages of top methods for picking the right number of topics in topic modeling?",
    "response": {
      "sections": [
        {
          "title": "Background on Topic Modeling and Number Selection",
          "text": "\nTopic modeling is a statistical approach to discover abstract themes or \"topics\" within a collection of documents. One of the most widely used techniques is Latent Dirichlet Allocation (LDA), which assumes documents are mixtures of topics, with each topic being a distribution over words. A critical challenge in topic modeling is determining the optimal number of topics (K) to generate meaningful and interpretable results.\n\nLDA requires the specification of several hyperparameters, including K (the number of topics), \u03b1 (expectation of topics per document), and \u03b2 (word distribution for each topic). Among these, selecting the appropriate value for K is particularly crucial for obtaining interpretable outputs <Paper corpusId=\"257353421\" paperTitle=\"(Mulahuwaish et al., 2023)\" isShortName></Paper>. When K is set too high, the resulting topics often have substantial overlap, while a low K value leads to overgeneralization, forcing diverse content into a limited number of topics <Paper corpusId=\"257353421\" paperTitle=\"(Mulahuwaish et al., 2023)\" isShortName></Paper>.\n\nThe selection of an optimal number of topics represents a longstanding challenge in the field. As Pan et al. explain, \"Too many topics can lead to small topic generalization scope and minor semantic content differences, making topic division difficult. Conversely, too few topics can result in overly broad semantic content generalization, neglecting smaller topics\" <Paper corpusId=\"265344346\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper>. The number of topics directly affects both accuracy and interpretability of the modeling results.\n\nTraditional topic modeling techniques like LDA and Probabilistic Latent Semantic Analysis (PLSA) are generative models that attempt to recreate original document word distributions with minimal loss. However, these approaches have several limitations. They tend to place uninformative words in topics with high probability since these words make up a large proportion of documents. Additionally, there's no guarantee that they will find topics that are truly representative of the corpus <Paper corpusId=\"221246303\" paperTitle=\"(Angelov, 2020)\" isShortName></Paper>.\n\nLDA specifically suffers from multiple shortcomings beyond the difficulty in setting the parameter K. These include deficiencies in handling short texts, challenges in capturing contextual meaning of sentences <Paper corpusId=\"253280653\" paperTitle=\"(Xu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"195886143\" paperTitle=\"(Dieng et al., 2019)\" isShortName></Paper>, and an inability to model topic correlations and evolution over time <Paper corpusId=\"253280653\" paperTitle=\"(Xu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nNewer approaches have emerged to address these limitations. For example, Tang et al. proposed using topic diversity as a criterion for selecting the optimal number of topics. They observed that with too few topics, learned topics tend to be close to the background language model and don't distinguish well between each other. As the number increases, topics become more distinguishable with finer granularity. However, when the number becomes too large, many small, closely related topics emerge, decreasing overall diversity <Paper corpusId=\"18878266\" paperTitle=\"(Tang et al., 2014)\" isShortName></Paper>.\n\nMore recent methods like Top2vec offer advantages over traditional approaches by automatically finding the appropriate number of topics and identifying more informative and representative topics. Top2vec leverages distributed representations of words, which helps overcome challenges associated with bag-of-words representations that ignore word semantics <Paper corpusId=\"221246303\" paperTitle=\"(Angelov, 2020)\" isShortName></Paper>.\n\nOther innovative approaches include methods that estimate the number of topics K from observed data rather than requiring it as a predefined parameter. Bing et al. developed an estimator that is adaptive to varying numbers of documents, document lengths, dictionary sizes, and topic counts <Paper corpusId=\"21694446\" paperTitle=\"(Bing et al., 2018)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Mulahuwaish et al., 2023)",
              "snippets": [
                "The LDA algorithm considers three hyperparameters: a, b, and K. The a hyperparameter encodes the number of topics expected in each document. The b hyperparameter lets it know the distribution of words for each topic in the document, and K defines the number of topics to use. Picking an accurate K value is crucial to an interpretable output from the algorithm. Choosing a high value may result in an output with topics with substantial overlap. Conversely, a low K would generalize too strongly and place most of the meaning behind a document into a smaller pool of topics."
              ],
              "metadata": null
            },
            {
              "id": "(Pan et al., 2023)",
              "snippets": [
                "The selection of the optimal number of topics is based on quality evaluation methods of topic models, and it's a longstanding challenge. Too many topics can lead to small topic generalization scope and minor semantic content differences, making topic division difficult. Conversely, too few topics can result in overly broad semantic content generalization, neglecting smaller topics. The selection of the optimal number of topics directly affects the accuracy and interpretability of LDA topic modelling results. Currently, researchers often estimate the initial number of topics contained in documents based on prior knowledge and then select the optimal number of topics using quality evaluation methods like perplexity, coherence, and topic similarity. This paper provides a summarized comparison of the core ideas and advantages and disadvantages of several typical methods for determining the optimal number of topics, as shown in Table 2. \n\nTable 2 reveals a rich variety of methods for determining the number of topics in LDA methods. The evaluation perspectives vary significantly, resulting in different methodologies. As a result, a consistent set of evaluation criteria for topic modelling results has not yet been established, and the problem of objectively assessing the effectiveness of modelling results remains unresolved. Perplexity remains a common choice in practical applications. However, some research suggests that coherence is the most effective method for measuring topic quality [46], with increased usage of this metric in recent studies. Despite the guidance provided by the above model evaluation methods, issues such as mixed topics, illogical topics, and indistinguishable topics can still arise."
              ],
              "metadata": null
            },
            {
              "id": "(Angelov, 2020)",
              "snippets": [
                "Traditional topic modeling techniques like LDA and PLSA are generative models; they seek to find topics that recreate the original documents word distributions with minimal loss. This necessitates these models to place uninformative words in topics with high probability, as they make up a large proportion of all documents. Additionally, there is no guarantee that they will find topics that are representative of the corpus. The results show they can find topics that are extremely specific or overly broad.\n\nThere are several advantages of top2vec over traditional topic modeling methods like LDA and PLSA. The primary advantages are that it automatically finds the number of topics and finds topics that are more informative and representative of the corpus. As demonstrated, stop-word lists are not required to find informative topic words, making it easy to use on a corpus of any domain or language. The use of distributed representations of words alleviates several challenges of traditional methods that use BOW representations of words, which ignore word semantics."
              ],
              "metadata": null
            },
            {
              "id": "(Xu et al., 2022)",
              "snippets": [
                "LDA is a probabilistic model that discovers latent topics in a text corpus and can be trained using collapsed Gibbs sampling [55,(Tang et al., 2014)(Huang, 2014). Specifically, LDA assumes K underlying topics, each of which is a distribution over a fixed vocabulary. Although LDA is reputed to yield promising results in modeling text corpora [61], it fundamentally suffers from several shortcomings, including difficulty in setting the parameter k, which refers to the number of topics to yield semantically meaningful results, a deficiency in handling short texts (Dieng et al., 2019), in capturing the contextual meaning of sentences (Dieng et al., 2019), as well as its inability to model topic correlations and the evolution of topics over time (Devlin et al., 2019)."
              ],
              "metadata": null
            },
            {
              "id": "(Dieng et al., 2019)",
              "snippets": [
                "Abstract Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word\u2019s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance."
              ],
              "metadata": null
            },
            {
              "id": "(Devlin et al., 2019)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Tang et al., 2014)",
              "snippets": [
                "Intuitively, it is desirable to discover the most significant topics first, and then dive deeper into the finer topics. With nPLSA, one can start with a large to generate topics that are largely divergent from existing topics, and then gradually decay to distinguish topics that are closer together. As long as is monotonically decreasing, one is still maximizing the objective (7) with the smallest explored. The question is when to stop generating new topics, or stop exploring a smaller . \n\nOur treatment makes use of a measure of diversity among the topics as a criterion to stop generating new topics. Diversity is widely studied as a criteria in ranking (Mei et al., 2010), query suggestion (Ma et al., 2010) and document summarization (Carbonell et al., 1998). Some existing work has used diversity as an evaluation criteria for topic quality (Mimno et al., 2011), but not for selecting the optimum number of topics (compare across different numbers of topics)",
                "Intuitively, if the number of topics is small, the learned topics tend to be close to the background language model and thus do not distinguish well between each other. When the number of topics grows, the granularity of topics becomes finer and the topics become more distinguishable, thus increasing the diversity. However, when the number of topics becomes too large, we start to obtain many small topics which may be too close to each other, which decreases the topic diversity. Therefore, diversity seems to be a good measure to capture the right granularity of topics."
              ],
              "metadata": null
            },
            {
              "id": "(Bing et al., 2018)",
              "snippets": [
                "We propose a new method of estimation in topic models, that is not a variation on the existing simplex finding algorithms, and that estimates the number of topics K from the observed data. We derive new finite sample minimax lower bounds for the estimation of A, as well as new upper bounds for our proposed estimator. We describe the scenarios where our estimator is minimax adaptive. Our finite sample analysis is valid for any number of documents (n), individual document length (N_i), dictionary size (p) and number of topics (K), and both p and K are allowed to increase with n, a situation not handled well by previous analyses."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Common Methods for Selecting Number of Topics",
          "text": "\n## Statistical and Mathematical Approaches\n\n1. **Perplexity Measurement**: A widely used approach that measures how well a probabilistic model predicts a sample. Lower perplexity values typically indicate better model fit, though the method may suggest selecting larger numbers of topics without converging to an optimal value <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper> <Paper corpusId=\"267200309\" paperTitle=\"(Williams et al., 2024)\" isShortName></Paper>. Recent studies have shown that perplexity scores may not correlate well with human judgment of topic quality <Paper corpusId=\"267200309\" paperTitle=\"(Williams et al., 2024)\" isShortName></Paper>.\n\n2. **Coherence Scores**: This method evaluates how semantically related the words within each topic are. Higher coherence scores typically indicate more interpretable topics <Paper corpusId=\"189998766\" paperTitle=\"(Islam, 2019)\" isShortName></Paper> <Paper corpusId=\"221949213\" paperTitle=\"(Ni et al., 2020)\" isShortName></Paper> <Paper corpusId=\"265344346\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper>. Researchers often build multiple models with different topic numbers and select the one with highest coherence scores <Paper corpusId=\"189998766\" paperTitle=\"(Islam, 2019)\" isShortName></Paper>.\n\n3. **JS Divergence Method**: This approach measures the dissimilarity between topics, with the assumption that less similar content among topics indicates a better classification model <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper>. However, the method may suffer from convergence issues as the number of topics increases <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper>.\n\n## Comparative and Iterative Methods\n\n4. **Model Comparison**: Many researchers test multiple models with different numbers of topics (K) and compare their performance using various metrics <Paper corpusId=\"237437304\" paperTitle=\"(Wei, 2021)\" isShortName></Paper>. This approach often involves V-fold cross-validation using perplexity or log-likelihood measurements <Paper corpusId=\"237437304\" paperTitle=\"(Wei, 2021)\" isShortName></Paper>.\n\n5. **Topic Diversity Assessment**: This technique examines how distinguishable topics are from each other. With few topics, the model tends to produce general topics similar to background language. As topics increase, they become more distinct, but too many topics leads to overlapping content and decreased diversity <Paper corpusId=\"18878266\" paperTitle=\"(Tang et al., 2014)\" isShortName></Paper> <Paper corpusId=\"54967583\" paperTitle=\"(Chae et al., 2018)\" isShortName></Paper>.\n\n6. **Maximum Marginal Relevance (MMR)**: This approach trains models with a large number of topics and then selects the top-K topics that balance relevance (coverage of the corpus) and diversity (minimizing similarity between selected topics) <Paper corpusId=\"18550518\" paperTitle=\"(Tang et al., 2016)\" isShortName></Paper>.\n\n7. **DivRank Algorithm**: Similar to MMR, this method trains models with many topics and then uses the DivRank algorithm to select topics based on their proportions and similarity measures <Paper corpusId=\"18550518\" paperTitle=\"(Tang et al., 2016)\" isShortName></Paper>.\n\n## Automated and Data-Driven Methods\n\n8. **Hierarchical Dirichlet Process (HDP)**: As a nonparametric Bayesian approach, HDP automatically determines the number of topics. However, it can be computationally inefficient for LDA and may produce more topics than necessary <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper> <Paper corpusId=\"54685028\" paperTitle=\"(Reisenbichler et al., 2018)\" isShortName></Paper> <Paper corpusId=\"221949213\" paperTitle=\"(Ni et al., 2020)\" isShortName></Paper>.\n\n9. **Stability Analysis**: This method assumes that if there is a \"natural\" number of topics in a dataset, that number will produce the most stable results across multiple clustering attempts <Paper corpusId=\"2664741\" paperTitle=\"(Llewellyn et al., 2016)\" isShortName></Paper>. This approach helps identify consistent topic patterns.\n\n10. **Advanced Embedding Methods**: Newer algorithms like Top2vec and BERTopic automatically determine appropriate topic numbers by leveraging distributed representations of words rather than requiring pre-specification <Paper corpusId=\"221246303\" paperTitle=\"(Angelov, 2020)\" isShortName></Paper> <Paper corpusId=\"274496753\" paperTitle=\"(Ocal, 2024)\" isShortName></Paper> <Paper corpusId=\"248530058\" paperTitle=\"(Egger et al., 2022)\" isShortName></Paper>.\n\n## Visual and Human-Centered Methods\n\n11. **Projection and Visualization**: This approach involves running a topic modeling method with different parameters, projecting the resulting topics into a single embedding space, and visually exploring the distribution to identify natural clusters <Paper corpusId=\"261321685\" paperTitle=\"(Andrienko et al., 2023)\" isShortName></Paper>.\n\n12. **Mixed Methods Approach**: This combines computational metrics with human judgment. For example, selecting candidate models based on statistical measures (residuals, coherence, exclusivity) and then having researchers examine example documents to make the final selection <Paper corpusId=\"247980825\" paperTitle=\"(Maharjan et al., 2022)\" isShortName></Paper>.\n\n13. **Topic Quality Assessment**: Researchers evaluate multiple dimensions including semantic coherence (probability of topic words co-occurring) and exclusivity (probability that top words don't appear in other topics) <Paper corpusId=\"247980825\" paperTitle=\"(Maharjan et al., 2022)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Gan et al., 2021)",
              "snippets": [
                "Based on the existing studies, it is known that a better topic selection model should have the following characteristics: (1) The topic model has certain predictive ability; i.e., the model can",
                "By analyzing the existing studies, it can be seen that although the LDA model can generate topics well by the probability distribution of words, it cannot give an optimal judgment about the number of topics to be selected. Most of the existing studies perform topic number selection from a certain feature that a good topic classification model should have. For example, the perplexity method considers that a good topic classification model should be able to effectively classify unknown documents outside the training set. The JS divergence or JS divergence approach considers that the less similar the content among the topics classified, the better the topic classification model. Both the coherence method and the stability method proposed by Greene judge the goodness of the topic classification by the similarity of the words within the same topic in the model. Although these existing topic selection methods are not comprehensive for topic classification, they also provide a good basis for us to think about how to judge whether a topic classification model is good or not.\n\nIn terms of the number of topics selected, the optimal topic number selection method can better select the appropriate number of topics for the LDA model. Compared with several other topic number selection methods, the optimal number of topics derived from our method is closest to the real number of topics. In our example, the number of topics selected by the perplexity method and the coherence method is much smaller than the real number of topics. The optimal number of topics selected by the JS divergence method is at the maximum boundary of the topic selection interval, which makes us skeptical about whether this selected number of topics is optimal. The optimal number of topics selected by the HDP method is much larger than the true number of topics.\n\nSecond, from the score curves, the score curve of the number of topics derived from our proposed optimal topic number selection method is closest to a U-shaped curve, which makes it possible to find the optimal extreme value point in the score curve. On the one hand, the score curve derived from the perplexity method has an optimal point at the position where the number of topics is small. However, when the number of topics exceeds a certain threshold, the perplexity value decreases sharply with the increase in the number of topics, so the perplexity method may have the problem that the larger the number of topics selected, the lower the perplexity value, and the curve is difficult to converge. The topic number score curve derived from the coherence method presents a smooth fluctuating state, and the state of this curve is less reliable than the U-shaped curve obtained by the optimal topic number selection method in finding the optimal value. The curves derived from the JS divergence method, on the other hand, exhibit the problem of failure to converge as the number of topics increases.\n\nThird, from the clustering graphs under different topic numbers, the optimal topic number selection method and the perplexity method are able to classify documents with large differences in content clearly. The optimal number of topics selected by the JS divergence method is large, and there is the problem of overclassifying content that is originally the same topic. The HDP method selects the largest number of topics, which leads it to divide the original same topic content into multiple subclasses, similar to the state of model overfitting in the training set,"
              ],
              "metadata": null
            },
            {
              "id": "(Williams et al., 2024)",
              "snippets": [
                "To calculate the optimal number of topics across a collection of documents, a good indication is the number of topics with which the model best predicts the data. For topic models such as LDA, a common indicator to measure the optimal number of topics is perplexity, a measure of how well a probabilistic model predicts a sample (Blei et al., 2009). Yet, recent studies have shown that predictive likelihood (in this case, perplexity) and human judgement are often not correlated (Chang et al., 2009)."
              ],
              "metadata": null
            },
            {
              "id": "(Islam, 2019)",
              "snippets": [
                "To find out the optimal number of topic, we build many LSA, NMF, LDA models with different values of number of topics (k) and pick the one that gives the highest coherence score. Choosing a 'k' that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics."
              ],
              "metadata": null
            },
            {
              "id": "(Ni et al., 2020)",
              "snippets": [
                "A common solution is to plug in a set of values and pick the optimal topic number either based on some intrinsic criterion, such as the coherence of the topics, or based on some extrinsic criterion, such as accuracy on a specific task, e.g. paraphrase identification. There also exist other methods to help with choosing the number of topics. For example, nonparametric Bayesian models, e.g. Hierarchical Dirichlet process are employed to automatically generate the number of topics (Teh et al., 2004). However, it is computationally inefficient to apply such nonparametric models to LDA (Wallach et al., 2009).\n\nCoherence measures which are based on word co-occurrence are widely applied to quantify the quality of topic models. The poor quality topics with the type of \"chained\", \"intruded\" and \"random\" could be detected using detected with coherence measures (Mimno et al., 2011). (Newman et al., 2010) proposed a coherence measure which is comparable to the human rating of topics. Their coherence measure (C UCI ) takes the set of the top J words (w 1 ,",
                ", w J ) for a given topic and sum a confirmation measure over all word pairs."
              ],
              "metadata": null
            },
            {
              "id": "(Pan et al., 2023)",
              "snippets": [
                "The selection of the optimal number of topics is based on quality evaluation methods of topic models, and it's a longstanding challenge. Too many topics can lead to small topic generalization scope and minor semantic content differences, making topic division difficult. Conversely, too few topics can result in overly broad semantic content generalization, neglecting smaller topics. The selection of the optimal number of topics directly affects the accuracy and interpretability of LDA topic modelling results. Currently, researchers often estimate the initial number of topics contained in documents based on prior knowledge and then select the optimal number of topics using quality evaluation methods like perplexity, coherence, and topic similarity. This paper provides a summarized comparison of the core ideas and advantages and disadvantages of several typical methods for determining the optimal number of topics, as shown in Table 2. \n\nTable 2 reveals a rich variety of methods for determining the number of topics in LDA methods. The evaluation perspectives vary significantly, resulting in different methodologies. As a result, a consistent set of evaluation criteria for topic modelling results has not yet been established, and the problem of objectively assessing the effectiveness of modelling results remains unresolved. Perplexity remains a common choice in practical applications. However, some research suggests that coherence is the most effective method for measuring topic quality [46], with increased usage of this metric in recent studies. Despite the guidance provided by the above model evaluation methods, issues such as mixed topics, illogical topics, and indistinguishable topics can still arise."
              ],
              "metadata": null
            },
            {
              "id": "(Wei, 2021)",
              "snippets": [
                "Computing the topic coherence score for different numbers of topics and choosing the model that gives the highest topic coherence would be one of the ways. Perplexity and log-likelihood based V-fold cross validation are also good options for finding the optimal number of topics. However, there is no standard to say which way is the best way to pick the optimal number of topics. Sometimes, the statistical optimal number of topics may not fit the business problem well."
              ],
              "metadata": null
            },
            {
              "id": "(Tang et al., 2014)",
              "snippets": [
                "Intuitively, it is desirable to discover the most significant topics first, and then dive deeper into the finer topics. With nPLSA, one can start with a large to generate topics that are largely divergent from existing topics, and then gradually decay to distinguish topics that are closer together. As long as is monotonically decreasing, one is still maximizing the objective (7) with the smallest explored. The question is when to stop generating new topics, or stop exploring a smaller . \n\nOur treatment makes use of a measure of diversity among the topics as a criterion to stop generating new topics. Diversity is widely studied as a criteria in ranking (Mei et al., 2010), query suggestion (Ma et al., 2010) and document summarization (Carbonell et al., 1998). Some existing work has used diversity as an evaluation criteria for topic quality (Mimno et al., 2011), but not for selecting the optimum number of topics (compare across different numbers of topics)",
                "Intuitively, if the number of topics is small, the learned topics tend to be close to the background language model and thus do not distinguish well between each other. When the number of topics grows, the granularity of topics becomes finer and the topics become more distinguishable, thus increasing the diversity. However, when the number of topics becomes too large, we start to obtain many small topics which may be too close to each other, which decreases the topic diversity. Therefore, diversity seems to be a good measure to capture the right granularity of topics."
              ],
              "metadata": null
            },
            {
              "id": "(Chae et al., 2018)",
              "snippets": [
                "Corporate social responsibility (CSR) is an essential business practice in industry and a popular topic in academic research. Several studies have attempted to understand topics or categories in CSR contexts and some have used qualitative techniques to analyze data from traditional communication channels such as corporate reports, newspapers, and websites. This study adopts computational content analysis for understanding themes or topics from CSR-related conversations in the Twitter-sphere, the largest microblogging social media platform. Specifically, a probabilistic topic modeling-based computational text analysis framework is introduced to answer three questions: (1) What CSR-related topics are being communicated in the Twitter-sphere and what are the prevalent topics or themes in CSR conversation? (topic prevalence); (2) How are those topics interrelated? (topic correlation); (3) How have those topics changed over time? (topic evolution). The topic modeling results are discussed, and the direction for future research is presented."
              ],
              "metadata": null
            },
            {
              "id": "(Tang et al., 2016)",
              "snippets": [
                "We compare the following algorithms for selecting top-K topics for data summarization.\n\n\u2022 PLSA/LDA. The classical PLSA or LDA model is directly utilized to learn exactly K topics.\n\n\u2022 PLSA/LDA-TopK. We first train PLSA/LDA with a large number of K topics, and then pick top-K topics with the largest sizes.\n\n\u2022 PLSA/LDA-MMR-TopK. PLSA/LDA is used to train a large number of K topics. Then the MMR algorithm [6] is used to select top-K topics from the K topics. Although the MMR algorithm is proposed in a query-dependent setting, we adapt it to our scenario. The relevance between the query and each topic is measured as the coverage of this topic in the whole data set, and the similarity among each pair of topics is calculated as the cosine similarity of the word distributions. The best results are reported by empirically tuning the parameter \u03bb in the MMR algorithm.\n\n\u2022 PLSA/LDA-DivRank-TopK. PLSA/LDA is used to train a large number of K topics, and then the Di-vRank algorithm [17] is used to select top-K topics. In the DivRank algorithm, we treat the proportions of the topics as the preference vector, and the weight between each pair of topics is calculated as the cosine similarity of the corresponding word distributions. The best results are reported by empirically tuning the parameter \u03b1 and \u03bb in the DivRank algorithm.\n\n\u2022 DivPLSA/DivLDA-TopK. DivPLSA/DivLDA is applied on the data set and then the top-K topics with the largest sizes are selected."
              ],
              "metadata": null
            },
            {
              "id": "(Reisenbichler et al., 2018)",
              "snippets": [
                "Despite recent advances allowing to infer the number of topics from the data, such as the Hierarchical Dirichlet Process as an extension of LDA (Teh et al., 2006)(Teh et al. , p. 1575)), it is still prevalent to choose the number of topics beforehand (Blei and Lafferty 2009, p. 81) and to employ post hoc procedures to validate the suitablility of choice. However, if the number of topics is chosen wrong, this can result in a poor performance (Tang et al. 2014, p. 7). Another eligible critique is the need for extensive parameter optimization before running (Asuncion et al. 2009, p. 30), possibly more arranging topic models to fit the needs of the researcher, than capturing what is really there (Schmidt 2013)."
              ],
              "metadata": null
            },
            {
              "id": "(Llewellyn et al., 2016)",
              "snippets": [
                "In our data set, we choose the number of clusters by two methods. When data has been anno- tated by humans the number of topics identified by humans was chosen as the cluster number. When the data had not been annotated by humans the cluster number was identified using an automatic method of stability analysis. This method was proposed by Greene, O' Callaghan, and Cunningham (2014), and it assumes that if there is a 'natural' number of topics within a data set, then this number of topics will give the most stable result each time the data is re-clustered."
              ],
              "metadata": null
            },
            {
              "id": "(Angelov, 2020)",
              "snippets": [
                "Traditional topic modeling techniques like LDA and PLSA are generative models; they seek to find topics that recreate the original documents word distributions with minimal loss. This necessitates these models to place uninformative words in topics with high probability, as they make up a large proportion of all documents. Additionally, there is no guarantee that they will find topics that are representative of the corpus. The results show they can find topics that are extremely specific or overly broad.\n\nThere are several advantages of top2vec over traditional topic modeling methods like LDA and PLSA. The primary advantages are that it automatically finds the number of topics and finds topics that are more informative and representative of the corpus. As demonstrated, stop-word lists are not required to find informative topic words, making it easy to use on a corpus of any domain or language. The use of distributed representations of words alleviates several challenges of traditional methods that use BOW representations of words, which ignore word semantics."
              ],
              "metadata": null
            },
            {
              "id": "(Ocal, 2024)",
              "snippets": [
                "With the latest advancements in the NLP field, novel topic modeling techniques have emerged such as Corex, Top2Vec, and BERTopic (Egger et al., 2022). BERTopic mainly addresses these limitations of LDA topic modeling and offers more advanced functions such as search functions (e.g., searching topics associated with a specific word, easily going from topic to documents), and hierarchical and dynamic topic modeling. Furthermore, as different from the LDA, BERTopic does not require intense preprocessing (removing stop-words, stemming, and lemmatization of text) to produce a better topic model [28]. Moreover, including BERTopic, several topic modeling methods (e.g., LDA, NMF, and Top2Vec ) were compared in prior research as in (Umamaheswaran et al., 2023), [28], and (Egger et al., 2022) and indicated the potency of BERTopic in analyzing text data. Despite BERTopic's many advantages, a few disadvantages also exist that come with it. First, its embedding approach may result in too many topics, demanding labor-intensive examination of each topic. Likewise, it may produce many outliers [67]."
              ],
              "metadata": null
            },
            {
              "id": "(Egger et al., 2022)",
              "snippets": [
                "The richness of social media data has opened a new avenue for social science research to gain insights into human behaviors and experiences. In particular, emerging data-driven approaches relying on topic models provide entirely new perspectives on interpreting social phenomena. However, the short, text-heavy, and unstructured nature of social media content often leads to methodological challenges in both data collection and analysis. In order to bridge the developing field of computational science and empirical social research, this study aims to evaluate the performance of four topic modeling techniques; namely latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), Top2Vec, and BERTopic. In view of the interplay between human relations and digital media, this research takes Twitter posts as the reference point and assesses the performance of different algorithms concerning their strengths and weaknesses in a social science context. Based on certain details during the analytical procedures and on quality issues, this research sheds light on the efficacy of using BERTopic and NMF to analyze Twitter data."
              ],
              "metadata": null
            },
            {
              "id": "(Andrienko et al., 2023)",
              "snippets": [
                "To determine the target number of topics, we used the approach proposed by Chen et al. [CAA*20]: running the selected method with different parameters, projecting the topics generated in the different runs into a single embedding space using a dimensionality reduction method, and exploring the topic distribution in this space, which is expected to reflect the similarities and differences between the topics. The number of visible distinct clusters in the projection suggests the potentially suitable number of topics. However, the main criterion is interpretability of the topics by a human. As it cannot be formally evaluated, we do not see a feasible way to fully automate the selection of the suitable number of topics."
              ],
              "metadata": null
            },
            {
              "id": "(Maharjan et al., 2022)",
              "snippets": [
                "The STM estimates topics based on a pre-specified number of topics (K). According to the authors of STM, there is no \"right\" answer to the number of topics appropriate for a given corpus (47,48), and no statistical tests to determine the optimal number of topics for a model nor the quality of those topics (49). STM does however support numerous diagnostic techniques for estimating the number of topics, including residuals (50), semantic coherence (51) and exclusivity analyses (52).\n\nIn this study, we employed a mixed methods approach to identifying the optimal number of topics (K), inspired by related work adopting a similar approach to analyzing Twitter data using STM (53). We began by estimating the model fit by comparing the residuals of the models with values of K ranging from 5 to 50, as recommended by the authors of the STM for a corpus size ranging from a few hundred to a few thousand documents (47). We then examined the diagnostic values of the residuals, semantic coherence, and exclusivity of each of the estimated models (see Figure 1), selecting ten candidate models with a value of K ranging from 20 to 30. These models contained low residuals-representing distances between observed and predicted values-and therefore best approximating the text of the reviews.\n\nWe next examined the semantic coherence (51) and exclusivity (52) of the individual topics of each of these candidate models using STM's topicQuality function. 5 Semantic coherence is a measure of the probability that a set of topic words6 co-occur within the corpus, and exclusivity refers to the probability that the top words representing the topic do not appear as top words for other topics.\n\nWhile these measures are efficient, researchers' judgment of topics by close reading of example documents is essential in the selection of the optimal STM model (39)."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Advantages of Topic Number Selection Methods",
          "text": "\n## Statistical Advantages\n\n1. **Perplexity Provides Predictive Power**: Perplexity measurement offers a quantitative metric for assessing how well a model can predict unseen data, providing an objective way to compare different topic configurations <Paper corpusId=\"4556015\" paperTitle=\"(Wesslen, 2018)\" isShortName></Paper>. Though researchers have found limitations to perplexity, it remains a common and accessible baseline for model evaluation <Paper corpusId=\"256968901\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>.\n\n2. **Coherence Scores Improve Interpretability**: Topic coherence metrics help identify semantically meaningful topics by measuring how well the top words in a topic relate to each other. Higher coherence scores typically indicate more interpretable topics that are easier for humans to understand <Paper corpusId=\"260306242\" paperTitle=\"(Aditya et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253362191\" paperTitle=\"(Zankadi et al., 2022)\" isShortName></Paper>.\n\n3. **Multiple Model Comparison Increases Reliability**: Building multiple topic models with different topic numbers and selecting the one with the highest coherence score helps researchers systematically identify the most appropriate model for their corpus <Paper corpusId=\"253362191\" paperTitle=\"(Zankadi et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260306242\" paperTitle=\"(Aditya et al., 2023)\" isShortName></Paper>.\n\n4. **Combined Metrics Offer Balanced Evaluation**: Methods that incorporate both semantic coherence and exclusivity provide a more comprehensive evaluation of topic quality, identifying topics that are both internally consistent and distinct from other topics <Paper corpusId=\"235409421\" paperTitle=\"(Savin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4556015\" paperTitle=\"(Wesslen, 2018)\" isShortName></Paper> <Paper corpusId=\"10183405\" paperTitle=\"(Roberts et al., 2014)\" isShortName></Paper>.\n\n## Robustness and Stability Advantages\n\n5. **Topic Diversity Assessment Improves Topic Differentiation**: Topic diversity measures help identify the optimal granularity of topics, where they are sufficiently distinct from each other without becoming too specific and overlapping <Paper corpusId=\"18878266\" paperTitle=\"(Tang et al., 2014)\" isShortName></Paper> <Paper corpusId=\"6012954\" paperTitle=\"(Ma et al., 2010)\" isShortName></Paper>.\n\n6. **Robustness to Parameter Changes**: Some topic modeling approaches demonstrate stability in identifying key topics regardless of the specific number of topics selected. This robustness helps ensure that the most important themes are captured even when parameters change <Paper corpusId=\"15240372\" paperTitle=\"(Randhawa et al., 2016)\" isShortName></Paper>.\n\n7. **U-shaped Metrics Enable Clear Optimization**: Methods that produce U-shaped evaluation curves (like certain optimal topic number selection approaches) make it easier to identify a clear optimal value, compared to methods that produce constantly decreasing or irregularly fluctuating curves <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper>.\n\n## Advanced Methodological Advantages\n\n8. **Automated Topic Number Determination**: Newer methods like Top2vec and BERTopic automatically determine the appropriate number of topics, reducing the need for manual parameter tuning and iterative testing <Paper corpusId=\"221246303\" paperTitle=\"(Angelov, 2020)\" isShortName></Paper>.\n\n9. **Embedding-Based Methods Improve Topic Quality**: Approaches that leverage distributed representations of words (rather than traditional bag-of-words) can identify more informative and representative topics without requiring stop-word lists, making them more adaptable across domains and languages <Paper corpusId=\"221246303\" paperTitle=\"(Angelov, 2020)\" isShortName></Paper>.\n\n10. **Mixed Methods Approach Combines Computational and Human Judgment**: Methods that integrate statistical metrics with human evaluation allow researchers to benefit from both computational efficiency and domain expertise. This approach helps select models that are both statistically sound and interpretable <Paper corpusId=\"247980825\" paperTitle=\"(Maharjan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"54967583\" paperTitle=\"(Chae et al., 2018)\" isShortName></Paper>.\n\n## Application-Specific Advantages\n\n11. **Specialized Dialogue Topic Modeling**: Some methods specifically balance different types of content (e.g., useful vs. uninformative) in dialogue data, helping to identify the optimal number of topics for conversational text <Paper corpusId=\"229157283\" paperTitle=\"(Zou et al., 2020)\" isShortName></Paper>.\n\n12. **Moderate Topic Numbers Enhance Usability**: For practical applications, choosing moderate topic numbers (K=10 or 20) often provides a good balance between capturing key themes and maintaining interpretability, making results more accessible for researchers <Paper corpusId=\"15240372\" paperTitle=\"(Randhawa et al., 2016)\" isShortName></Paper>.\n\n13. **Hierarchical Topic Exploration**: Some methods allow researchers to start with broader topics and then explore finer-grained topics, providing a more intuitive way to navigate complex document collections <Paper corpusId=\"18878266\" paperTitle=\"(Tang et al., 2014)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Wesslen, 2018)",
              "snippets": [
                "Initially in the topic model literature, prediction accuracy was the key goal of model evaluation. Wallach et al. (2009a) outlined different evaluation methods on LDA using Gibbs sampling. They consider two approaches to evaluating topic models: maximizing the held-out documents likelihood (perplexity) and document completion in which long documents are trained on part of the document and evaluated on the model's ability to correctly \"complete\" the document. They find that methods like harmonic mean, importance sampling and document completion methods are inaccurate and may distort the relative advantage of one model versus another model. Instead, they recommend either the Chib-style estimator or the \"left-to-right\" algorithm as more accurate evaluation methods [40]. However, Chang el al. ( 2009) explored the trade-off between prediction and interpretability. Through the word intrusion tasks, they found the counter-intuitive result that highly predictive topics tend to be negatively correlated with interpretability. Semantic coherence was introduced by Mimno et al. (2011) as a measure for how internally consistent words are within topics. On the other hand, exclusivity is a measure to identify words that have high probabilities for only a few topics rather than many topics. Roberts et al. argue that a \"topic that is both cohesive and exclusive is more likely to be semantically useful\" [8]",
                "Taddy (2012) explored estimation methods for choosing the optimal number of topics via block-diagonal approximations to the information matrix and goodness-of-fit analysis for likelihood-based model selection (Taddy, 2011)",
                "In a more systematic review of topic model performance, (Tang et al., 2014) analyzed LDA performance controlling for four limiting factors: document length, number of documents, and the two prior distribution hyperparameters",
                "Third, they find that collections with too many topics lend statistical inference methods to be inefficient. Fourth, they find that LDA performance is affected by how well-separated the underlying topics are relative to a Euclidean measure. Last, they find that the variability of hyperparameters is important depending on the number of topics within documents. For example, they recommend using a lower alpha (Dirichlet parameter) when documents have few topics, whereas using a high alpha for documents with many topics."
              ],
              "metadata": null
            },
            {
              "id": "(Zeng et al., 2023)",
              "snippets": [
                "We select the LDA method for topic detection. A tricky problem with this method is to determine the number of topics, which is important to the quality of text mining. To solve the problem, the perplexity has been widely used to find the optimal number of topics (Hasan et al., 2020), which represents the uncertainty of a text document pertaining to a particular topic in a fitted topic model. It has been proved in theory that the smaller the perplexity is, the better the model is. Hence, we may try a range of the number of topics, compare the corresponding perplexities, and then choose a number with the minimal perplexity as the optimal. In practice, however, some other factors need to be taken into consideration. For instance, the number of topics should not be too large for the ease of interpretation."
              ],
              "metadata": null
            },
            {
              "id": "(Aditya et al., 2023)",
              "snippets": [
                "Topic modeling is categorized as an unsupervised learning technique that offers a time-efficient approach since it does not require researchers to manually code a training set of documents prior to the analysis (Asmussen et al., 2019). Along with the benefits, the optimal number of topics may not be predetermined. Analysts are required to identify the optimal number of topics in order to achieve the best possible distribution of topics across documents and the distribution of keywords within each topic using the LDA algorithm to achieve an ideal topic-keyword composition (Islam, 2019). Coherence metrics have been introduced to differentiate between good and bad topics based on the interpretability of their top words (Mimno et al., 2011). The coherence score measures how semantically close the most prominent words are in a given topic (Morstatter et al., 2018). Typically, multiple models are constructed with varying numbers of topics (k), and the optimal number of topics is determined by selecting the number of topics that produces the highest coherence score. This number of topics often provides a more relevant and easily interpretable division of topics (Islam, 2019)."
              ],
              "metadata": null
            },
            {
              "id": "(Zankadi et al., 2022)",
              "snippets": [
                "The approach adopted to find the optimal number of topics for LDA and LSI models is to build many models (LDA and LSI) with different values for a number of topics (k) and pick the one with the highest coherence value concerning the significance of the generated topics.\n\nThe search for the optimal number of topics started with a range from two to 98, with a step of 6. During the process, only one hyperparameter varied (number of topics) and the other remained unchanged until reaching the highest coherence score. Figures 3 and 4 present the values of the coherence score about the number of topics for LDA and LSA models with BOW and TFIDF corpora.\n\nThe optimal number of topics of each model is the one that has the highest coherence score concerning the topic's relevance and significance. The training of the BERTopic model results in several topics equal to 40 with a coherence score equal to 0,62."
              ],
              "metadata": null
            },
            {
              "id": "(Savin et al., 2022)",
              "snippets": [
                "Conversely, setting the number too high may introduce uninterpretable topics that pick out idiosyncratic word combinations (Griffiths et al., 2004).While one can follow computational linguistic literature and calculate perplexity score (fitness) of the LDA model based on different numbers of topics (Blei et al., 2009), most of the time this number is chosen ad hoc and relatively small to maintain interpretability of results.It is by all means easier both to label and overview 10 and not 100 topics.\n\nIn our study we will choose the number of topics that aims to optimize not just perplexity score of the model, but also semantic coherence and exclusivity of the topics.In simple words, the perplexity score is goodness of the LDA model trained on one part of the data to predict the word distribution on the other part of it.Semantic coherence quantifies the extent to which frequent words from the same topic tend to appear in the same patents.Exclusivity analyzes whether popular words from one topic have low likelihood in all other topics.Thus, we follow (Roberts et al., 2014) in arguing that semantically interpretable topics should consist of words that tend to co-occur within documents, and that their top keywords are unlikely to overlap with keywords from other topics."
              ],
              "metadata": null
            },
            {
              "id": "(Roberts et al., 2014)",
              "snippets": [
                "Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments."
              ],
              "metadata": null
            },
            {
              "id": "(Tang et al., 2014)",
              "snippets": [
                "Intuitively, it is desirable to discover the most significant topics first, and then dive deeper into the finer topics. With nPLSA, one can start with a large to generate topics that are largely divergent from existing topics, and then gradually decay to distinguish topics that are closer together. As long as is monotonically decreasing, one is still maximizing the objective (7) with the smallest explored. The question is when to stop generating new topics, or stop exploring a smaller . \n\nOur treatment makes use of a measure of diversity among the topics as a criterion to stop generating new topics. Diversity is widely studied as a criteria in ranking (Mei et al., 2010), query suggestion (Ma et al., 2010) and document summarization (Carbonell et al., 1998). Some existing work has used diversity as an evaluation criteria for topic quality (Mimno et al., 2011), but not for selecting the optimum number of topics (compare across different numbers of topics)",
                "Intuitively, if the number of topics is small, the learned topics tend to be close to the background language model and thus do not distinguish well between each other. When the number of topics grows, the granularity of topics becomes finer and the topics become more distinguishable, thus increasing the diversity. However, when the number of topics becomes too large, we start to obtain many small topics which may be too close to each other, which decreases the topic diversity. Therefore, diversity seems to be a good measure to capture the right granularity of topics."
              ],
              "metadata": null
            },
            {
              "id": "(Ma et al., 2010)",
              "snippets": [
                "In order to improve the user search experience, Query Suggestion, a technique for generating alternative queries to Web users, has become an indispensable feature for commercial search engines. However, previous work mainly focuses on suggesting relevant queries to the original query while ignoring the diversity in the suggestions, which will potentially dissatisfy Web users' information needs. In this paper, we present a novel unified method to suggest both semantically relevant and diverse queries to Web users. The proposed approach is based on Markov random walk and hitting time analysis on the query-URL bipartite graph. It can effectively prevent semantically redundant queries from receiving a high rank, hence encouraging diversities in the results. We evaluate our method on a large commercial clickthrough dataset in terms of relevance measurement and diversity measurement. The experimental results show that our method is very effective in generating both relevant and diverse query suggestions."
              ],
              "metadata": null
            },
            {
              "id": "(Randhawa et al., 2016)",
              "snippets": [
                "Similar to other topic modeling methods, our algorithm takes as input the number of topics to be extracted, K. For the results described so far, we fixed K = 10. Table 6 displays the top-four topics for Mr. Kaminski when the number of topics K is set to 5 and 50. Notice that in both cases, the overall word score remains unaffected by the change in K, the only change is the clustering of topics. Observe that the top topic is identical for these two cases, and for the case K = 10 of Table 2. Even the second-ranked topic is quite similar. This illustrates the robustness of our approach in capturing the key topics from a corpus. Notice that the case K = 5 is quite crude in the sense that all words are clubbed into 5 clusters and hence we observe a diffuse topic such as Topic 4. On the other hand K = 50 is much more pointed and picks up a large number of small clusters, which leads to identifying many specialized topics. In our extensive experiments, we found that moderate K values, such as K = 10 or 20, work well in identifying key topics."
              ],
              "metadata": null
            },
            {
              "id": "(Gan et al., 2021)",
              "snippets": [
                "Based on the existing studies, it is known that a better topic selection model should have the following characteristics: (1) The topic model has certain predictive ability; i.e., the model can",
                "By analyzing the existing studies, it can be seen that although the LDA model can generate topics well by the probability distribution of words, it cannot give an optimal judgment about the number of topics to be selected. Most of the existing studies perform topic number selection from a certain feature that a good topic classification model should have. For example, the perplexity method considers that a good topic classification model should be able to effectively classify unknown documents outside the training set. The JS divergence or JS divergence approach considers that the less similar the content among the topics classified, the better the topic classification model. Both the coherence method and the stability method proposed by Greene judge the goodness of the topic classification by the similarity of the words within the same topic in the model. Although these existing topic selection methods are not comprehensive for topic classification, they also provide a good basis for us to think about how to judge whether a topic classification model is good or not.\n\nIn terms of the number of topics selected, the optimal topic number selection method can better select the appropriate number of topics for the LDA model. Compared with several other topic number selection methods, the optimal number of topics derived from our method is closest to the real number of topics. In our example, the number of topics selected by the perplexity method and the coherence method is much smaller than the real number of topics. The optimal number of topics selected by the JS divergence method is at the maximum boundary of the topic selection interval, which makes us skeptical about whether this selected number of topics is optimal. The optimal number of topics selected by the HDP method is much larger than the true number of topics.\n\nSecond, from the score curves, the score curve of the number of topics derived from our proposed optimal topic number selection method is closest to a U-shaped curve, which makes it possible to find the optimal extreme value point in the score curve. On the one hand, the score curve derived from the perplexity method has an optimal point at the position where the number of topics is small. However, when the number of topics exceeds a certain threshold, the perplexity value decreases sharply with the increase in the number of topics, so the perplexity method may have the problem that the larger the number of topics selected, the lower the perplexity value, and the curve is difficult to converge. The topic number score curve derived from the coherence method presents a smooth fluctuating state, and the state of this curve is less reliable than the U-shaped curve obtained by the optimal topic number selection method in finding the optimal value. The curves derived from the JS divergence method, on the other hand, exhibit the problem of failure to converge as the number of topics increases.\n\nThird, from the clustering graphs under different topic numbers, the optimal topic number selection method and the perplexity method are able to classify documents with large differences in content clearly. The optimal number of topics selected by the JS divergence method is large, and there is the problem of overclassifying content that is originally the same topic. The HDP method selects the largest number of topics, which leads it to divide the original same topic content into multiple subclasses, similar to the state of model overfitting in the training set,"
              ],
              "metadata": null
            },
            {
              "id": "(Angelov, 2020)",
              "snippets": [
                "Traditional topic modeling techniques like LDA and PLSA are generative models; they seek to find topics that recreate the original documents word distributions with minimal loss. This necessitates these models to place uninformative words in topics with high probability, as they make up a large proportion of all documents. Additionally, there is no guarantee that they will find topics that are representative of the corpus. The results show they can find topics that are extremely specific or overly broad.\n\nThere are several advantages of top2vec over traditional topic modeling methods like LDA and PLSA. The primary advantages are that it automatically finds the number of topics and finds topics that are more informative and representative of the corpus. As demonstrated, stop-word lists are not required to find informative topic words, making it easy to use on a corpus of any domain or language. The use of distributed representations of words alleviates several challenges of traditional methods that use BOW representations of words, which ignore word semantics."
              ],
              "metadata": null
            },
            {
              "id": "(Maharjan et al., 2022)",
              "snippets": [
                "The STM estimates topics based on a pre-specified number of topics (K). According to the authors of STM, there is no \"right\" answer to the number of topics appropriate for a given corpus (47,48), and no statistical tests to determine the optimal number of topics for a model nor the quality of those topics (49). STM does however support numerous diagnostic techniques for estimating the number of topics, including residuals (50), semantic coherence (51) and exclusivity analyses (52).\n\nIn this study, we employed a mixed methods approach to identifying the optimal number of topics (K), inspired by related work adopting a similar approach to analyzing Twitter data using STM (53). We began by estimating the model fit by comparing the residuals of the models with values of K ranging from 5 to 50, as recommended by the authors of the STM for a corpus size ranging from a few hundred to a few thousand documents (47). We then examined the diagnostic values of the residuals, semantic coherence, and exclusivity of each of the estimated models (see Figure 1), selecting ten candidate models with a value of K ranging from 20 to 30. These models contained low residuals-representing distances between observed and predicted values-and therefore best approximating the text of the reviews.\n\nWe next examined the semantic coherence (51) and exclusivity (52) of the individual topics of each of these candidate models using STM's topicQuality function. 5 Semantic coherence is a measure of the probability that a set of topic words6 co-occur within the corpus, and exclusivity refers to the probability that the top words representing the topic do not appear as top words for other topics.\n\nWhile these measures are efficient, researchers' judgment of topics by close reading of example documents is essential in the selection of the optimal STM model (39)."
              ],
              "metadata": null
            },
            {
              "id": "(Chae et al., 2018)",
              "snippets": [
                "Corporate social responsibility (CSR) is an essential business practice in industry and a popular topic in academic research. Several studies have attempted to understand topics or categories in CSR contexts and some have used qualitative techniques to analyze data from traditional communication channels such as corporate reports, newspapers, and websites. This study adopts computational content analysis for understanding themes or topics from CSR-related conversations in the Twitter-sphere, the largest microblogging social media platform. Specifically, a probabilistic topic modeling-based computational text analysis framework is introduced to answer three questions: (1) What CSR-related topics are being communicated in the Twitter-sphere and what are the prevalent topics or themes in CSR conversation? (topic prevalence); (2) How are those topics interrelated? (topic correlation); (3) How have those topics changed over time? (topic evolution). The topic modeling results are discussed, and the direction for future research is presented."
              ],
              "metadata": null
            },
            {
              "id": "(Zou et al., 2020)",
              "snippets": [
                "Effect of Topic Numbers in SATM. The topic number is a critical hyper-parameter in topic models because it potentially affects the convergence rate and the inference quality. Here, we report results of TDS+SATM with different topic numbers in Figure 4. Figure 4(a) shows the effects of K that ranges from 20 to 150 with K s = K o . It shows a performance decline trend and a variance increase trend when K is continuously increased after exceeding 50. It indicates that a proper topic number is sufficient for capturing main topics and a larger one makes the topic inference unstable. Figure  4(b) shows the effects of different ratios of K o /K, where we fix K=50 and adjust K o in range of 5 to 45. The results show that an overly unbalanced number of K s and K o can hurt the performance. It indicates that each dialogue is a mixture of useful and uninformative content, either of which can not be ignored when performing topic modeling on dialogues."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Disadvantages of Topic Number Selection Methods",
          "text": "\n## Statistical Method Limitations\n\n1. **Perplexity Decreases Without Convergence**: The perplexity method tends to favor increasingly larger numbers of topics without reaching a clear optimum. As the number of topics exceeds a threshold, perplexity values decrease sharply, making it difficult to identify a natural stopping point <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper> <Paper corpusId=\"199683368\" paperTitle=\"(Sergei, 2018)\" isShortName></Paper>.\n\n2. **Poor Correlation with Human Judgment**: Despite its popularity, perplexity scores often don't align well with human assessments of topic quality. Research indicates that lower perplexity values don't necessarily translate to more interpretable or meaningful topics for human readers <Paper corpusId=\"221949213\" paperTitle=\"(Ni et al., 2020)\" isShortName></Paper> <Paper corpusId=\"4556015\" paperTitle=\"(Wesslen, 2018)\" isShortName></Paper>.\n\n3. **Lack of Clear Evaluation Criteria**: A fundamental challenge in topic modeling is the difficulty in formulating semantic concepts of topics and establishing linguistic criteria for distinguishing between topics. This makes it challenging to define quality measures that reliably indicate optimal topic numbers <Paper corpusId=\"199683368\" paperTitle=\"(Sergei, 2018)\" isShortName></Paper> <Paper corpusId=\"265344346\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper>.\n\n4. **Inconsistent Evaluation Metrics**: There is no consensus on evaluation criteria for topic modeling results, with different methods approaching the problem from vastly different perspectives. This inconsistency makes it difficult to objectively assess the effectiveness of modeling results <Paper corpusId=\"265344346\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper>.\n\n## Practical Implementation Challenges\n\n5. **Computational Inefficiency with Large Datasets**: Some approaches, such as those using SVD decomposition and Kullback-Leibler divergence calculations, are computationally demanding and often impractical for large datasets. Studies using these methods have typically been limited to small collections of texts <Paper corpusId=\"199683368\" paperTitle=\"(Sergei, 2018)\" isShortName></Paper>.\n\n6. **Need for Multiple Attempts and Manual Intervention**: Even with sophisticated selection methods, practitioners often resort to trying multiple models with different topic numbers and manually selecting the most appropriate one based on subjective assessment, which is time-consuming and potentially biased <Paper corpusId=\"214557002\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>.\n\n7. **Reliance on Pre-Parameter Optimization**: Topic models often require extensive parameter optimization before execution, which may lead to models that fit the researcher's expectations rather than capturing what is actually present in the data <Paper corpusId=\"54685028\" paperTitle=\"(Reisenbichler et al., 2018)\" isShortName></Paper>.\n\n## Model-Specific Disadvantages\n\n8. **Hierarchical Dirichlet Process (HDP) Inefficiency**: While HDP can automatically determine the number of topics, it is computationally inefficient when applied to LDA and tends to generate more topics than necessary, complicating interpretation <Paper corpusId=\"221949213\" paperTitle=\"(Ni et al., 2020)\" isShortName></Paper> <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper>.\n\n9. **JS Divergence Method Convergence Issues**: The JS divergence approach, which measures topic dissimilarity, often fails to converge as the number of topics increases, limiting its reliability for optimal topic number selection <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper>.\n\n10. **Coherence Method Fluctuations**: The topic number score curve derived from coherence methods often presents fluctuating patterns rather than clear U-shaped curves, making it difficult to identify optimal values with confidence <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper>.\n\n## Content and Interpretation Issues\n\n11. **Risk of Overlapping or Indistinguishable Topics**: Selecting too many topics can lead to content overlap and increased similarity between topics, making them difficult to distinguish. This is particularly problematic with the perplexity method, which tends to favor larger topic numbers <Paper corpusId=\"214557002\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"265344346\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper>.\n\n12. **Inability to Handle Language Nuances**: Traditional topic modeling approaches like LDA ignore word order and semantics, making them less sensitive to contextual meaning and smaller topics. This limitation affects their ability to isolate finer details in text <Paper corpusId=\"259145117\" paperTitle=\"(Lakatos et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253280653\" paperTitle=\"(Xu et al., 2022)\" isShortName></Paper>.\n\n13. **Overclassification with Large Topic Numbers**: Methods that select larger numbers of topics, such as JS divergence and HDP, tend to overclassify content that should belong to the same topic, dividing coherent themes into multiple subclasses similar to model overfitting <Paper corpusId=\"239468760\" paperTitle=\"(Gan et al., 2021)\" isShortName></Paper>.\n\n## Emerging Method Limitations\n\n14. **Too Many Topics with Newer Embedding Models**: Despite their advantages, newer embedding-based methods like BERTopic may produce excessive numbers of topics, requiring labor-intensive examination of each topic to determine relevance <Paper corpusId=\"274496753\" paperTitle=\"(Ocal, 2024)\" isShortName></Paper>.\n\n15. **Outlier Generation Issues**: Advanced models like BERTopic can generate many outlier topics that don't fit well into the overall topic structure, complicating interpretation and analysis <Paper corpusId=\"274496753\" paperTitle=\"(Ocal, 2024)\" isShortName></Paper>.\n\n16. **Fixed Topic Number Assumption**: Many topic modeling approaches, including traditional ones like LSA, PLSA, and LDA, assume a fixed number of topics must be specified before modeling. This fundamental limitation requires significant effort to determine the appropriate number <Paper corpusId=\"266051883\" paperTitle=\"(Ulloa, 2023)\" isShortName></Paper> <Paper corpusId=\"16475344\" paperTitle=\"(Alghamdi et al., 2015)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Gan et al., 2021)",
              "snippets": [
                "Based on the existing studies, it is known that a better topic selection model should have the following characteristics: (1) The topic model has certain predictive ability; i.e., the model can",
                "By analyzing the existing studies, it can be seen that although the LDA model can generate topics well by the probability distribution of words, it cannot give an optimal judgment about the number of topics to be selected. Most of the existing studies perform topic number selection from a certain feature that a good topic classification model should have. For example, the perplexity method considers that a good topic classification model should be able to effectively classify unknown documents outside the training set. The JS divergence or JS divergence approach considers that the less similar the content among the topics classified, the better the topic classification model. Both the coherence method and the stability method proposed by Greene judge the goodness of the topic classification by the similarity of the words within the same topic in the model. Although these existing topic selection methods are not comprehensive for topic classification, they also provide a good basis for us to think about how to judge whether a topic classification model is good or not.\n\nIn terms of the number of topics selected, the optimal topic number selection method can better select the appropriate number of topics for the LDA model. Compared with several other topic number selection methods, the optimal number of topics derived from our method is closest to the real number of topics. In our example, the number of topics selected by the perplexity method and the coherence method is much smaller than the real number of topics. The optimal number of topics selected by the JS divergence method is at the maximum boundary of the topic selection interval, which makes us skeptical about whether this selected number of topics is optimal. The optimal number of topics selected by the HDP method is much larger than the true number of topics.\n\nSecond, from the score curves, the score curve of the number of topics derived from our proposed optimal topic number selection method is closest to a U-shaped curve, which makes it possible to find the optimal extreme value point in the score curve. On the one hand, the score curve derived from the perplexity method has an optimal point at the position where the number of topics is small. However, when the number of topics exceeds a certain threshold, the perplexity value decreases sharply with the increase in the number of topics, so the perplexity method may have the problem that the larger the number of topics selected, the lower the perplexity value, and the curve is difficult to converge. The topic number score curve derived from the coherence method presents a smooth fluctuating state, and the state of this curve is less reliable than the U-shaped curve obtained by the optimal topic number selection method in finding the optimal value. The curves derived from the JS divergence method, on the other hand, exhibit the problem of failure to converge as the number of topics increases.\n\nThird, from the clustering graphs under different topic numbers, the optimal topic number selection method and the perplexity method are able to classify documents with large differences in content clearly. The optimal number of topics selected by the JS divergence method is large, and there is the problem of overclassifying content that is originally the same topic. The HDP method selects the largest number of topics, which leads it to divide the original same topic content into multiple subclasses, similar to the state of model overfitting in the training set,"
              ],
              "metadata": null
            },
            {
              "id": "(Sergei, 2018)",
              "snippets": [
                "Firstly, it is difficult in TM to formulate the semantic concept of a topic, the linguistic criteria for dividing two topics among themselves and, accordingly, the quality measures for topics and topic solutions. Secondly, in TM, as well as in cluster analysis, it is difficult to formulate an adequate functional dependence, which, on the one hand, would characterize the quality of the topic model, and on the other would be a function of the number of topics. Nevertheless, there are several works in which the authors have attempted to solve the problem of choosing the number of topics specifically in topic modeling. The authors of work [38], based on the ideas of cluster analysis, show that topic solutions with minimal correlation between the topics measured with the help of the cosine measure correspond to solutions with a minimum value of another quality measure, perplexity. This is an interesting work, but we have never come across data on which the function of the perplexity dependence on the number of topics would have a minimum (as it has as per the authors), instead of monotonically decreasing. In another work [39], the authors propose performing the singularvalue decomposition (SVD) of matrices \u03c6 wt and \u03b8 td , then selecting two vectors containing singular quantities and calculating the distance between them on the basis of the Kullback-Leibler divergence, which it is proposed to minimize. In this case, the optimal number of topics corresponds to the situation where both matrices are described by the same number of singular quantities. The authors of this approach do not verify it with the help of alternative measures of the quality of the solutions obtained, and in addition the operations of SVD decomposition and calculation of the Kullback-Leibler divergence severely restrict the application of this approach to large data. The collections used in [39] did not exceed 2,500 texts."
              ],
              "metadata": null
            },
            {
              "id": "(Ni et al., 2020)",
              "snippets": [
                "A common solution is to plug in a set of values and pick the optimal topic number either based on some intrinsic criterion, such as the coherence of the topics, or based on some extrinsic criterion, such as accuracy on a specific task, e.g. paraphrase identification. There also exist other methods to help with choosing the number of topics. For example, nonparametric Bayesian models, e.g. Hierarchical Dirichlet process are employed to automatically generate the number of topics (Teh et al., 2004). However, it is computationally inefficient to apply such nonparametric models to LDA (Wallach et al., 2009).\n\nCoherence measures which are based on word co-occurrence are widely applied to quantify the quality of topic models. The poor quality topics with the type of \"chained\", \"intruded\" and \"random\" could be detected using detected with coherence measures (Mimno et al., 2011). (Newman et al., 2010) proposed a coherence measure which is comparable to the human rating of topics. Their coherence measure (C UCI ) takes the set of the top J words (w 1 ,",
                ", w J ) for a given topic and sum a confirmation measure over all word pairs."
              ],
              "metadata": null
            },
            {
              "id": "(Wesslen, 2018)",
              "snippets": [
                "Initially in the topic model literature, prediction accuracy was the key goal of model evaluation. Wallach et al. (2009a) outlined different evaluation methods on LDA using Gibbs sampling. They consider two approaches to evaluating topic models: maximizing the held-out documents likelihood (perplexity) and document completion in which long documents are trained on part of the document and evaluated on the model's ability to correctly \"complete\" the document. They find that methods like harmonic mean, importance sampling and document completion methods are inaccurate and may distort the relative advantage of one model versus another model. Instead, they recommend either the Chib-style estimator or the \"left-to-right\" algorithm as more accurate evaluation methods [40]. However, Chang el al. ( 2009) explored the trade-off between prediction and interpretability. Through the word intrusion tasks, they found the counter-intuitive result that highly predictive topics tend to be negatively correlated with interpretability. Semantic coherence was introduced by Mimno et al. (2011) as a measure for how internally consistent words are within topics. On the other hand, exclusivity is a measure to identify words that have high probabilities for only a few topics rather than many topics. Roberts et al. argue that a \"topic that is both cohesive and exclusive is more likely to be semantically useful\" [8]",
                "Taddy (2012) explored estimation methods for choosing the optimal number of topics via block-diagonal approximations to the information matrix and goodness-of-fit analysis for likelihood-based model selection (Taddy, 2011)",
                "In a more systematic review of topic model performance, (Tang et al., 2014) analyzed LDA performance controlling for four limiting factors: document length, number of documents, and the two prior distribution hyperparameters",
                "Third, they find that collections with too many topics lend statistical inference methods to be inefficient. Fourth, they find that LDA performance is affected by how well-separated the underlying topics are relative to a Euclidean measure. Last, they find that the variability of hyperparameters is important depending on the number of topics within documents. For example, they recommend using a lower alpha (Dirichlet parameter) when documents have few topics, whereas using a high alpha for documents with many topics."
              ],
              "metadata": null
            },
            {
              "id": "(Pan et al., 2023)",
              "snippets": [
                "The selection of the optimal number of topics is based on quality evaluation methods of topic models, and it's a longstanding challenge. Too many topics can lead to small topic generalization scope and minor semantic content differences, making topic division difficult. Conversely, too few topics can result in overly broad semantic content generalization, neglecting smaller topics. The selection of the optimal number of topics directly affects the accuracy and interpretability of LDA topic modelling results. Currently, researchers often estimate the initial number of topics contained in documents based on prior knowledge and then select the optimal number of topics using quality evaluation methods like perplexity, coherence, and topic similarity. This paper provides a summarized comparison of the core ideas and advantages and disadvantages of several typical methods for determining the optimal number of topics, as shown in Table 2. \n\nTable 2 reveals a rich variety of methods for determining the number of topics in LDA methods. The evaluation perspectives vary significantly, resulting in different methodologies. As a result, a consistent set of evaluation criteria for topic modelling results has not yet been established, and the problem of objectively assessing the effectiveness of modelling results remains unresolved. Perplexity remains a common choice in practical applications. However, some research suggests that coherence is the most effective method for measuring topic quality [46], with increased usage of this metric in recent studies. Despite the guidance provided by the above model evaluation methods, issues such as mixed topics, illogical topics, and indistinguishable topics can still arise."
              ],
              "metadata": null
            },
            {
              "id": "(Xu et al., 2020)",
              "snippets": [
                "Many studies have shown that the quality of LDA topic model modeling is closely related to the selection of the number of topics. Therefore, choosing an optimal number of topics is the focus of the LDA topic model application, and it is also difficult. The most commonly used method is to use Perplexity as the evaluation criterion and choose the model with the minimum Perplexity to determine the optimal number of topics. Although the minimum Perplexity can make the prediction ability of the model the best, it tends to choose a larger number of topics, which leads to higher similarity between the selected topics, resulting in problems that are difficult to identify the topic, and affecting the result of further analysis. After comprehensive consideration, select a certain range of values to make multiple attempts, and determine the appropriate number of topics by artificial selection."
              ],
              "metadata": null
            },
            {
              "id": "(Reisenbichler et al., 2018)",
              "snippets": [
                "Despite recent advances allowing to infer the number of topics from the data, such as the Hierarchical Dirichlet Process as an extension of LDA (Teh et al., 2006)(Teh et al. , p. 1575)), it is still prevalent to choose the number of topics beforehand (Blei and Lafferty 2009, p. 81) and to employ post hoc procedures to validate the suitablility of choice. However, if the number of topics is chosen wrong, this can result in a poor performance (Tang et al. 2014, p. 7). Another eligible critique is the need for extensive parameter optimization before running (Asuncion et al. 2009, p. 30), possibly more arranging topic models to fit the needs of the researcher, than capturing what is really there (Schmidt 2013)."
              ],
              "metadata": null
            },
            {
              "id": "(Lakatos et al., 2023)",
              "snippets": [
                "To adapt it to a specific context, Batch [28] or Online Variational Bayes [29] methods have become popular. To measure the quality of the topics generated by LDA and thus find the right number of topics, the perplexity [17] values of each topic are computed. For dictionary construction, the bag-of-words [30] technique is a common choice, with lemmatization and stop word removal.\n\nConsidering our system, a drawback of this model is that it is a probabilistic statistical method that ignores word order and semantics and is less sensitive to finding smaller topics. It means that it finds fewer topics and cannot isolate finer details. In practice, when using LDA, we tend to focus on e.g., the top 10 words, which narrows down the list of key terms, especially when there are few topics. Although these properties of the model are parameterizable, our experience shows that the more words we select from a given topic, the more the quality of the words or phrases associated with that topic deteriorates. This leads to greater overlap between the words of the topics, which requires further corrections."
              ],
              "metadata": null
            },
            {
              "id": "(Xu et al., 2022)",
              "snippets": [
                "LDA is a probabilistic model that discovers latent topics in a text corpus and can be trained using collapsed Gibbs sampling [55,(Tang et al., 2014)(Huang, 2014). Specifically, LDA assumes K underlying topics, each of which is a distribution over a fixed vocabulary. Although LDA is reputed to yield promising results in modeling text corpora [61], it fundamentally suffers from several shortcomings, including difficulty in setting the parameter k, which refers to the number of topics to yield semantically meaningful results, a deficiency in handling short texts (Dieng et al., 2019), in capturing the contextual meaning of sentences (Dieng et al., 2019), as well as its inability to model topic correlations and the evolution of topics over time (Devlin et al., 2019)."
              ],
              "metadata": null
            },
            {
              "id": "(Ocal, 2024)",
              "snippets": [
                "With the latest advancements in the NLP field, novel topic modeling techniques have emerged such as Corex, Top2Vec, and BERTopic (Egger et al., 2022). BERTopic mainly addresses these limitations of LDA topic modeling and offers more advanced functions such as search functions (e.g., searching topics associated with a specific word, easily going from topic to documents), and hierarchical and dynamic topic modeling. Furthermore, as different from the LDA, BERTopic does not require intense preprocessing (removing stop-words, stemming, and lemmatization of text) to produce a better topic model [28]. Moreover, including BERTopic, several topic modeling methods (e.g., LDA, NMF, and Top2Vec ) were compared in prior research as in (Umamaheswaran et al., 2023), [28], and (Egger et al., 2022) and indicated the potency of BERTopic in analyzing text data. Despite BERTopic's many advantages, a few disadvantages also exist that come with it. First, its embedding approach may result in too many topics, demanding labor-intensive examination of each topic. Likewise, it may produce many outliers [67]."
              ],
              "metadata": null
            },
            {
              "id": "(Ulloa, 2023)",
              "snippets": [
                "The methods described above are commonly used for topic modeling tasks. Several works use them as part of a text categorization process or through comparing methods (Alghamdi et al., 2015)10,11,12]. However, LSA, PLSA, and LDA have several disadvantages, such as fixing the number of topics, lack of capturing non-linear relationships between words, and the assumption that a document can contain different topics, which may not be valid. For this reason, other proposals have been developed that consider the possible dependence on the context of words."
              ],
              "metadata": null
            },
            {
              "id": "(Alghamdi et al., 2015)",
              "snippets": [
                "Topic models provide a convenient way to analyze large of unclassified text. A topic contains a cluster of words that frequently occur together. A topic modeling can connect words with similar meanings and distinguish between uses of words with multiple meanings. This paper provides two categories that can be under the field of topic modeling. First one discusses the area of methods of topic modeling, which has four methods that can be considerable under this category. These methods are Latent semantic analysis (LSA), Probabilistic latent semantic analysis (PLSA), Latent Dirichlet allocation (LDA), and Correlated topic model (CTM). The second category is called topic evolution models, which model topics by considering an important factor time. In the second category, different models are discussed, such as topic over time (TOT), dynamic topic models (DTM), multiscale topic tomography, dynamic topic correlation detection, detecting topic evolution in scientific literature, etc."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Evaluation Metrics for Topic Quality",
          "text": "\n## Statistical Quality Metrics\n\n1. **Perplexity**: A measure of how well a probabilistic model predicts a sample, with lower values typically indicating better predictive capabilities. While commonly used, perplexity scores may not correlate well with human judgment of topic quality and tend to favor larger numbers of topics without reaching a clear optimum <Paper corpusId=\"260261038\" paperTitle=\"(Xiang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"265344346\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253199593\" paperTitle=\"(Siahaan et al., 2022)\" isShortName></Paper>.\n\n2. **Coherence Scores**: Evaluate how semantically related the words within each topic are, with higher scores typically indicating more interpretable topics. Coherence measures the probability that a set of topic words co-occur within the corpus <Paper corpusId=\"260261038\" paperTitle=\"(Xiang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"236669792\" paperTitle=\"(Khatoon et al., 2021)\" isShortName></Paper> <Paper corpusId=\"247980825\" paperTitle=\"(Maharjan et al., 2022)\" isShortName></Paper>.\n\n3. **Exclusivity**: Assesses whether the top words representing a topic do not appear as top words for other topics. Higher exclusivity indicates more distinct topics that are easier to interpret <Paper corpusId=\"247980825\" paperTitle=\"(Maharjan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"235409421\" paperTitle=\"(Savin et al., 2022)\" isShortName></Paper>.\n\n4. **Residuals Analysis**: Represents distances between observed and predicted values in the model, with lower residuals indicating better approximation of the text data. Used to identify candidate models for further evaluation <Paper corpusId=\"247980825\" paperTitle=\"(Maharjan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259900130\" paperTitle=\"(Olson et al., 2023)\" isShortName></Paper>.\n\n## Semantic and Distinctiveness Metrics\n\n5. **Topic Uniqueness**: Measures the proportion of unique words in a topic, computed over the top words in the vocabulary. Higher uniqueness scores indicate more distinct topics with less overlap <Paper corpusId=\"202583512\" paperTitle=\"(Jain et al., 2019)\" isShortName></Paper>.\n\n6. **Cosine Similarity**: Used to assess similarity between topics or documents, with lower values indicating less topic overlap. Can help determine the optimal number of topics by identifying configurations with minimal similarity between topics <Paper corpusId=\"225644780\" paperTitle=\"(Lee et al., 2020)\" isShortName></Paper>.\n\n7. **Topic Diversity**: Evaluates how distinguishable topics are from each other. An optimal number of topics should create distinct topics without excessive overlap or redundancy <Paper corpusId=\"265344346\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper>.\n\n## Validation and Evaluation Approaches\n\n8. **Cross-Validation**: Techniques like V-fold cross-validation using perplexity or log-likelihood measurements help identify the optimal number of topics through systematic testing across different data subsets <Paper corpusId=\"237437304\" paperTitle=\"(Wei, 2021)\" isShortName></Paper>.\n\n9. **Elbow Method**: Used with perplexity scores to identify the point at which adding more topics provides diminishing returns, often visualized as the \"elbow\" in a graph plotting perplexity against topic numbers <Paper corpusId=\"248638274\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>.\n\n10. **Model Comparison**: Building multiple models with different numbers of topics (k) and selecting the one with the highest coherence score or optimal combination of metrics. This approach helps identify the most appropriate model for a given corpus <Paper corpusId=\"260306242\" paperTitle=\"(Aditya et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253362191\" paperTitle=\"(Zankadi et al., 2022)\" isShortName></Paper>.\n\n## Human and Contextual Evaluation\n\n11. **Human Judgment**: Despite quantitative metrics, researchers' qualitative assessment through close reading of documents remains essential for selecting the optimal model. This involves manually evaluating topic interpretability and meaningfulness <Paper corpusId=\"247980825\" paperTitle=\"(Maharjan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"232168004\" paperTitle=\"(Abdulaziz et al., 2021)\" isShortName></Paper>.\n\n12. **Mixed Methods Approach**: Combining computational metrics (like residuals, coherence, exclusivity) with human judgment for a more comprehensive evaluation. This approach helps balance statistical robustness with practical interpretability <Paper corpusId=\"247980825\" paperTitle=\"(Maharjan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"54967583\" paperTitle=\"(Chae et al., 2018)\" isShortName></Paper>.\n\n13. **Context-Specific Assessment**: Consideration of corpus size, document length, and domain characteristics when determining the optimal number of topics. Small corpora may require fewer topics, while larger collections might benefit from more granular topic divisions <Paper corpusId=\"232168004\" paperTitle=\"(Abdulaziz et al., 2021)\" isShortName></Paper>.\n\n14. **Balance of Metrics**: Considering both perplexity and coherence together when selecting the optimal number of topics. The ideal configuration should have both low perplexity and high coherence to balance predictive power with interpretability <Paper corpusId=\"260261038\" paperTitle=\"(Xiang et al., 2023)\" isShortName></Paper>.\n\n15. **Diminishing Returns Analysis**: Identifying the point at which increasing the number of topics provides minimal improvement in coherence scores. This approach helps avoid creating excessive topics that might reduce interpretability or increase computational demands <Paper corpusId=\"235349283\" paperTitle=\"(Xue et al., 2021)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Xiang et al., 2023)",
              "snippets": [
                "In topic modeling, perplexity and coherence are commonly used to evaluate the quality and interpretability of models. Perplexity is a metric used to measure a model's ability to fit new text data, with lower perplexity indicating better predictive capabilities. Coherence, on the other hand, is used to assess the interpretability of the topics generated by a topic model, with higher coherence indicating better quality of generated topics. Generally, perplexity decreases with an increasing number of topics, while coherence initially increases and then decreases with the number of topics. Therefore, when selecting the optimal number of topics, it is necessary to consider the changes in perplexity and coherence together. The optimal number of topics should have both low perplexity and the highest point of coherence. This approach helps avoid overfitting issues while improving the quality and interpretability of the topic model."
              ],
              "metadata": null
            },
            {
              "id": "(Pan et al., 2023)",
              "snippets": [
                "The selection of the optimal number of topics is based on quality evaluation methods of topic models, and it's a longstanding challenge. Too many topics can lead to small topic generalization scope and minor semantic content differences, making topic division difficult. Conversely, too few topics can result in overly broad semantic content generalization, neglecting smaller topics. The selection of the optimal number of topics directly affects the accuracy and interpretability of LDA topic modelling results. Currently, researchers often estimate the initial number of topics contained in documents based on prior knowledge and then select the optimal number of topics using quality evaluation methods like perplexity, coherence, and topic similarity. This paper provides a summarized comparison of the core ideas and advantages and disadvantages of several typical methods for determining the optimal number of topics, as shown in Table 2. \n\nTable 2 reveals a rich variety of methods for determining the number of topics in LDA methods. The evaluation perspectives vary significantly, resulting in different methodologies. As a result, a consistent set of evaluation criteria for topic modelling results has not yet been established, and the problem of objectively assessing the effectiveness of modelling results remains unresolved. Perplexity remains a common choice in practical applications. However, some research suggests that coherence is the most effective method for measuring topic quality [46], with increased usage of this metric in recent studies. Despite the guidance provided by the above model evaluation methods, issues such as mixed topics, illogical topics, and indistinguishable topics can still arise."
              ],
              "metadata": null
            },
            {
              "id": "(Siahaan et al., 2022)",
              "snippets": [
                "Perplexity is considered to be a good approximation for a case where the test data are similar to training data. While perplexity only considers intrinsic measurements, topic coherence also consideres extrinsic measurements. These two measurements are supposed to make a distinction between semantically expoundable topics and factitious topics of statistical inference. In this study, we also compared the two estimation approaches against human judgement."
              ],
              "metadata": null
            },
            {
              "id": "(Khatoon et al., 2021)",
              "snippets": [
                "To select and realize the most efficient workflow, several topic models are trained by orchestrating preprocessing steps and applying different combinations of hyperparameters. For calculating the most appropriate number of topics, we used the genism coherence model. We choose 8 topics returned by LDA for the selected dataset because this number of topics achieved the highest coherence score (0.45)."
              ],
              "metadata": null
            },
            {
              "id": "(Maharjan et al., 2022)",
              "snippets": [
                "The STM estimates topics based on a pre-specified number of topics (K). According to the authors of STM, there is no \"right\" answer to the number of topics appropriate for a given corpus (47,48), and no statistical tests to determine the optimal number of topics for a model nor the quality of those topics (49). STM does however support numerous diagnostic techniques for estimating the number of topics, including residuals (50), semantic coherence (51) and exclusivity analyses (52).\n\nIn this study, we employed a mixed methods approach to identifying the optimal number of topics (K), inspired by related work adopting a similar approach to analyzing Twitter data using STM (53). We began by estimating the model fit by comparing the residuals of the models with values of K ranging from 5 to 50, as recommended by the authors of the STM for a corpus size ranging from a few hundred to a few thousand documents (47). We then examined the diagnostic values of the residuals, semantic coherence, and exclusivity of each of the estimated models (see Figure 1), selecting ten candidate models with a value of K ranging from 20 to 30. These models contained low residuals-representing distances between observed and predicted values-and therefore best approximating the text of the reviews.\n\nWe next examined the semantic coherence (51) and exclusivity (52) of the individual topics of each of these candidate models using STM's topicQuality function. 5 Semantic coherence is a measure of the probability that a set of topic words6 co-occur within the corpus, and exclusivity refers to the probability that the top words representing the topic do not appear as top words for other topics.\n\nWhile these measures are efficient, researchers' judgment of topics by close reading of example documents is essential in the selection of the optimal STM model (39)."
              ],
              "metadata": null
            },
            {
              "id": "(Savin et al., 2022)",
              "snippets": [
                "Conversely, setting the number too high may introduce uninterpretable topics that pick out idiosyncratic word combinations (Griffiths et al., 2004).While one can follow computational linguistic literature and calculate perplexity score (fitness) of the LDA model based on different numbers of topics (Blei et al., 2009), most of the time this number is chosen ad hoc and relatively small to maintain interpretability of results.It is by all means easier both to label and overview 10 and not 100 topics.\n\nIn our study we will choose the number of topics that aims to optimize not just perplexity score of the model, but also semantic coherence and exclusivity of the topics.In simple words, the perplexity score is goodness of the LDA model trained on one part of the data to predict the word distribution on the other part of it.Semantic coherence quantifies the extent to which frequent words from the same topic tend to appear in the same patents.Exclusivity analyzes whether popular words from one topic have low likelihood in all other topics.Thus, we follow (Roberts et al., 2014) in arguing that semantically interpretable topics should consist of words that tend to co-occur within documents, and that their top keywords are unlikely to overlap with keywords from other topics."
              ],
              "metadata": null
            },
            {
              "id": "(Olson et al., 2023)",
              "snippets": [
                "In the literature, there are different methods or metrics for evaluating the quality of topic models [49]. The model fit was estimated by comparing the residuals [52] of the topic models, with the number of topics ranging from 10 to 150. The results are displayed in Figure 3. The results indicated that the optimal number of topics was between 40 and 100, and, specifically, the residual was the lowest when k was 48, 52, and 57. The authors reviewed the results of three models (k = 48, 52, 57) in terms of topic interpretability, and 52 was chosen as the optimal k value."
              ],
              "metadata": null
            },
            {
              "id": "(Jain et al., 2019)",
              "snippets": [
                "In this section, we perform a study on choosing the right number of topics(k) in Table 6. Here, topic uniqueness refers to the proportion of unique words in a topic, computed over the top words in the vocabulary. Higher the topic uniqueness score, more distinct are the obtained topics. We compute the Spearman correlation on the avgSim metric using the word pairs from RG-65 (Rubenstein et al., 1965). With k = 10, we obtained a topic uniqueness of 32.23, which dropped to 27.12 for k=20 topics. Thus increasing the number of topics increases overlap which harms our model as the topic weight gets divided while training the embeddings. This effect can be clearly seen in the correlation coefficient which drops from 68.5 to 66.9 for 10 & 20 topics respectively. Using k=5 improved the topic uniqueness score to 34.05, but the perplexity score (Blei et al., 2009) reduced, indicating that the topic model requires more degrees of freedom to fit the corpus. We also observed not very distinct topics at k=5 (i.e. a topic could be mixture of sports and history), resulting in reduced correlation coefficient of 67.1."
              ],
              "metadata": null
            },
            {
              "id": "(Lee et al., 2020)",
              "snippets": [
                "One of the methods for determining the optimal number of topics is to select the lowest number of topics measured in the LDA result, i.e., the topic-keyword matrix (Blei, 2012). \n\nIn this study, the cosine similarity for each document was used to determine the optimal number of topics (=K). The cosine similarity is closer to one as the similarity between documents becomes higher. Therefore, there is less topic similarity with a lower cosine similarity. The selection criteria for the number of topics were based on the low cosine similarity. Figure 4 shows how the similarity value changes with the K value, such as from K = five to 11. These are clear data that show the K selection."
              ],
              "metadata": null
            },
            {
              "id": "(Wei, 2021)",
              "snippets": [
                "Computing the topic coherence score for different numbers of topics and choosing the model that gives the highest topic coherence would be one of the ways. Perplexity and log-likelihood based V-fold cross validation are also good options for finding the optimal number of topics. However, there is no standard to say which way is the best way to pick the optimal number of topics. Sometimes, the statistical optimal number of topics may not fit the business problem well."
              ],
              "metadata": null
            },
            {
              "id": "(Lu et al., 2022)",
              "snippets": [
                "In this paper, perplexity is used to determine the number of topics, and the experimental results are shown in Figure 5. According to the elbow method, it can be seen that the LDA model has the lowest perplexity when K = 10; therefore, setting topics = 10 and iterations = 500, and presenting the high-frequency words (top-10) in the topic-word distribution of each topic."
              ],
              "metadata": null
            },
            {
              "id": "(Aditya et al., 2023)",
              "snippets": [
                "Topic modeling is categorized as an unsupervised learning technique that offers a time-efficient approach since it does not require researchers to manually code a training set of documents prior to the analysis (Asmussen et al., 2019). Along with the benefits, the optimal number of topics may not be predetermined. Analysts are required to identify the optimal number of topics in order to achieve the best possible distribution of topics across documents and the distribution of keywords within each topic using the LDA algorithm to achieve an ideal topic-keyword composition (Islam, 2019). Coherence metrics have been introduced to differentiate between good and bad topics based on the interpretability of their top words (Mimno et al., 2011). The coherence score measures how semantically close the most prominent words are in a given topic (Morstatter et al., 2018). Typically, multiple models are constructed with varying numbers of topics (k), and the optimal number of topics is determined by selecting the number of topics that produces the highest coherence score. This number of topics often provides a more relevant and easily interpretable division of topics (Islam, 2019)."
              ],
              "metadata": null
            },
            {
              "id": "(Zankadi et al., 2022)",
              "snippets": [
                "The approach adopted to find the optimal number of topics for LDA and LSI models is to build many models (LDA and LSI) with different values for a number of topics (k) and pick the one with the highest coherence value concerning the significance of the generated topics.\n\nThe search for the optimal number of topics started with a range from two to 98, with a step of 6. During the process, only one hyperparameter varied (number of topics) and the other remained unchanged until reaching the highest coherence score. Figures 3 and 4 present the values of the coherence score about the number of topics for LDA and LSA models with BOW and TFIDF corpora.\n\nThe optimal number of topics of each model is the one that has the highest coherence score concerning the topic's relevance and significance. The training of the BERTopic model results in several topics equal to 40 with a coherence score equal to 0,62."
              ],
              "metadata": null
            },
            {
              "id": "(Abdulaziz et al., 2021)",
              "snippets": [
                "As previously mentioned, large number of topics can lead to wasted topics and in contrast, small number of topics can hide useful topics. Therefore, deciding the number of topics depends on the size and the general subject of the documents. For the documents, the paper implemented LDA with K =5-10 and 15 and evaluated the quality of the outputs manually. The paper focused on the weight of words and the meaning of words in each topic. This paper found that an increase the words more than 10 words to each topics causes sharing some words between the topics as well as words with low weight."
              ],
              "metadata": null
            },
            {
              "id": "(Chae et al., 2018)",
              "snippets": [
                "Corporate social responsibility (CSR) is an essential business practice in industry and a popular topic in academic research. Several studies have attempted to understand topics or categories in CSR contexts and some have used qualitative techniques to analyze data from traditional communication channels such as corporate reports, newspapers, and websites. This study adopts computational content analysis for understanding themes or topics from CSR-related conversations in the Twitter-sphere, the largest microblogging social media platform. Specifically, a probabilistic topic modeling-based computational text analysis framework is introduced to answer three questions: (1) What CSR-related topics are being communicated in the Twitter-sphere and what are the prevalent topics or themes in CSR conversation? (topic prevalence); (2) How are those topics interrelated? (topic correlation); (3) How have those topics changed over time? (topic evolution). The topic modeling results are discussed, and the direction for future research is presented."
              ],
              "metadata": null
            },
            {
              "id": "(Xue et al., 2021)",
              "snippets": [
                "According to the method for evaluating topic modeling results in [26], picking a number that represents the end of a rapid growth of the CS usually provides an interpretable results, which is 5 in our case. Choosing a higher value of the number of topics with higher CS results in diminishing returns. Although as we continue to increase the number of topics, it gives us a higher CS, we reduce the number of candidate users as shown in column \"Average number of candidate users\" of Table 2. Also, this increase of CS might also be caused by setting the number of topics too large for the dataset so that the same keywords are repeated in multiple topics [26]."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Practical Considerations and Best Practices",
          "text": "\n## Balancing Statistical Indicators with Practical Needs\n\nWhen selecting the optimal number of topics for a modeling task, practitioners must balance statistical metrics with practical considerations. While perplexity and coherence scores provide objective measurements, they don't always lead to the most useful or interpretable results. Perplexity tends to favor larger numbers of topics without reaching a clear optimum, potentially resulting in topics with higher similarity that are difficult to distinguish <Paper corpusId=\"214557002\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. Coherence scores, on the other hand, may indicate an optimal number of topics statistically, but the resulting model might not align well with specific business or research objectives <Paper corpusId=\"237437304\" paperTitle=\"(Wei, 2021)\" isShortName></Paper>.\n\nA practical approach involves considering both perplexity and coherence together when selecting the optimal topic count. The ideal configuration should demonstrate both low perplexity and high coherence to balance predictive power with interpretability <Paper corpusId=\"260261038\" paperTitle=\"(Xiang et al., 2023)\" isShortName></Paper>. However, as coherence continues to increase with more topics, researchers should be wary of diminishing returns\u2014selecting a number that represents the end of rapid growth in coherence scores often provides interpretable results while avoiding unnecessary complexity <Paper corpusId=\"235349283\" paperTitle=\"(Xue et al., 2021)\" isShortName></Paper>.\n\n## Corpus-Specific Considerations\n\nThe appropriate number of topics depends significantly on the size and general subject matter of the corpus. Larger document collections may benefit from more granular topic divisions, while smaller corpora often require fewer topics to maintain meaningful distinctions <Paper corpusId=\"232168004\" paperTitle=\"(Abdulaziz et al., 2021)\" isShortName></Paper>. Research has shown that increasing the number of topics beyond what is appropriate for a given corpus can lead to word overlap between topics and diminished topic uniqueness scores <Paper corpusId=\"202583512\" paperTitle=\"(Jain et al., 2019)\" isShortName></Paper>.\n\nFor example, one study found that when increasing from 10 to 20 topics, topic uniqueness dropped from 32.23 to 27.12, indicating greater word overlap between topics. Conversely, reducing to 5 topics improved uniqueness to 34.05 but resulted in less distinct topics that mixed different domains (like sports and history) and reduced correlation coefficients <Paper corpusId=\"202583512\" paperTitle=\"(Jain et al., 2019)\" isShortName></Paper> <Paper corpusId=\"18309234\" paperTitle=\"(Rubenstein et al., 1965)\" isShortName></Paper>. These findings suggest that corpus-specific testing is essential to find the sweet spot where topics remain distinct without becoming overly general.\n\n## Moderate Topic Numbers for Interpretability\n\nExtensive experimentation has demonstrated that moderate numbers of topics (K=10 or K=20) generally work well for identifying key themes while maintaining interpretability <Paper corpusId=\"15240372\" paperTitle=\"(Randhawa et al., 2016)\" isShortName></Paper>. This approach provides a good balance between capturing sufficient detail and avoiding excessive fragmentation of related content. When topic numbers are set too high (e.g., K=50), the model tends to create many specialized, small clusters that may be difficult to manage or interpret. Conversely, when set too low (e.g., K=5), topics become diffuse and may combine unrelated themes <Paper corpusId=\"15240372\" paperTitle=\"(Randhawa et al., 2016)\" isShortName></Paper>.\n\nThe concept of robust topic identification is particularly notable\u2014research has shown that the top-ranked topics often remain consistent across different topic number settings, indicating that key themes will be captured regardless of the specific parameter choice <Paper corpusId=\"15240372\" paperTitle=\"(Randhawa et al., 2016)\" isShortName></Paper>. This robustness provides some reassurance that the most important content will be represented even when the perfect topic number remains uncertain.\n\n## Iterative Testing and Manual Evaluation\n\nGiven the challenges in determining the optimal number of topics algorithmically, many researchers advocate for an iterative approach combined with human judgment. Building multiple models with different numbers of topics (k) and selecting the one that gives the highest coherence score is a common strategy <Paper corpusId=\"189998766\" paperTitle=\"(Islam, 2019)\" isShortName></Paper>. Alternatively, researchers may calculate coherence measures for a range of topic numbers (e.g., 1 to 25) and select the configuration with maximum coherence <Paper corpusId=\"225062393\" paperTitle=\"(Schafermeier et al., 2020)\" isShortName></Paper>.\n\nFor increased reliability, some experts recommend training multiple models for each potential topic number and selecting the one with maximum coherence, allowing the process to be automated while still finding the best performer for each configuration <Paper corpusId=\"225062393\" paperTitle=\"(Schafermeier et al., 2020)\" isShortName></Paper>. However, there's growing recognition that statistical metrics alone cannot determine the most appropriate number of topics\u2014interpretability by human experts remains a crucial criterion that cannot be fully automated <Paper corpusId=\"261321685\" paperTitle=\"(Andrienko et al., 2023)\" isShortName></Paper>.\n\n## Visualization and Alternative Selection Methods\n\nSome researchers employ more sophisticated approaches to topic number selection. One method involves running a topic modeling algorithm with different parameters, projecting the resulting topics into a single embedding space, and exploring the distribution to identify natural clusters. The number of visible distinct clusters in the projection suggests potentially suitable topic numbers <Paper corpusId=\"261321685\" paperTitle=\"(Andrienko et al., 2023)\" isShortName></Paper>.\n\nAnother technique uses the reconstruction error from Non-negative Matrix Factorization (NMF) to identify an optimal topic number through the \"elbow criterion,\" finding the point where increasing topics provides negligible improvement in error reduction. However, this approach may recommend very large numbers of topics (e.g., around 500) that are impractical for human interpretation <Paper corpusId=\"225062393\" paperTitle=\"(Schafermeier et al., 2020)\" isShortName></Paper>.\n\n## Balancing Multiple Optimization Criteria\n\nRather than optimizing for a single metric, some researchers argue for considering multiple criteria simultaneously. Beyond perplexity, semantic coherence and exclusivity of topics should factor into the decision. Semantic coherence quantifies how frequently words from the same topic appear together in documents, while exclusivity analyzes whether top words from one topic have low probabilities in other topics <Paper corpusId=\"235409421\" paperTitle=\"(Savin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"10183405\" paperTitle=\"(Roberts et al., 2014)\" isShortName></Paper>.\n\nThis multi-criteria approach acknowledges that semantically interpretable topics should consist of words that tend to co-occur within documents, and their top keywords should have minimal overlap with keywords from other topics <Paper corpusId=\"235409421\" paperTitle=\"(Savin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"10183405\" paperTitle=\"(Roberts et al., 2014)\" isShortName></Paper>. By considering both statistical fit and semantic quality, researchers can develop more useful and interpretable topic models.\n\n## Limitations of LDA for Topic Number Selection\n\nTraditional LDA approaches face additional challenges related to topic number selection. The model ignores word order and semantics, making it less sensitive to contextual meaning and smaller topics. This limitation can prevent LDA from isolating finer details in text, regardless of the topic number selected <Paper corpusId=\"259145117\" paperTitle=\"(Lakatos et al., 2023)\" isShortName></Paper> <Paper corpusId=\"5652538\" paperTitle=\"(Hoffman et al., 2012)\" isShortName></Paper>.\n\nIn practice, focusing on the top 10 words from each topic (as is common with LDA) can narrow down the list of key terms, especially when the number of topics is small. Increasing the number of words per topic often leads to diminished quality and greater overlap between topics, requiring further corrections <Paper corpusId=\"259145117\" paperTitle=\"(Lakatos et al., 2023)\" isShortName></Paper>. This inherent limitation of LDA suggests that topic number selection should be considered alongside other model constraints when planning a topic modeling approach.\n\n## Final Selection and Validation\n\nAfter statistical evaluation, most experts recommend a final validation through human judgment. Researchers should review topic outputs with different numbers of topics and determine which configuration provides the most meaningful and interpretable results for their specific research question <Paper corpusId=\"256968901\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>. This manual assessment is particularly important when the automatically determined \"optimal\" number of topics seems too large or small for practical interpretation.\n\nSelecting an appropriate number of topics remains crucial for meaningful output, as highlighted by various researchers. Choosing a high value may result in substantial topic overlap, while a low value would overgeneralize and force diverse content into a limited number of topics <Paper corpusId=\"257353421\" paperTitle=\"(Mulahuwaish et al., 2023)\" isShortName></Paper>. This fundamental tension underscores why topic number selection remains as much an art as a science, requiring both statistical rigor and domain expertise.",
          "citations": [
            {
              "id": "(Xu et al., 2020)",
              "snippets": [
                "Many studies have shown that the quality of LDA topic model modeling is closely related to the selection of the number of topics. Therefore, choosing an optimal number of topics is the focus of the LDA topic model application, and it is also difficult. The most commonly used method is to use Perplexity as the evaluation criterion and choose the model with the minimum Perplexity to determine the optimal number of topics. Although the minimum Perplexity can make the prediction ability of the model the best, it tends to choose a larger number of topics, which leads to higher similarity between the selected topics, resulting in problems that are difficult to identify the topic, and affecting the result of further analysis. After comprehensive consideration, select a certain range of values to make multiple attempts, and determine the appropriate number of topics by artificial selection."
              ],
              "metadata": null
            },
            {
              "id": "(Wei, 2021)",
              "snippets": [
                "Computing the topic coherence score for different numbers of topics and choosing the model that gives the highest topic coherence would be one of the ways. Perplexity and log-likelihood based V-fold cross validation are also good options for finding the optimal number of topics. However, there is no standard to say which way is the best way to pick the optimal number of topics. Sometimes, the statistical optimal number of topics may not fit the business problem well."
              ],
              "metadata": null
            },
            {
              "id": "(Xiang et al., 2023)",
              "snippets": [
                "In topic modeling, perplexity and coherence are commonly used to evaluate the quality and interpretability of models. Perplexity is a metric used to measure a model's ability to fit new text data, with lower perplexity indicating better predictive capabilities. Coherence, on the other hand, is used to assess the interpretability of the topics generated by a topic model, with higher coherence indicating better quality of generated topics. Generally, perplexity decreases with an increasing number of topics, while coherence initially increases and then decreases with the number of topics. Therefore, when selecting the optimal number of topics, it is necessary to consider the changes in perplexity and coherence together. The optimal number of topics should have both low perplexity and the highest point of coherence. This approach helps avoid overfitting issues while improving the quality and interpretability of the topic model."
              ],
              "metadata": null
            },
            {
              "id": "(Xue et al., 2021)",
              "snippets": [
                "According to the method for evaluating topic modeling results in [26], picking a number that represents the end of a rapid growth of the CS usually provides an interpretable results, which is 5 in our case. Choosing a higher value of the number of topics with higher CS results in diminishing returns. Although as we continue to increase the number of topics, it gives us a higher CS, we reduce the number of candidate users as shown in column \"Average number of candidate users\" of Table 2. Also, this increase of CS might also be caused by setting the number of topics too large for the dataset so that the same keywords are repeated in multiple topics [26]."
              ],
              "metadata": null
            },
            {
              "id": "(Abdulaziz et al., 2021)",
              "snippets": [
                "As previously mentioned, large number of topics can lead to wasted topics and in contrast, small number of topics can hide useful topics. Therefore, deciding the number of topics depends on the size and the general subject of the documents. For the documents, the paper implemented LDA with K =5-10 and 15 and evaluated the quality of the outputs manually. The paper focused on the weight of words and the meaning of words in each topic. This paper found that an increase the words more than 10 words to each topics causes sharing some words between the topics as well as words with low weight."
              ],
              "metadata": null
            },
            {
              "id": "(Jain et al., 2019)",
              "snippets": [
                "In this section, we perform a study on choosing the right number of topics(k) in Table 6. Here, topic uniqueness refers to the proportion of unique words in a topic, computed over the top words in the vocabulary. Higher the topic uniqueness score, more distinct are the obtained topics. We compute the Spearman correlation on the avgSim metric using the word pairs from RG-65 (Rubenstein et al., 1965). With k = 10, we obtained a topic uniqueness of 32.23, which dropped to 27.12 for k=20 topics. Thus increasing the number of topics increases overlap which harms our model as the topic weight gets divided while training the embeddings. This effect can be clearly seen in the correlation coefficient which drops from 68.5 to 66.9 for 10 & 20 topics respectively. Using k=5 improved the topic uniqueness score to 34.05, but the perplexity score (Blei et al., 2009) reduced, indicating that the topic model requires more degrees of freedom to fit the corpus. We also observed not very distinct topics at k=5 (i.e. a topic could be mixture of sports and history), resulting in reduced correlation coefficient of 67.1."
              ],
              "metadata": null
            },
            {
              "id": "(Rubenstein et al., 1965)",
              "snippets": [
                "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
              ],
              "metadata": null
            },
            {
              "id": "(Randhawa et al., 2016)",
              "snippets": [
                "Similar to other topic modeling methods, our algorithm takes as input the number of topics to be extracted, K. For the results described so far, we fixed K = 10. Table 6 displays the top-four topics for Mr. Kaminski when the number of topics K is set to 5 and 50. Notice that in both cases, the overall word score remains unaffected by the change in K, the only change is the clustering of topics. Observe that the top topic is identical for these two cases, and for the case K = 10 of Table 2. Even the second-ranked topic is quite similar. This illustrates the robustness of our approach in capturing the key topics from a corpus. Notice that the case K = 5 is quite crude in the sense that all words are clubbed into 5 clusters and hence we observe a diffuse topic such as Topic 4. On the other hand K = 50 is much more pointed and picks up a large number of small clusters, which leads to identifying many specialized topics. In our extensive experiments, we found that moderate K values, such as K = 10 or 20, work well in identifying key topics."
              ],
              "metadata": null
            },
            {
              "id": "(Islam, 2019)",
              "snippets": [
                "To find out the optimal number of topic, we build many LSA, NMF, LDA models with different values of number of topics (k) and pick the one that gives the highest coherence score. Choosing a 'k' that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics."
              ],
              "metadata": null
            },
            {
              "id": "(Schafermeier et al., 2020)",
              "snippets": [
                "We calculated the coherence measure C V for every topic number from 1 to 25",
                "We achieved the maximum coherence with a topic number of t = 22. We therefore use 22 topics in our subsequent analysis. Note that for each topic number, we trained one single instance of the NMF model. We did this with a fixed random seed to ensure reproducibility. NMF is nondeterministic and the achieved model quality could actually vary from run to run. Our reasoning here was that NMF is still relatively stable. However, for future work we would recommend to train multiple models for each topic number and use the one with the maximum coherence. Using this method the full process of finding a model with a good topic number can be automated.\n\nBefore the C V measure, we experimented with other heuristics to select a topic number. As an example, we tried to take advantage of the matrix reconstruction error mentioned in section 2.2. This error optimally should be a monotonically decreasing function of the number of topics, since the convex combination of more topic vectors should allow for an equally or more accurate document reconstruction. Our intuition was that plotting the reconstruction error for an increasing number of topics should result in a curve that starts with a strong (negative) slope and slowly converges to a minimal error with a slope close to zero. Our idea was then to select the topic number through the elbow criterion, i.e., by finding a point where the further decrease of the error with increasing t is neglectible. Figure 3.2 (right) shows the curve resulting from this approach. While it matches our intuition, the optimal number of topics would lie somewhere near 500. We find this number too large to give a comprehensible overview over research topics. We conjecture, however, that our approach would lead to success in selecting a good model for other downstream tasks such as classification."
              ],
              "metadata": null
            },
            {
              "id": "(Andrienko et al., 2023)",
              "snippets": [
                "To determine the target number of topics, we used the approach proposed by Chen et al. [CAA*20]: running the selected method with different parameters, projecting the topics generated in the different runs into a single embedding space using a dimensionality reduction method, and exploring the topic distribution in this space, which is expected to reflect the similarities and differences between the topics. The number of visible distinct clusters in the projection suggests the potentially suitable number of topics. However, the main criterion is interpretability of the topics by a human. As it cannot be formally evaluated, we do not see a feasible way to fully automate the selection of the suitable number of topics."
              ],
              "metadata": null
            },
            {
              "id": "(Savin et al., 2022)",
              "snippets": [
                "Conversely, setting the number too high may introduce uninterpretable topics that pick out idiosyncratic word combinations (Griffiths et al., 2004).While one can follow computational linguistic literature and calculate perplexity score (fitness) of the LDA model based on different numbers of topics (Blei et al., 2009), most of the time this number is chosen ad hoc and relatively small to maintain interpretability of results.It is by all means easier both to label and overview 10 and not 100 topics.\n\nIn our study we will choose the number of topics that aims to optimize not just perplexity score of the model, but also semantic coherence and exclusivity of the topics.In simple words, the perplexity score is goodness of the LDA model trained on one part of the data to predict the word distribution on the other part of it.Semantic coherence quantifies the extent to which frequent words from the same topic tend to appear in the same patents.Exclusivity analyzes whether popular words from one topic have low likelihood in all other topics.Thus, we follow (Roberts et al., 2014) in arguing that semantically interpretable topics should consist of words that tend to co-occur within documents, and that their top keywords are unlikely to overlap with keywords from other topics."
              ],
              "metadata": null
            },
            {
              "id": "(Roberts et al., 2014)",
              "snippets": [
                "Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments."
              ],
              "metadata": null
            },
            {
              "id": "(Lakatos et al., 2023)",
              "snippets": [
                "To adapt it to a specific context, Batch [28] or Online Variational Bayes [29] methods have become popular. To measure the quality of the topics generated by LDA and thus find the right number of topics, the perplexity [17] values of each topic are computed. For dictionary construction, the bag-of-words [30] technique is a common choice, with lemmatization and stop word removal.\n\nConsidering our system, a drawback of this model is that it is a probabilistic statistical method that ignores word order and semantics and is less sensitive to finding smaller topics. It means that it finds fewer topics and cannot isolate finer details. In practice, when using LDA, we tend to focus on e.g., the top 10 words, which narrows down the list of key terms, especially when there are few topics. Although these properties of the model are parameterizable, our experience shows that the more words we select from a given topic, the more the quality of the words or phrases associated with that topic deteriorates. This leads to greater overlap between the words of the topics, which requires further corrections."
              ],
              "metadata": null
            },
            {
              "id": "(Hoffman et al., 2012)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Zeng et al., 2023)",
              "snippets": [
                "We select the LDA method for topic detection. A tricky problem with this method is to determine the number of topics, which is important to the quality of text mining. To solve the problem, the perplexity has been widely used to find the optimal number of topics (Hasan et al., 2020), which represents the uncertainty of a text document pertaining to a particular topic in a fitted topic model. It has been proved in theory that the smaller the perplexity is, the better the model is. Hence, we may try a range of the number of topics, compare the corresponding perplexities, and then choose a number with the minimal perplexity as the optimal. In practice, however, some other factors need to be taken into consideration. For instance, the number of topics should not be too large for the ease of interpretation."
              ],
              "metadata": null
            },
            {
              "id": "(Mulahuwaish et al., 2023)",
              "snippets": [
                "The LDA algorithm considers three hyperparameters: a, b, and K. The a hyperparameter encodes the number of topics expected in each document. The b hyperparameter lets it know the distribution of words for each topic in the document, and K defines the number of topics to use. Picking an accurate K value is crucial to an interpretable output from the algorithm. Choosing a high value may result in an output with topics with substantial overlap. Conversely, a low K would generalize too strongly and place most of the meaning behind a document into a smaller pool of topics."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are advantages and disadvantages of top methods for picking the right number of topics in topic modeling?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define topic modeling and its applications.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Topic modeling is a crucial technique in natural language processing (NLP) that helps identify underlying themes or topics within a collection of documents.",
              "In addition to topic models' effective application to traditional problems like information retrieval, visualization, statistical inference, multilingual modeling, and linguistic understanding, Applications of Topic Models also reviews topic models' ability to unlock large text collections for qualitative analysis. It reviews their successful use by researchers to help understand fiction, non-fiction, scientific publications, and political texts."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should highlight the importance of picking the right number of topics in the performance of topic models.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Researchers consistently face the difficulty of deciding on the optimal number of topics in advance, which can significantly impact the effectiveness of their analyses  (7, Rahman et al., 2018). The importance of this selection process is underscored by its direct relationship to the overall performance and utility of topic models  (22, Ruiz et al., 2023). Moreover, the choice of topic number is intrinsically linked to the evaluation of topic model quality, making it a longstanding challenge that requires careful consideration and robust methodologies  (23, Pan et al., 2023). Given its critical role in shaping the outcomes of topic modeling, selecting the right number of topics remains a key focus for researchers seeking to improve the accuracy and reliability of their models."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention the most important methods for topic modeling, such as elbow method, coherence metrics, and likelihood, together with their advantages and disadvantages.",
            "weight": 0.13333333333333333,
            "evidence": [
              "### 4. Elbow Method**Advantages:**- **Simplicity:** The elbow method is easy to visualize and interpret, using a plot to identify the \"elbow point\" where adding more topics yields diminishing returns in model performance metrics.- **Broad Applicability:** It can be used with various performance metrics like coherence or perplexity.**Disadvantages:**- **Ambiguity:** The \"elbow point\" may not always be clear-cut, leading to ambiguous decisions.- **Over-reliance on Visual Inspection:** The method relies heavily on visual interpretation, which can introduce subjectivity.",
              "### 2. Coherence Metrics**Advantages:**- **Human Interpretability:** Coherence metrics, such as UMass, UCI, and NPMI, focus on the semantic similarity of words within topics, aligning well with human judgment.- **Model Quality:** High coherence often correlates with more interpretable and meaningful topics.**Disadvantages:**- **Computational Cost:** Calculating coherence metrics, especially on large datasets, can be computationally intensive.- **Dependency on External Resources:** Some coherence metrics (e.g., UCI and NPMI) require access to external corpora or search engines.",
              "### 1. Perplexity and Likelihood**Advantages:**- **Quantitative Measure:** Provides a quantitative way to compare models, helping to identify the model that best generalizes to new data.- **Ease of Implementation:** Many computational packages for LDA and other methods have built-in functions to calculate perplexity and likelihood. **Disadvantages:**- **Overfitting:** Lower perplexity does not always imply a better model. It might indicate overfitting to the training data.- **Interpretability:** Perplexity and likelihood alone do not provide qualitative insights into topic coherence or interpretability.",
              "### 5. Cross-Validation**Advantages:**- **Robustness:** Provides a robust way to estimate the generalizability of the model by training and testing across multiple subsets of the data.- **Comprehensive:** Can be combined with other metrics like coherence to balance quantitative and qualitative performance.**Disadvantages:**- **Computationally Intensive:** Cross-validation can be very time-consuming, especially with large datasets and complex models.- **Implementation Complexity:** Requires setting up and running multiple models, which can be technically challenging."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should conclude that the choice of the method depends on different factors such as computational capability, the intended application, and maybe a mix of various methods would be required.",
            "weight": 0.13333333333333333,
            "evidence": [
              "In practice, a combination of these methods is often used to make a well-informed decision. For example, one might use perplexity or coherence to narrow down a range of potential topic numbers, then use stability analysis or manual inspection to make a final decision. It's also worth noting that the \"right\" number of topics can be subjective and may vary depending on the intended use of the model. Therefore, domain expertise and the specific goals of the analysis should always be considered alongside these quantitative methods."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could name LDA as one of the most important methods for topic modeling.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Latent Dirichlet Allocation (LDA), as it is the most used [6,12,17,20,32], state-of-the-art method [25] and simplest method [8].",
              "The topic modelling method LDA is an unsupervised, probabilistic modelling method which extracts topics from a collection of papers."
            ]
          }
        ]
      }
    }
  },
  {
    "question": "What publicly available datasets are typically used for evaluating type inference systems in python?",
    "response": {
      "sections": [
        {
          "title": "Introduction/Background to Type Inference Evaluation",
          "text": "\nType inference is a critical component of modern programming language tooling that automatically deduces the types of variables, function parameters, and return values without explicit type annotations. In Python, which transitioned from a dynamically typed language to supporting optional static typing through type hints (PEP 484), evaluating type inference systems poses unique challenges.\n\nType inference evaluation typically measures how accurately a system can predict types compared to ground truth annotations. These evaluations assess precision (correctness of inferred types), recall (percentage of variables that receive predictions), and F1 scores (harmonic mean of precision and recall). Performance is often measured across different categories of types, from basic primitives (int, str) to complex collections and custom classes.\n\nTo properly evaluate type inference systems, researchers need datasets that contain Python code with reliable ground truth type annotations. These datasets ideally should represent diverse programming patterns, varying complexity levels, and real-world usage scenarios. The evaluation process typically involves removing existing type annotations, running the inference system, and comparing the inferred types against the original annotations. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
          "citations": []
        },
        {
          "title": "Major Benchmark Datasets",
          "text": "\nThe research community has developed and adopted several key datasets for evaluating type inference systems in Python:\n\n## ManyTypes4Py\nManyTypes4Py is one of the most widely used datasets in recent evaluations of Python type inference systems <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper>. This large-scale dataset contains 5,382 Python projects with over 869,000 type annotations. A notable feature of ManyTypes4Py is that it addresses code duplication issues by removing duplicate source files, which helps mitigate duplication bias that can artificially inflate performance metrics <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper>. The dataset is formatted in JSON and comes with a lightweight static analyzer pipeline to extract type information from abstract syntax trees (ASTs). For evaluation purposes, ManyTypes4Py is typically split into training, validation, and test sets by files <Paper corpusId=\"259951409\" paperTitle=\"(Peng et al., 2023)\" isShortName></Paper>.\n\n## Typilus Dataset\nThe dataset released by Allamanis et al. alongside their Typilus model has become another standard benchmark <Paper corpusId=\"216056383\" paperTitle=\"(Allamanis et al., 2020)\" isShortName></Paper>. This dataset contains approximately 600 Python projects with type annotations. Initially, the Typilus dataset provided graph representations of source code specifically designed for training the Typilus model <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper>. Several subsequent studies have used this dataset for evaluation, including HiTyper <Paper corpusId=\"246680113\" paperTitle=\"(Peng et al., 2021)\" isShortName></Paper> and Li et al.'s cross-language type inference approach <Paper corpusId=\"250072092\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\n## TypeWriter Dataset\nThe TypeWriter OSS (Open Source Software) dataset, compiled by Pradel et al., focuses on Python functions from GitHub repositories <Paper corpusId=\"208909790\" paperTitle=\"(Pradel et al., 2019)\" isShortName></Paper>. This dataset has been used in evaluations of various approaches, including Fried et al.'s work on incoder <Paper corpusId=\"248157108\" paperTitle=\"(Fried et al., 2022)\" isShortName></Paper>. When using this dataset for evaluation, researchers typically process the functions to identify those with PEP 484 return type hint annotations and remove other types of annotations for testing inference capabilities.\n\n## CrossDomainTypes4Py\nFor evaluating domain adaptation in type inference, Gruner et al. introduced the CrossDomainTypes4Py dataset <Paper corpusId=\"251710434\" paperTitle=\"(Gruner et al., 2022)\" isShortName></Paper>. This dataset is designed to support transfer learning methods for type inference across different programming domains and is often used alongside ManyTypes4Py for cross-domain experiments.\n\nThese benchmark datasets have been consistently used across multiple studies, enabling fair comparisons between different type inference approaches <Paper corpusId=\"266573806\" paperTitle=\"(Venkatesh et al., 2023)\" isShortName></Paper> <Paper corpusId=\"270878649\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Each dataset offers different characteristics in terms of size, diversity, and annotation quality, allowing researchers to evaluate their models under various conditions.",
          "citations": [
            {
              "id": "(Mir et al., 2021)",
              "snippets": [
                "Recently, Allamanis et al. [8] proposed the Typilus model, which is a graphbased neural model that predicts type annotations for Python. The Typilus model [8] is accompanied by a dataset that contains 600 Python projects. Moreover, the source code files of Typilus' dataset are converted to graph representations that are only suitable for training the Typilus model. The Many-Types4Py dataset provides JSON-formatted analyzed source [16] is not collected solely for the ML-based type inference task, meaning that a large number of projects in the dataset may not have type annotations at all, especially given the time that the dataset was created. Allamanis [12] showed that the Python-150K dataset suffers from code duplication despite the removal of project forks."
              ],
              "metadata": null
            },
            {
              "id": "(Peng et al., 2023)",
              "snippets": [
                "We follow previous work (Mir et al., 2021), (Peng et al., 2021) and evaluate our approach on the ManyTypes4Py dataset (Mir et al., 2021) by splitting the dataset into a training set and a test set with an 80:20 ratio. We use the training set to train the baseline models and select examples for TYPEGEN and evaluate the performance of TYPEGEN and baselines in the test set."
              ],
              "metadata": null
            },
            {
              "id": "(Allamanis et al., 2020)",
              "snippets": [
                "Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program\u2019s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace \u2014 a continuous relaxation of the discrete space of types \u2014 and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered."
              ],
              "metadata": null
            },
            {
              "id": "(Peng et al., 2021)",
              "snippets": [
                "We evaluate HiTyper on two public datasets. One dataset is released by Allamanis et al. in the paper of Typilus (Allamanis et al., 2020), and the other is ManyTypes4Py [35], one large dataset recently released for this task."
              ],
              "metadata": null
            },
            {
              "id": "(Li et al., 2021)",
              "snippets": [
                "For Python, we used the dataset provided by Allamanis et al. (Allamanis et al., 2020). For Java, we used the CodeSearchNet dataset [24]",
                "For TypeScript, we used a TypeScript dataset (Hellendoorn et al., 2018) provided by Hellendoorn et al."
              ],
              "metadata": null
            },
            {
              "id": "(Pradel et al., 2019)",
              "snippets": [
                "We collect Python from a large internal code repository. OSS corpus We search GitHub for all projects tagged as python3. \n\nWe also search Libraries.io for all Python projects that include mypy as a dependency. We then remove all projects that have less than 50 stars on GitHub, to ensure that the included projects are of substantial public interest. To ease future work to compare with TypeWriter, all results for the OSS corpus are available for download."
              ],
              "metadata": null
            },
            {
              "id": "(Fried et al., 2022)",
              "snippets": [
                "We evaluate on two datasets: one we construct from CodeXGLUE and the dataset from TypeWriter OSS (Pradel et al., 2019)",
                "We run an abstract syntax tree (AST) processor on all functions in the development and test sets of this dataset to (1) identify functions with a PEP 4846 return type hint annotation that is not None and (2) remove all other type",
                "To compare to the supervised TypeWriter approach, we obtain its predictions on the open-source software (OSS) dataset used in that work (Pradel et al., 2019), consisting of Python functions from GitHub."
              ],
              "metadata": null
            },
            {
              "id": "(Gruner et al., 2022)",
              "snippets": [
                "In this paper, we demonstrate the negative impact of the mentioned issues on the Type4Py type inference system and aim to mitigate them by using transfer learning methods (Ganin et al., 2015), (Shen et al., 2017). For our cross-domain experiments we use the benchmark dataset ManyTypes4Py (Mir et al., 2021) and our new Cross-DomainTypes4Py dataset, which we present in this paper."
              ],
              "metadata": null
            },
            {
              "id": "(Venkatesh et al., 2023)",
              "snippets": [
                "Current literature primarily assesses the performance of such type inference tools based on large-scale real-world benchmark datasets, notably Type4Py, Hi-Typer, and Typilus [4]. On the contrary, open-sourced solutions only rely on specifically-designed test cases."
              ],
              "metadata": null
            },
            {
              "id": "(Wang et al., 2024)",
              "snippets": [
                "In line with prior studies [2], [17], we conduct the evaluation on the ManyTypes4Py dataset [27] for testing."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Dataset Sources and Collection Methods",
          "text": "\nThe construction of evaluation datasets for type inference systems follows several common approaches, with GitHub being the primary source of Python code. Researchers typically apply specific criteria to select repositories that are likely to contain type annotations. For instance, Boone et al. mined 5,996 open-source projects from GitHub and Libraries.io that were likely to include type annotations for their model training <Paper corpusId=\"208527555\" paperTitle=\"(Boone et al., 2019)\" isShortName></Paper>.\n\nWhen collecting from GitHub, researchers often employ quality filters to ensure the selected repositories represent meaningful code. Pradel et al. searched for projects tagged as \"python3\" on GitHub and additionally used Libraries.io to find Python projects that include mypy (a popular static type checker) as a dependency. They further refined their selection by removing projects with fewer than 50 stars on GitHub \"to ensure that the included projects are of substantial public interest\" <Paper corpusId=\"208909790\" paperTitle=\"(Pradel et al., 2019)\" isShortName></Paper>.\n\nDataset creation typically involves extensive data cleaning and deduplication steps. The ManyTypes4Py dataset addresses the issue of code duplication by removing duplicate source files, a problem that Allamanis identified in other datasets like Python-150K <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper> <Paper corpusId=\"56482376\" paperTitle=\"(Allamanis, 2018)\" isShortName></Paper>. This deduplication is crucial as code duplication can artificially inflate performance metrics, sometimes by up to 100%, leading to unreliable evaluation results.\n\nSome researchers combine multiple sources to create more comprehensive datasets. Cui et al. created the typePY dataset from 4,577 top-starred GitHub repositories and supplemented it with the probPY dataset (published by Xu et al.) and the typeshed dataset, which contains human-labeled annotations for Python standard libraries and third-party packages. They reported both original annotation counts and counts after \"data cleaning and deduplication,\" highlighting the importance of these preprocessing steps <Paper corpusId=\"235658605\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\nThe collection methods reflect different priorities in dataset construction. While some datasets focus on quantity and diversity of code examples, others prioritize the quality and reliability of annotations. The typeshed dataset, for example, contains human-labeled type annotations, making it particularly reliable, but it covers only function parameters and return types and lacks contextual information due to frequent code updates <Paper corpusId=\"235658605\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.",
          "citations": [
            {
              "id": "(Boone et al., 2019)",
              "snippets": [
                "Using 5,996 open source projects mined from GitHub and Libraries.io that are likely to have type annotations, we train the model to predict types of functions without annotations."
              ],
              "metadata": null
            },
            {
              "id": "(Pradel et al., 2019)",
              "snippets": [
                "We collect Python from a large internal code repository. OSS corpus We search GitHub for all projects tagged as python3. \n\nWe also search Libraries.io for all Python projects that include mypy as a dependency. We then remove all projects that have less than 50 stars on GitHub, to ensure that the included projects are of substantial public interest. To ease future work to compare with TypeWriter, all results for the OSS corpus are available for download."
              ],
              "metadata": null
            },
            {
              "id": "(Mir et al., 2021)",
              "snippets": [
                "Recently, Allamanis et al. [8] proposed the Typilus model, which is a graphbased neural model that predicts type annotations for Python. The Typilus model [8] is accompanied by a dataset that contains 600 Python projects. Moreover, the source code files of Typilus' dataset are converted to graph representations that are only suitable for training the Typilus model. The Many-Types4Py dataset provides JSON-formatted analyzed source [16] is not collected solely for the ML-based type inference task, meaning that a large number of projects in the dataset may not have type annotations at all, especially given the time that the dataset was created. Allamanis [12] showed that the Python-150K dataset suffers from code duplication despite the removal of project forks."
              ],
              "metadata": null
            },
            {
              "id": "(Allamanis, 2018)",
              "snippets": [
                "The field of big code relies on mining large corpora of code to perform some learning task towards creating better tools for software engineers. A significant threat to this approach was recently identified by Lopes et al. (2017) who found a large amount of near-duplicate code on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this work, we explore the effects of code duplication on machine learning models showing that reported performance metrics are sometimes inflated by up to 100% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present a duplication index for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them. Finally, we release tools to help the community avoid this problem in future research."
              ],
              "metadata": null
            },
            {
              "id": "(Cui et al., 2021)",
              "snippets": [
                "Our datasets are summarized in Table I. The Original and Valid present the number of annotations before and after data cleaning and deduplication. typePY is our source code dataset collected from 4,577 top-star GitHub repositories. probPY stands for the dataset published in Xu et al. (Xu et al., 2016), and typeshed is a human-labeled dataset containing only the annotations for function parameters and return values [14]",
                "For the probPY [13] dataset, we exploit the merged data combining the result of PySonar2 and dynamic analysis",
                "The typeshed dataset contains human-label type annotations for Python standard libraries and third-party packages, making it more reliable. However, the typeshed dataset only covers the annotations for function parameters and return types, and there is no contextual information due to frequent code updates."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Dataset Characteristics and Statistics",
          "text": "\n- **ManyTypes4Py**: Contains 5,382 Python projects with more than 869,000 type annotations. The dataset has been cleaned to remove duplicate source code files, addressing the duplication bias that can artificially inflate performance metrics. It is structured in JSON format and typically split into training, validation, and test sets by files. <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper>\n\n- **Typilus Dataset**: Comprises approximately 600 Python projects. The source code files are converted to graph representations designed for training the Typilus model, though this format may limit usability for other approaches. <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper> <Paper corpusId=\"216056383\" paperTitle=\"(Allamanis et al., 2020)\" isShortName></Paper>\n\n- **typePY**: Collected from 4,577 top-starred GitHub repositories. The dataset underwent cleaning and deduplication processes, resulting in a notable difference between original and valid annotation counts. <Paper corpusId=\"235658605\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>\n\n- **probPY**: Published by Xu et al., this dataset combines results from static analysis (PySonar2) and dynamic analysis approaches to gather type information. <Paper corpusId=\"235658605\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>\n\n- **typeshed**: A human-labeled dataset containing annotations for Python standard libraries and third-party packages. While considered more reliable due to human labeling, it only covers function parameters and return types without contextual information. The dataset faces challenges due to frequent code updates. <Paper corpusId=\"235658605\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>\n\n- **Pyn Benchmark**: Used by Pynguin (a test generation framework), this benchmark contains 17 real-world Python projects collected from datasets including BugsInPy and ManyTypes4Py. It has been adopted by multiple tools such as CodaMosa and Coverup. <Paper corpusId=\"277104003\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper> <Paper corpusId=\"246706202\" paperTitle=\"(Lukasczyk et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259860757\" paperTitle=\"(Lemieux et al., 2023)\" isShortName></Paper>\n\n- **Boone et al. Dataset**: Contains 5,996 open-source projects mined from GitHub and Libraries.io, specifically selected for their likelihood of having type annotations. Used for training models to predict types for functions without annotations. <Paper corpusId=\"208527555\" paperTitle=\"(Boone et al., 2019)\" isShortName></Paper>",
          "citations": [
            {
              "id": "(Mir et al., 2021)",
              "snippets": [
                "Recently, Allamanis et al. [8] proposed the Typilus model, which is a graphbased neural model that predicts type annotations for Python. The Typilus model [8] is accompanied by a dataset that contains 600 Python projects. Moreover, the source code files of Typilus' dataset are converted to graph representations that are only suitable for training the Typilus model. The Many-Types4Py dataset provides JSON-formatted analyzed source [16] is not collected solely for the ML-based type inference task, meaning that a large number of projects in the dataset may not have type annotations at all, especially given the time that the dataset was created. Allamanis [12] showed that the Python-150K dataset suffers from code duplication despite the removal of project forks."
              ],
              "metadata": null
            },
            {
              "id": "(Allamanis et al., 2020)",
              "snippets": [
                "Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program\u2019s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace \u2014 a continuous relaxation of the discrete space of types \u2014 and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered."
              ],
              "metadata": null
            },
            {
              "id": "(Cui et al., 2021)",
              "snippets": [
                "Our datasets are summarized in Table I. The Original and Valid present the number of annotations before and after data cleaning and deduplication. typePY is our source code dataset collected from 4,577 top-star GitHub repositories. probPY stands for the dataset published in Xu et al. (Xu et al., 2016), and typeshed is a human-labeled dataset containing only the annotations for function parameters and return values [14]",
                "For the probPY [13] dataset, we exploit the merged data combining the result of PySonar2 and dynamic analysis",
                "The typeshed dataset contains human-label type annotations for Python standard libraries and third-party packages, making it more reliable. However, the typeshed dataset only covers the annotations for function parameters and return types, and there is no contextual information due to frequent code updates."
              ],
              "metadata": null
            },
            {
              "id": "(Liu et al., 2025)",
              "snippets": [
                "We utilize the benchmark provided by Pynguin (Lukasczyk et al., 2022), which we refer to as Pyn. This benchmark has been widely adopted by tools such as CodaMosa (Lemieux et al., 2023) and Coverup [20]. Pyn comprises 17 real-world projects collected from datasets such as BugsInPy (Widyasari et al., 2020) and ManyTypes4Py (Mir et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Lukasczyk et al., 2022)",
              "snippets": [
                "Automated unit test generation is a well-known methodology aiming to reduce the developers\u2019 effort of writing tests manually. Prior research focused mainly on statically typed programming languages like Java. In practice, however, dynamically typed languages have received a huge gain in popularity over the last decade. This introduces the need for tools and research on test generation for these languages, too. We introduce Pynguin, an extendable testgeneration framework for Python, which generates regression tests with high code coverage. Pynguin is designed to be easily usable by practitioners; it is also extensible to allow researchers to adapt it for their needs and to enable future research. We provide a demo of Pynguin at https://youtu.be/UiGrG25Vts0; further information, documentation, the tool, and its source code are available at https://www.pynguin.eu."
              ],
              "metadata": null
            },
            {
              "id": "(Lemieux et al., 2023)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Boone et al., 2019)",
              "snippets": [
                "Using 5,996 open source projects mined from GitHub and Libraries.io that are likely to have type annotations, we train the model to predict types of functions without annotations."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Dataset Limitations and Considerations",
          "text": "\n- **Code Duplication Issues**: A significant concern in many code datasets is duplication. Allamanis identified that datasets like Python-150K suffer from code duplication despite attempts to remove project forks, which can artificially inflate performance metrics by up to 100% when evaluating machine learning models <Paper corpusId=\"56482376\" paperTitle=\"(Allamanis, 2018)\" isShortName></Paper>. The ManyTypes4Py dataset specifically addresses this limitation by removing duplicate source files to provide more reliable evaluation results.\n\n- **Inconsistent Annotation Coverage**: Many datasets collected from GitHub may have inconsistent type annotation coverage. Some projects in these datasets may not have type annotations at all, especially in older collections, making them less suitable for comprehensive type inference evaluation <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper>.\n\n- **Format Constraints**: Some datasets are formatted for specific models, limiting their general applicability. For example, the Typilus dataset converts source code files to graph representations specifically designed for training the Typilus model, which may not be suitable for evaluating other approaches <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper> <Paper corpusId=\"216056383\" paperTitle=\"(Allamanis et al., 2020)\" isShortName></Paper>.\n\n- **Missing Contextual Information**: The typeshed dataset, while valuable for its human-labeled annotations, only covers function parameters and return types. It lacks contextual information due to frequent code updates, limiting its usefulness for comprehensive type inference evaluation <Paper corpusId=\"235658605\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\n- **Data Cleaning Requirements**: Datasets often require significant preprocessing before they can be used reliably. For example, Cui et al. reported notable differences between \"Original\" and \"Valid\" annotation counts after data cleaning and deduplication in their datasets, highlighting the need for careful preparation <Paper corpusId=\"235658605\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\n- **Temporal Relevance**: As Python's typing system continues to evolve with new PEPs (Python Enhancement Proposals), older datasets may not reflect current typing practices and capabilities, potentially limiting their relevance for evaluating modern type inference systems <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\n- **Benchmark Diversity**: While benchmarks aim to represent real-world code, they may still lack diversity in programming styles, domains, or complexity levels, potentially limiting how well evaluation results generalize to all Python code <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
          "citations": [
            {
              "id": "(Allamanis, 2018)",
              "snippets": [
                "The field of big code relies on mining large corpora of code to perform some learning task towards creating better tools for software engineers. A significant threat to this approach was recently identified by Lopes et al. (2017) who found a large amount of near-duplicate code on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this work, we explore the effects of code duplication on machine learning models showing that reported performance metrics are sometimes inflated by up to 100% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present a duplication index for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them. Finally, we release tools to help the community avoid this problem in future research."
              ],
              "metadata": null
            },
            {
              "id": "(Mir et al., 2021)",
              "snippets": [
                "Recently, Allamanis et al. [8] proposed the Typilus model, which is a graphbased neural model that predicts type annotations for Python. The Typilus model [8] is accompanied by a dataset that contains 600 Python projects. Moreover, the source code files of Typilus' dataset are converted to graph representations that are only suitable for training the Typilus model. The Many-Types4Py dataset provides JSON-formatted analyzed source [16] is not collected solely for the ML-based type inference task, meaning that a large number of projects in the dataset may not have type annotations at all, especially given the time that the dataset was created. Allamanis [12] showed that the Python-150K dataset suffers from code duplication despite the removal of project forks."
              ],
              "metadata": null
            },
            {
              "id": "(Allamanis et al., 2020)",
              "snippets": [
                "Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program\u2019s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace \u2014 a continuous relaxation of the discrete space of types \u2014 and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered."
              ],
              "metadata": null
            },
            {
              "id": "(Cui et al., 2021)",
              "snippets": [
                "Our datasets are summarized in Table I. The Original and Valid present the number of annotations before and after data cleaning and deduplication. typePY is our source code dataset collected from 4,577 top-star GitHub repositories. probPY stands for the dataset published in Xu et al. (Xu et al., 2016), and typeshed is a human-labeled dataset containing only the annotations for function parameters and return values [14]",
                "For the probPY [13] dataset, we exploit the merged data combining the result of PySonar2 and dynamic analysis",
                "The typeshed dataset contains human-label type annotations for Python standard libraries and third-party packages, making it more reliable. However, the typeshed dataset only covers the annotations for function parameters and return types, and there is no contextual information due to frequent code updates."
              ],
              "metadata": null
            }
          ]
        },
        {
          "title": "Domain-Specific and Specialized Datasets",
          "text": "\nWhile the major benchmark datasets provide general evaluation frameworks for Python type inference systems, researchers have also developed specialized datasets to address particular challenges or domains. These specialized datasets enable the evaluation of type inference systems under specific conditions or for particular use cases.\n\nThe CrossDomainTypes4Py dataset was specifically designed to evaluate domain adaptation in type inference systems <Paper corpusId=\"251710434\" paperTitle=\"(Gruner et al., 2022)\" isShortName></Paper>. This dataset complements the more general ManyTypes4Py benchmark and allows researchers to test transfer learning methods that can adapt type inference systems to different programming domains <Paper corpusId=\"251710434\" paperTitle=\"(Gruner et al., 2022)\" isShortName></Paper> <Paper corpusId=\"2871880\" paperTitle=\"(Ganin et al., 2015)\" isShortName></Paper> <Paper corpusId=\"3861990\" paperTitle=\"(Shen et al., 2017)\" isShortName></Paper>. The CrossDomainTypes4Py dataset has been used alongside ManyTypes4Py in several recent evaluations, including work by Elkobi et al. on type inference systems <Paper corpusId=\"260512650\" paperTitle=\"(Elkobi et al., 2023)\" isShortName></Paper>.\n\nFor more granular evaluation of specific typing capabilities, the TEP (Type Evaluation for Python) framework provides a micro-benchmark consisting of 154 code snippets with 845 type annotations as ground truth <Paper corpusId=\"268033677\" paperTitle=\"(Venkatesh et al., 2024)\" isShortName></Paper>. Unlike larger datasets that evaluate overall performance, this micro-benchmark focuses on particular aspects of typing to provide more targeted assessment.\n\nThe Pynguin benchmark (referred to as \"Pyn\") consists of 17 real-world Python projects collected from various sources including BugsInPy <Paper corpusId=\"226274286\" paperTitle=\"(Widyasari et al., 2020)\" isShortName></Paper> and ManyTypes4Py <Paper corpusId=\"233210280\" paperTitle=\"(Mir et al., 2021)\" isShortName></Paper>. This benchmark has gained adoption for evaluating various Python tools, including CodaMosa <Paper corpusId=\"259860757\" paperTitle=\"(Lemieux et al., 2023)\" isShortName></Paper> and Coverup <Paper corpusId=\"277104003\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. While originally designed for test generation research <Paper corpusId=\"246706202\" paperTitle=\"(Lukasczyk et al., 2022)\" isShortName></Paper>, the Pyn benchmark has proven valuable for type inference evaluation due to its collection of real-world Python projects with diverse typing characteristics.\n\nThese specialized datasets complement the major benchmarks by addressing specific research questions or supporting evaluation in particular contexts. They enable researchers to assess type inference systems not only on general performance metrics but also on their ability to handle domain adaptation, specific typing scenarios, or real-world code from diverse sources.",
          "citations": [
            {
              "id": "(Gruner et al., 2022)",
              "snippets": [
                "In this paper, we demonstrate the negative impact of the mentioned issues on the Type4Py type inference system and aim to mitigate them by using transfer learning methods (Ganin et al., 2015), (Shen et al., 2017). For our cross-domain experiments we use the benchmark dataset ManyTypes4Py (Mir et al., 2021) and our new Cross-DomainTypes4Py dataset, which we present in this paper."
              ],
              "metadata": null
            },
            {
              "id": "(Ganin et al., 2015)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Shen et al., 2017)",
              "snippets": [
                "Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classification. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classification adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches."
              ],
              "metadata": null
            },
            {
              "id": "(Elkobi et al., 2023)",
              "snippets": [
                "For the evaluation, we use the recent datasets CrossDomainTypes4Py [8] and ManyTypes4Py (Mir et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Venkatesh et al., 2024)",
              "snippets": [
                "To answer RQ2, we choose the micro-benchmark from T E P [19], a general framework for evaluating type inference tools in Python. T E P contains a micro-benchmark with 154 code snippets and 845 type annotations as ground truth."
              ],
              "metadata": null
            },
            {
              "id": "(Widyasari et al., 2020)",
              "snippets": [
                "The 2019 edition of Stack Overflow developer survey highlights that, for the first time, Python outperformed Java in terms of popularity. The gap between Python and Java further widened in the 2020 edition of the survey. Unfortunately, despite the rapid increase in Python's popularity, there are not many testing and debugging tools that are designed for Python. This is in stark contrast with the abundance of testing and debugging tools for Java. Thus, there is a need to push research on tools that can help Python developers. One factor that contributed to the rapid growth of Java testing and debugging tools is the availability of benchmarks. A popular benchmark is the Defects4J benchmark; its initial version contained 357 real bugs from 5 real-world Java programs. Each bug comes with a test suite that can expose the bug. Defects4J has been used by hundreds of testing and debugging studies and has helped to push the frontier of research in these directions. In this project, inspired by Defects4J, we create another benchmark database and tool that contain 493 real bugs from 17 real-world Python programs. We hope our benchmark can help catalyze future work on testing and debugging tools that work on Python programs."
              ],
              "metadata": null
            },
            {
              "id": "(Mir et al., 2021)",
              "snippets": [
                "Recently, Allamanis et al. [8] proposed the Typilus model, which is a graphbased neural model that predicts type annotations for Python. The Typilus model [8] is accompanied by a dataset that contains 600 Python projects. Moreover, the source code files of Typilus' dataset are converted to graph representations that are only suitable for training the Typilus model. The Many-Types4Py dataset provides JSON-formatted analyzed source [16] is not collected solely for the ML-based type inference task, meaning that a large number of projects in the dataset may not have type annotations at all, especially given the time that the dataset was created. Allamanis [12] showed that the Python-150K dataset suffers from code duplication despite the removal of project forks."
              ],
              "metadata": null
            },
            {
              "id": "(Lemieux et al., 2023)",
              "snippets": [
                "Please click on the paper title to read the abstract on Semantic Scholar."
              ],
              "metadata": null
            },
            {
              "id": "(Liu et al., 2025)",
              "snippets": [
                "We utilize the benchmark provided by Pynguin (Lukasczyk et al., 2022), which we refer to as Pyn. This benchmark has been widely adopted by tools such as CodaMosa (Lemieux et al., 2023) and Coverup [20]. Pyn comprises 17 real-world projects collected from datasets such as BugsInPy (Widyasari et al., 2020) and ManyTypes4Py (Mir et al., 2021)."
              ],
              "metadata": null
            },
            {
              "id": "(Lukasczyk et al., 2022)",
              "snippets": [
                "Automated unit test generation is a well-known methodology aiming to reduce the developers\u2019 effort of writing tests manually. Prior research focused mainly on statically typed programming languages like Java. In practice, however, dynamically typed languages have received a huge gain in popularity over the last decade. This introduces the need for tools and research on test generation for these languages, too. We introduce Pynguin, an extendable testgeneration framework for Python, which generates regression tests with high code coverage. Pynguin is designed to be easily usable by practitioners; it is also extensible to allow researchers to adapt it for their needs and to enable future research. We provide a demo of Pynguin at https://youtu.be/UiGrG25Vts0; further information, documentation, the tool, and its source code are available at https://www.pynguin.eu."
              ],
              "metadata": null
            }
          ]
        }
      ]
    },
    "rubric": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What publicly available datasets are typically used for evaluating type inference systems in python?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define what is the goal of using a type inference system for programming languages in general.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Goal of type inference: Automatically deduce the most general type for each expression. Two key points: 1. Automatically inferring types: This means the programmer has to write no types, but still gets all the benefit from static typing 2. Inferring the most general type: This means we want to infer polymorphic types whenever possible"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should emphasize on the importance of an automatic type inference system for Python.",
            "weight": 0.13333333333333333,
            "evidence": [
              " its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs.",
              "In Python, which is dynamically typed, this determination takes place at runtime. To address potential ambiguities, developers can utilize type annotations, which explicitly specifies the expected data types of variables or function returns. As the complexity of software projects increases, programmers find it increasingly challenging to maintain consistent data types. In response to this challenge, both industry and academia have developed type inference tools and static type checkers."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss the need for a unified approach for evaluating different type inference systems and mention several evaluation metrics, including exact matches, report of missing types, accuracy, etc.",
            "weight": 0.13333333333333333,
            "evidence": [
              "In light of the growing interest in type inference research for Python, both researchers and practitioners require a standardized process to assess the performance of various type inference techniques.",
              "Exact matches: The number of inferred types that exactly match the ground truth. This metric is used widely used in the literature to evaluate type inference tools (Allamanis et al., 2020; Peng et al., 2022; Mir et al., 2022).",
              "Report of missing types: List of types that are present in the ground truth but are unreported by the tools.",
              "### **Accuracy** The correctness of the inferred types compared to the ground truth. ### **Performance** The computation resources and time required to run the type inference. ### **Coverage** The range and variety of code constructs and libraries handled by the inference system."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should enumerate publicly available datasets used for evaluating type inference systems in Python and provide a brief description for each of them.",
            "weight": 0.13333333333333333,
            "evidence": [
              "1. **ManyTypes4Py**: - **Description**: ManyTypes4Py is a large Python dataset for machine learning-based type inference. It contains 5,382 Python projects with over 869,000 type annotations. The dataset is split into training, validation, and test sets by files to facilitate the training and evaluation of machine learning models. - **Features**: The dataset includes a lightweight static analyzer pipeline to extract type information from abstract syntax trees (ASTs) and store the results in JSON-formatted files.",
              "The Typilus model [8] is accompanied by a dataset that contains 600 Python projects. Moreover, the source code files of Typilus' dataset are converted to graph representations that are only suitable for training the Typilus model.",
              "2. **TypeEvalPy**: - **Description**: TypeEvalPy is a micro-benchmarking framework for evaluating type inference tools. It contains 154 code snippets with 845 type annotations across 18 categories targeting various Python features. - **Features**: The framework manages the execution of containerized tools, transforms inferred types into a standardized format, and produces meaningful metrics for assessment.",
              "3. **BigQuery Public Datasets**: - **Description**: BigQuery provides a range of public datasets that can be used for various purposes, including type inference. These datasets are accessible through the Google Cloud Public Dataset Program and can be queried using SQL or GoogleSQL. - **Features**: The datasets include a variety of data sources, such as weather information, GitHub repository data, and Wikipedia revision history.",
              "Raychev et al. [16] published the Python-150K dataset in 2016, which contains 8,422 Python projects.",
              "Python-150K dataset [16] is not collected solely for the ML-based type inference task, meaning that a large number of projects in the dataset may not have type annotations at all, especially given the time that the dataset was created.",
              "Our main dataset, BetterTypes4Py, is constructed by selecting a high-quality subset from the ManyTypes4Py dataset (Mir et al., 2021), which was used to train Type4Py.",
              "InferTypes4Py, a test set derived from the source code of Typilus, Type4Py, and our own tool, none of which were used as CodeT5's (pre-)training data"
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could explain different categories of methods for type inference in Python such as rule-based and ML-based approaches.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze-style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic."
            ]
          }
        ]
      }
    }
  }
]